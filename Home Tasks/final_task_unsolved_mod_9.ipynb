{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_task_unsolved_mod_9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/delhian/NLP_course/blob/master/Home%20Tasks/final_task_unsolved_mod_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P13OapCzIwT"
      },
      "source": [
        "## Постановка задачи"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-NCaT9b-lQA"
      },
      "source": [
        "В рамках данного финального задания мы посторим сеть для перевода с немецкого языка на английский."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlPQCXQf9zwT"
      },
      "source": [
        "*Этот блокнот основан на [реализации с открытым исходным кодом](https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb) модели seq2seq NMT в PyTorch.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ9Lx1I3-ybs"
      },
      "source": [
        "Мы попробуем реализовать модель из этой статьи -> [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215).\n",
        "\n",
        "Модель будет обучена переводу с немецкого на английский, но ее можно применить к любой проблеме, связанной с переходом от одной последовательности к другой, например, к обобщению."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZFfVYKu_KfH"
      },
      "source": [
        "Наиболее распространенными моделями последовательностей (seq2seq) являются модели *энкодер-декодер*, которые (обычно) используют *рекуррентную нейронную сеть* (RNN) для *кодирования* исходного (входного) предложения в один вектор. В этом ноутбуке мы будем называть этот единственный вектор *контекстным вектором*. Вы можете рассматривать вектор контекста как абстрактное представление всего входного предложения. Этот вектор затем *декодируется* вторым RNN, который учится выводить целевое (выходное) предложение, генерируя его по одному слову за раз."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GwZM6Ep_XUZ"
      },
      "source": [
        "![](https://github.com/neychev/made_nlp_course/blob/master/week06_SelfAttention_and_NMT_practice/img/seq2seq1.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxYH--4k_w44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c01721cb-2130-4dd4-b869-ffa7525cd801"
      },
      "source": [
        "!pip install https://download.pytorch.org/whl/nightly/cu102/torch-1.8.0.dev20210210-cp37-cp37m-linux_x86_64.whl\n",
        "!pip install torchtext==0.9.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.8.0.dev20210210\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu102/torch-1.8.0.dev20210210-cp37-cp37m-linux_x86_64.whl (857.4 MB)\n",
            "\u001b[K     |███████████████████████████████▏| 834.1 MB 1.2 MB/s eta 0:00:19tcmalloc: large alloc 1147494400 bytes == 0x55e7b5cb6000 @  0x7f234d753615 0x55e77bf344cc 0x55e77c01447a 0x55e77bf372ed 0x55e77c028e1d 0x55e77bfaae99 0x55e77bfa59ee 0x55e77bf38bda 0x55e77bfaad00 0x55e77bfa59ee 0x55e77bf38bda 0x55e77bfa7737 0x55e77c029c66 0x55e77bfa6daf 0x55e77c029c66 0x55e77bfa6daf 0x55e77c029c66 0x55e77bfa6daf 0x55e77bf39039 0x55e77bf7c409 0x55e77bf37c52 0x55e77bfaac25 0x55e77bfa59ee 0x55e77bf38bda 0x55e77bfa7737 0x55e77bfa59ee 0x55e77bf38bda 0x55e77bfa6915 0x55e77bf38afa 0x55e77bfa6c0d 0x55e77bfa59ee\n",
            "\u001b[K     |████████████████████████████████| 857.4 MB 5.2 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0.dev20210210) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0.dev20210210) (3.7.4.3)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu111\n",
            "    Uninstalling torch-1.9.0+cu111:\n",
            "      Successfully uninstalled torch-1.9.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu111 requires torch==1.9.0, but you have torch 1.8.0.dev20210210 which is incompatible.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.8.0.dev20210210 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.0.dev20210210\n",
            "Collecting torchtext==0.9.0\n",
            "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (1.19.5)\n",
            "Collecting torch==1.8.0\n",
            "  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 735.5 MB 10 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->torchtext==0.9.0) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (3.0.4)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.8.0.dev20210210\n",
            "    Uninstalling torch-1.8.0.dev20210210:\n",
            "      Successfully uninstalled torch-1.8.0.dev20210210\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.10.0\n",
            "    Uninstalling torchtext-0.10.0:\n",
            "      Successfully uninstalled torchtext-0.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu111 requires torch==1.9.0, but you have torch 1.8.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.0 torchtext-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-ZlxYkLC20f"
      },
      "source": [
        "!python -m spacy download en > /dev/null\n",
        "!python -m spacy download de > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7140amzT9qCM"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "from torchtext.vocab import Vocab\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "import io\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import spacy\n",
        "\n",
        "from collections import Counter\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mII7kLtZ40Nz"
      },
      "source": [
        "Зафиксируем все возможные случайные сиды, для максимальной воспроизводимости"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmlStWex_pvX"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgBRyR5nBflE"
      },
      "source": [
        "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
        "train_urls = ('train.de.gz', 'train.en.gz')\n",
        "val_urls = ('val.de.gz', 'val.en.gz')\n",
        "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RVdE7fNBi34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4037d7b-8741-41f0-8a35-3e914fbee513"
      },
      "source": [
        "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
        "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
        "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train.de.gz: 100%|██████████| 637k/637k [00:00<00:00, 8.33MB/s]\n",
            "train.en.gz: 100%|██████████| 569k/569k [00:00<00:00, 11.3MB/s]\n",
            "val.de.gz: 100%|██████████| 24.7k/24.7k [00:00<00:00, 7.91MB/s]\n",
            "val.en.gz: 100%|██████████| 21.6k/21.6k [00:00<00:00, 4.50MB/s]\n",
            "test_2016_flickr.de.gz: 100%|██████████| 22.9k/22.9k [00:00<00:00, 5.54MB/s]\n",
            "test_2016_flickr.en.gz: 100%|██████████| 21.1k/21.1k [00:00<00:00, 9.08MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZBrvv_1ism7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "410d47af-8737-4c00-ad4c-705925805cad"
      },
      "source": [
        "train_filepaths"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/.data/train.de', '/content/.data/train.en']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "droT9QIi4_Ss"
      },
      "source": [
        "Проинициализируем токенизаторы для исходного и целевого языка:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY-2-Ph0Cybp"
      },
      "source": [
        "de_tokenizer = get_tokenizer('spacy', language='de')\n",
        "en_tokenizer = get_tokenizer('spacy', language='en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my53nYHOBwFI"
      },
      "source": [
        "## Загрузка данных\n",
        "\n",
        "Напишите функцию по созданию словаря, возвращающую словарь из `torchtext`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMAJgOZBnk29"
      },
      "source": [
        "def build_vocab(filepath, tokenizer):\n",
        "  counter = Counter()\n",
        "  with io.open(filepath, encoding=\"utf8\") as f:\n",
        "    for string_ in f:\n",
        "      counter.update(tokenizer(string_))\n",
        "  return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCb6xpkiCM0l"
      },
      "source": [
        "de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
        "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyQF0RwPDbzb"
      },
      "source": [
        "Проверка функции:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkD87GFyEoAX"
      },
      "source": [
        "assert en_vocab['a'] < 10\n",
        "assert de_vocab['Ein'] < 15000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K30E1uWT5PgQ"
      },
      "source": [
        "Напишем функцию для составления большого тензора, который будет хранить на каждой строчке исходное токенизированное предложение и целевое."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Cwm-wy3DbKi"
      },
      "source": [
        "def data_process(filepaths):\n",
        "  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
        "  data = []\n",
        "  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
        "    de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)],\n",
        "                            dtype=torch.long)\n",
        "    en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)],\n",
        "                            dtype=torch.long)\n",
        "    data.append((de_tensor_, en_tensor_))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sV7nGzsFMSC"
      },
      "source": [
        "Проинициализируйте переменные `train_data, val_data, test_data`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWhM0WZNFdXJ"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "train_data = data_process(train_filepaths)\n",
        "val_data = data_process(val_filepaths)\n",
        "test_data = data_process(test_filepaths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpToqCEyFjwd"
      },
      "source": [
        "assert len(train_data) == 29000\n",
        "assert len(val_data) == 1014\n",
        "assert len(test_data) == 1000\n",
        "assert train_data[23][0].sum().item() > 3220"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35zSx-HlGfhn"
      },
      "source": [
        "#YOUR CODE HERE\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jY4bt_CGkj0"
      },
      "source": [
        "assert device is not None\n",
        "assert device.type == 'cuda'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Btfg-D1aGyQy"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "PAD_IDX = de_vocab['<pad>']\n",
        "BOS_IDX = de_vocab['<bos>']\n",
        "EOS_IDX = de_vocab['<eos>']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VoleUx4G1G1"
      },
      "source": [
        "Дополните функцию `generate_batch` чтобы она добавляла паддинг в конце и чтобы первыми символами были `BOS_IDX`, а последним `EOS_IDX`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bkwmSw6HYL-"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "def generate_batch(data_batch):\n",
        "  de_batch, en_batch = [], []\n",
        "  for (de_item, en_item) in data_batch:\n",
        "    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
        "  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
        "  return de_batch, en_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QABYiLrGJFf"
      },
      "source": [
        "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "                       shuffle=True, collate_fn=generate_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F9WdPUPHuDO"
      },
      "source": [
        "import random\n",
        "from typing import Tuple\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9CZqwEm8kgR"
      },
      "source": [
        "## Подготовка модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ5FSyXU503R"
      },
      "source": [
        "Наконец мы переходим к написанию самой нейросети. Она будет состоять из 3 различных классов, а финальная сеть будет состоять из этих трех классов. Получится несколько объемно, но когда мы начинаем переходить к более серьезным сетям."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNK-ORW1In6y"
      },
      "source": [
        "Создадим модель Seq2Seq на основе GRU:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JekezzB5I0_O"
      },
      "source": [
        "Создайте класс `Encoder`, который состоит из слоя Embedding, GRU(bidirectional) и Linear:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvvpXnXcIxNT"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim: int,\n",
        "                 emb_dim: int,\n",
        "                 enc_hid_dim: int,\n",
        "                 dec_hid_dim: int,\n",
        "                 dropout: float):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor) -> Tuple[Tensor]:\n",
        "\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "\n",
        "        return outputs, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9QCQDhkJUZy"
      },
      "source": [
        "Оставим класс Attention, чтобы сохранить преемственность с оригинальной статьей. Подбробнее про Attention мы поговорим в следующих модулях."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRCJw2w1Iv5Y"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 enc_hid_dim: int,\n",
        "                 dec_hid_dim: int,\n",
        "                 attn_dim: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "\n",
        "        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n",
        "\n",
        "        self.attn = nn.Linear(self.attn_in, attn_dim)\n",
        "\n",
        "    def forward(self,\n",
        "                decoder_hidden: Tensor,\n",
        "                encoder_outputs: Tensor) -> Tensor:\n",
        "\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "\n",
        "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "        energy = torch.tanh(self.attn(torch.cat((\n",
        "            repeated_decoder_hidden,\n",
        "            encoder_outputs),\n",
        "            dim = 2)))\n",
        "\n",
        "        attention = torch.sum(energy, dim=2)\n",
        "\n",
        "        return F.softmax(attention, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RdFhl6KJvB9"
      },
      "source": [
        "Добавим класс `Decoder`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7WCAa7YJyg2"
      },
      "source": [
        "Аналогично классу `Encoder` сделайте класс `Decoder` таким же по архитектуре, однако линейный слой будет иметь размерность emb_dim  плюс размерность выходного слоя Attention."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk-tKFwoKCIp"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 output_dim: int,\n",
        "                 emb_dim: int,\n",
        "                 enc_hid_dim: int,\n",
        "                 dec_hid_dim: int,\n",
        "                 dropout: int,\n",
        "                 attention: nn.Module):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dropout = dropout\n",
        "        self.attention = attention\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "\n",
        "        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def _weighted_encoder_rep(self,\n",
        "                              decoder_hidden: Tensor,\n",
        "                              encoder_outputs: Tensor) -> Tensor:\n",
        "\n",
        "        a = self.attention(decoder_hidden, encoder_outputs)\n",
        "\n",
        "        a = a.unsqueeze(1)\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n",
        "\n",
        "        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n",
        "\n",
        "        return weighted_encoder_rep\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "                input: Tensor,\n",
        "                decoder_hidden: Tensor,\n",
        "                encoder_outputs: Tensor) -> Tuple[Tensor]:\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,\n",
        "                                                          encoder_outputs)\n",
        "\n",
        "        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n",
        "\n",
        "        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n",
        "\n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n",
        "\n",
        "        output = self.out(torch.cat((output,\n",
        "                                     weighted_encoder_rep,\n",
        "                                     embedded), dim = 1))\n",
        "\n",
        "        return output, decoder_hidden.squeeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "steO_DGUKxu_"
      },
      "source": [
        "Наконец, объединим все в класс модели `Seq2Seq`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8yAxyZiK4t4"
      },
      "source": [
        "Напишите, что должен принимать на вход класс Seq2Seq и проинициализируйте экземпляр этого класса:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ict3MDeKxOj"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self,\n",
        "                 encoder: nn.Module,\n",
        "                 decoder: nn.Module,\n",
        "                 device: torch.device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                teacher_forcing_ratio: float = 0.5) -> Tensor:\n",
        "\n",
        "        batch_size = src.shape[1]\n",
        "        max_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "        # first input to the decoder is the <sos> token\n",
        "        output = trg[0,:]\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.max(1)[1]\n",
        "            output = (trg[t] if teacher_force else top1)\n",
        "\n",
        "        return outputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-FbdHsi6uFC"
      },
      "source": [
        "Вы можете попробовать тут свои значения для разных слоев. Единственное - правильно задайте размеры входных слоев для Encoder и Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXUquwEVLI4m"
      },
      "source": [
        "INPUT_DIM = len(de_vocab)\n",
        "OUTPUT_DIM = len(en_vocab)\n",
        "ENC_EMB_DIM = 64\n",
        "DEC_EMB_DIM = 64\n",
        "ENC_HID_DIM = 128\n",
        "DEC_HID_DIM = 128\n",
        "ATTN_DIM = 32\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3bwd5mWLnMC"
      },
      "source": [
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sxA2jKgLxu2"
      },
      "source": [
        "Напишите функцию, считающую параметры модели:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khvc3_qgL8P4"
      },
      "source": [
        "def count_parameters(model: nn.Module):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR5jUX5LMEQt"
      },
      "source": [
        "assert count_parameters(model) > 7_000_000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ots0k0mMMSJo"
      },
      "source": [
        "Проинициализируем веса, как это рекомендуют в оригинальной статье:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs8cWMvnJdSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12fa7c91-f62b-454b-e7e1-8f1de13a67a4"
      },
      "source": [
        "def init_weights(m: nn.Module):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(19206, 64)\n",
              "    (rnn): GRU(64, 128, bidirectional=True)\n",
              "    (fc): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=384, out_features=32, bias=True)\n",
              "    )\n",
              "    (embedding): Embedding(10840, 64)\n",
              "    (rnn): GRU(320, 128)\n",
              "    (out): Linear(in_features=448, out_features=10840, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-wKKaoDJdMY"
      },
      "source": [
        "PAD_IDX = en_vocab.stoi['<pad>']\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfBl6-DF8yUZ"
      },
      "source": [
        "## Обучение модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzSjMJYffi21"
      },
      "source": [
        "Напишите функцию обучения модели. Если получится, добавьте в тренировочный цикл запись значений функции потерь и перплексии в tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJ-0BmNgf1YA"
      },
      "source": [
        "import math\n",
        "import time\n",
        "\n",
        "\n",
        "def train(model: nn.Module,\n",
        "          iterator: torch.utils.data.DataLoader,\n",
        "          optimizer: optim.Optimizer,\n",
        "          criterion: nn.Module,\n",
        "          clip: float):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for _, (src, trg) in enumerate(iterator):\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg)\n",
        "\n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        trg = trg[1:].view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPC0UqNO7PSE"
      },
      "source": [
        "Напишем функцию для валидации модели. Хочу отметить, что при валидации наиболее важным моментом является выключение метода teacher-forcing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cs7FWIJ-MgfW"
      },
      "source": [
        "\n",
        "\n",
        "def evaluate(model: nn.Module,\n",
        "             iterator: torch.utils.data.DataLoader,\n",
        "             criterion: nn.Module):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for _, (src, trg) in enumerate(iterator):\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICUCceM6M047"
      },
      "source": [
        "def epoch_time(start_time: int,\n",
        "               end_time: int):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjmAW7RiM0xW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46f58689-dfb0-4c95-b169-80d1c0d20597"
      },
      "source": [
        "\n",
        "\n",
        "N_EPOCHS = 50\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iter, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "test_loss = evaluate(model, test_iter, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 1m 25s\n",
            "\tTrain Loss: 2.210 | Train PPL:   9.112\n",
            "\t Val. Loss: 3.697 |  Val. PPL:  40.317\n",
            "Epoch: 02 | Time: 1m 25s\n",
            "\tTrain Loss: 2.141 | Train PPL:   8.511\n",
            "\t Val. Loss: 3.700 |  Val. PPL:  40.464\n",
            "Epoch: 03 | Time: 1m 25s\n",
            "\tTrain Loss: 2.085 | Train PPL:   8.044\n",
            "\t Val. Loss: 3.719 |  Val. PPL:  41.238\n",
            "Epoch: 04 | Time: 1m 24s\n",
            "\tTrain Loss: 2.016 | Train PPL:   7.509\n",
            "\t Val. Loss: 3.753 |  Val. PPL:  42.629\n",
            "Epoch: 05 | Time: 1m 24s\n",
            "\tTrain Loss: 1.981 | Train PPL:   7.249\n",
            "\t Val. Loss: 3.696 |  Val. PPL:  40.301\n",
            "Epoch: 06 | Time: 1m 23s\n",
            "\tTrain Loss: 1.945 | Train PPL:   6.997\n",
            "\t Val. Loss: 3.700 |  Val. PPL:  40.456\n",
            "Epoch: 07 | Time: 1m 24s\n",
            "\tTrain Loss: 1.890 | Train PPL:   6.621\n",
            "\t Val. Loss: 3.722 |  Val. PPL:  41.358\n",
            "Epoch: 08 | Time: 1m 24s\n",
            "\tTrain Loss: 1.845 | Train PPL:   6.326\n",
            "\t Val. Loss: 3.734 |  Val. PPL:  41.840\n",
            "Epoch: 09 | Time: 1m 23s\n",
            "\tTrain Loss: 1.792 | Train PPL:   6.004\n",
            "\t Val. Loss: 3.797 |  Val. PPL:  44.557\n",
            "Epoch: 10 | Time: 1m 24s\n",
            "\tTrain Loss: 1.793 | Train PPL:   6.006\n",
            "\t Val. Loss: 3.706 |  Val. PPL:  40.698\n",
            "Epoch: 11 | Time: 1m 24s\n",
            "\tTrain Loss: 1.772 | Train PPL:   5.882\n",
            "\t Val. Loss: 3.708 |  Val. PPL:  40.781\n",
            "Epoch: 12 | Time: 1m 24s\n",
            "\tTrain Loss: 1.723 | Train PPL:   5.602\n",
            "\t Val. Loss: 3.744 |  Val. PPL:  42.251\n",
            "Epoch: 13 | Time: 1m 24s\n",
            "\tTrain Loss: 1.688 | Train PPL:   5.408\n",
            "\t Val. Loss: 3.761 |  Val. PPL:  42.989\n",
            "Epoch: 14 | Time: 1m 24s\n",
            "\tTrain Loss: 1.661 | Train PPL:   5.266\n",
            "\t Val. Loss: 3.751 |  Val. PPL:  42.562\n",
            "Epoch: 15 | Time: 1m 23s\n",
            "\tTrain Loss: 1.628 | Train PPL:   5.091\n",
            "\t Val. Loss: 3.769 |  Val. PPL:  43.324\n",
            "Epoch: 16 | Time: 1m 24s\n",
            "\tTrain Loss: 1.603 | Train PPL:   4.966\n",
            "\t Val. Loss: 3.777 |  Val. PPL:  43.687\n",
            "Epoch: 17 | Time: 1m 24s\n",
            "\tTrain Loss: 1.588 | Train PPL:   4.893\n",
            "\t Val. Loss: 3.788 |  Val. PPL:  44.155\n",
            "Epoch: 18 | Time: 1m 24s\n",
            "\tTrain Loss: 1.566 | Train PPL:   4.787\n",
            "\t Val. Loss: 3.793 |  Val. PPL:  44.370\n",
            "Epoch: 19 | Time: 1m 25s\n",
            "\tTrain Loss: 1.555 | Train PPL:   4.733\n",
            "\t Val. Loss: 3.768 |  Val. PPL:  43.277\n",
            "Epoch: 20 | Time: 1m 25s\n",
            "\tTrain Loss: 1.530 | Train PPL:   4.619\n",
            "\t Val. Loss: 3.819 |  Val. PPL:  45.572\n",
            "Epoch: 21 | Time: 1m 24s\n",
            "\tTrain Loss: 1.519 | Train PPL:   4.565\n",
            "\t Val. Loss: 3.784 |  Val. PPL:  43.977\n",
            "Epoch: 22 | Time: 1m 25s\n",
            "\tTrain Loss: 1.509 | Train PPL:   4.520\n",
            "\t Val. Loss: 3.807 |  Val. PPL:  45.021\n",
            "Epoch: 23 | Time: 1m 25s\n",
            "\tTrain Loss: 1.516 | Train PPL:   4.552\n",
            "\t Val. Loss: 3.764 |  Val. PPL:  43.109\n",
            "Epoch: 24 | Time: 1m 24s\n",
            "\tTrain Loss: 1.465 | Train PPL:   4.328\n",
            "\t Val. Loss: 3.811 |  Val. PPL:  45.217\n",
            "Epoch: 25 | Time: 1m 25s\n",
            "\tTrain Loss: 1.467 | Train PPL:   4.335\n",
            "\t Val. Loss: 3.833 |  Val. PPL:  46.183\n",
            "Epoch: 26 | Time: 1m 24s\n",
            "\tTrain Loss: 1.441 | Train PPL:   4.224\n",
            "\t Val. Loss: 3.845 |  Val. PPL:  46.736\n",
            "Epoch: 27 | Time: 1m 25s\n",
            "\tTrain Loss: 1.401 | Train PPL:   4.058\n",
            "\t Val. Loss: 3.841 |  Val. PPL:  46.595\n",
            "Epoch: 28 | Time: 1m 25s\n",
            "\tTrain Loss: 1.420 | Train PPL:   4.137\n",
            "\t Val. Loss: 3.873 |  Val. PPL:  48.095\n",
            "Epoch: 29 | Time: 1m 25s\n",
            "\tTrain Loss: 1.393 | Train PPL:   4.027\n",
            "\t Val. Loss: 3.826 |  Val. PPL:  45.858\n",
            "Epoch: 30 | Time: 1m 24s\n",
            "\tTrain Loss: 1.401 | Train PPL:   4.061\n",
            "\t Val. Loss: 3.830 |  Val. PPL:  46.080\n",
            "Epoch: 31 | Time: 1m 24s\n",
            "\tTrain Loss: 1.365 | Train PPL:   3.915\n",
            "\t Val. Loss: 3.849 |  Val. PPL:  46.953\n",
            "Epoch: 32 | Time: 1m 25s\n",
            "\tTrain Loss: 1.349 | Train PPL:   3.855\n",
            "\t Val. Loss: 3.905 |  Val. PPL:  49.670\n",
            "Epoch: 33 | Time: 1m 25s\n",
            "\tTrain Loss: 1.366 | Train PPL:   3.918\n",
            "\t Val. Loss: 3.887 |  Val. PPL:  48.786\n",
            "Epoch: 34 | Time: 1m 25s\n",
            "\tTrain Loss: 1.338 | Train PPL:   3.812\n",
            "\t Val. Loss: 3.913 |  Val. PPL:  50.048\n",
            "Epoch: 35 | Time: 1m 25s\n",
            "\tTrain Loss: 1.319 | Train PPL:   3.741\n",
            "\t Val. Loss: 3.897 |  Val. PPL:  49.242\n",
            "Epoch: 36 | Time: 1m 25s\n",
            "\tTrain Loss: 1.306 | Train PPL:   3.693\n",
            "\t Val. Loss: 3.893 |  Val. PPL:  49.048\n",
            "Epoch: 37 | Time: 1m 25s\n",
            "\tTrain Loss: 1.284 | Train PPL:   3.613\n",
            "\t Val. Loss: 3.912 |  Val. PPL:  50.008\n",
            "Epoch: 38 | Time: 1m 24s\n",
            "\tTrain Loss: 1.284 | Train PPL:   3.612\n",
            "\t Val. Loss: 3.925 |  Val. PPL:  50.656\n",
            "Epoch: 39 | Time: 1m 24s\n",
            "\tTrain Loss: 1.288 | Train PPL:   3.627\n",
            "\t Val. Loss: 3.959 |  Val. PPL:  52.410\n",
            "Epoch: 40 | Time: 1m 25s\n",
            "\tTrain Loss: 1.272 | Train PPL:   3.569\n",
            "\t Val. Loss: 3.940 |  Val. PPL:  51.420\n",
            "Epoch: 41 | Time: 1m 24s\n",
            "\tTrain Loss: 1.264 | Train PPL:   3.538\n",
            "\t Val. Loss: 3.905 |  Val. PPL:  49.634\n",
            "Epoch: 42 | Time: 1m 25s\n",
            "\tTrain Loss: 1.252 | Train PPL:   3.496\n",
            "\t Val. Loss: 3.976 |  Val. PPL:  53.282\n",
            "Epoch: 43 | Time: 1m 25s\n",
            "\tTrain Loss: 1.234 | Train PPL:   3.436\n",
            "\t Val. Loss: 4.000 |  Val. PPL:  54.592\n",
            "Epoch: 44 | Time: 1m 24s\n",
            "\tTrain Loss: 1.234 | Train PPL:   3.434\n",
            "\t Val. Loss: 3.996 |  Val. PPL:  54.376\n",
            "Epoch: 45 | Time: 1m 24s\n",
            "\tTrain Loss: 1.236 | Train PPL:   3.442\n",
            "\t Val. Loss: 3.992 |  Val. PPL:  54.149\n",
            "Epoch: 46 | Time: 1m 24s\n",
            "\tTrain Loss: 1.226 | Train PPL:   3.408\n",
            "\t Val. Loss: 4.024 |  Val. PPL:  55.932\n",
            "Epoch: 47 | Time: 1m 24s\n",
            "\tTrain Loss: 1.209 | Train PPL:   3.349\n",
            "\t Val. Loss: 3.984 |  Val. PPL:  53.732\n",
            "Epoch: 48 | Time: 1m 24s\n",
            "\tTrain Loss: 1.215 | Train PPL:   3.371\n",
            "\t Val. Loss: 4.011 |  Val. PPL:  55.183\n",
            "Epoch: 49 | Time: 1m 23s\n",
            "\tTrain Loss: 1.189 | Train PPL:   3.283\n",
            "\t Val. Loss: 4.057 |  Val. PPL:  57.774\n",
            "Epoch: 50 | Time: 1m 25s\n",
            "\tTrain Loss: 1.206 | Train PPL:   3.339\n",
            "\t Val. Loss: 4.057 |  Val. PPL:  57.805\n",
            "| Test Loss: 4.073 | Test PPL:  58.708 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MPC8pElfI-2"
      },
      "source": [
        "Добейтесь перплексии на валидационной выборке порядка 35-40. Попробуйте различные значения параметров сети и learning rate."
      ]
    }
  ]
}