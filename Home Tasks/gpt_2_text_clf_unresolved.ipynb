{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "gpt_2_text_clf_unresolved.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "04SFZ3Wo0exC"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f76e72fccfab418d8593fff6d4058e2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_63f68546caa14e8d8b4623e0426ba121",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8346ba5775c348eda2f9504995b28107",
              "IPY_MODEL_3fa4358942a04c369a907e00a97a8da0",
              "IPY_MODEL_7b122e4eab644216b583397745ee2930"
            ]
          }
        },
        "63f68546caa14e8d8b4623e0426ba121": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8346ba5775c348eda2f9504995b28107": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8b6973a861f241d1a6f71297d751dd40",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2ff05483d5e2496f8961b58c30b918d6"
          }
        },
        "3fa4358942a04c369a907e00a97a8da0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_01d3c27d045e4421b9d0edc8878d1115",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 718,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 718,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5cd704a2a8634872ab89eb5d01cbd1e3"
          }
        },
        "7b122e4eab644216b583397745ee2930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e476e8c22f274ad28c7e3b162d093ff7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 718/718 [00:00&lt;00:00, 16.6kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1bd6e1c71cb64b67857afbc41ac2c674"
          }
        },
        "8b6973a861f241d1a6f71297d751dd40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2ff05483d5e2496f8961b58c30b918d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "01d3c27d045e4421b9d0edc8878d1115": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5cd704a2a8634872ab89eb5d01cbd1e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e476e8c22f274ad28c7e3b162d093ff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1bd6e1c71cb64b67857afbc41ac2c674": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5bc6bc532e6b416e8d33ff1bde63c138": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d9ea049bbd4e44139a74b398a839a0a9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_42529c4be6b64dc5a68a083e1f6c3b6c",
              "IPY_MODEL_24c08cc5b90747099749c9b01c945aac",
              "IPY_MODEL_59f613229e4b4dcdbfb14e2dc2daee4f"
            ]
          }
        },
        "d9ea049bbd4e44139a74b398a839a0a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "42529c4be6b64dc5a68a083e1f6c3b6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_245064cd92074711876fdd915f48da56",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_120bda7570224ac1a5d1e7ec842106e2"
          }
        },
        "24c08cc5b90747099749c9b01c945aac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e0892ca421bb45d98f0cf0402b0172d3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1042301,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1042301,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_765f187e694a4847980cdd11bdf379cd"
          }
        },
        "59f613229e4b4dcdbfb14e2dc2daee4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7dbb588406064e598399f14ae3c1eef4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 732kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a91bed4f69dd459696f5d95aa853d2ec"
          }
        },
        "245064cd92074711876fdd915f48da56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "120bda7570224ac1a5d1e7ec842106e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e0892ca421bb45d98f0cf0402b0172d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "765f187e694a4847980cdd11bdf379cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7dbb588406064e598399f14ae3c1eef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a91bed4f69dd459696f5d95aa853d2ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b22a56594da041a18604682e61565f1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d6f8790b4cd14170a326db07d4f5f8d0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_66131500fadd49f898c9c2b60f0b11ab",
              "IPY_MODEL_8cf6801011994a1290c84cd6fd8c24c0",
              "IPY_MODEL_e0ecbf095f83414aa880440aacc33a0d"
            ]
          }
        },
        "d6f8790b4cd14170a326db07d4f5f8d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "66131500fadd49f898c9c2b60f0b11ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d15c37a57079415597117e03bc1de61a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6d9ff048a86c4510a049d3492a99232a"
          }
        },
        "8cf6801011994a1290c84cd6fd8c24c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3526952314a84ed4ab38485d9069c8a6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ab4a4f32fd3d4f4e98aee0ee43c4eeb9"
          }
        },
        "e0ecbf095f83414aa880440aacc33a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a5c287247bdd4454ac19cc6e0a5c9017",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 1.04MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_49ed5ea6cd70458ea75e2418f09e5df3"
          }
        },
        "d15c37a57079415597117e03bc1de61a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6d9ff048a86c4510a049d3492a99232a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3526952314a84ed4ab38485d9069c8a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ab4a4f32fd3d4f4e98aee0ee43c4eeb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a5c287247bdd4454ac19cc6e0a5c9017": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "49ed5ea6cd70458ea75e2418f09e5df3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4f06d5863e8744bcbb78b93fb84e4c2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a608d8a7d2824349a9df3e66ef70641d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a7ef244a4f164b7c8108ffb161c56561",
              "IPY_MODEL_b8d52ba1b3ee42ddbc4b8215bb92d343",
              "IPY_MODEL_d0aaf7b3ec55453ea5dcfc92c75938b8"
            ]
          }
        },
        "a608d8a7d2824349a9df3e66ef70641d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a7ef244a4f164b7c8108ffb161c56561": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bd28c2213b2d466a912becf29e0505d0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a62ada280f3d4142896c56acf0fdec38"
          }
        },
        "b8d52ba1b3ee42ddbc4b8215bb92d343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a8a02ee545a845b9b474fafddbbb88c0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d705194fd7384ca39a701a8a53912415"
          }
        },
        "d0aaf7b3ec55453ea5dcfc92c75938b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7f117f670e4b46f8b198869b31adff2f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3/3 [00:00&lt;00:00, 60.78it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_415f9e29b00e4ac785d98d4ded1422ac"
          }
        },
        "bd28c2213b2d466a912becf29e0505d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a62ada280f3d4142896c56acf0fdec38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a8a02ee545a845b9b474fafddbbb88c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d705194fd7384ca39a701a8a53912415": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7f117f670e4b46f8b198869b31adff2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "415f9e29b00e4ac785d98d4ded1422ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/delhian/NLP_course/blob/master/Home%20Tasks/gpt_2_text_clf_unresolved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd5wsNhS0ewB"
      },
      "source": [
        "## Возможности и ограничения языковых моделей"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9YkOa3Jwbl-",
        "outputId": "931a3ecb-61d4-438e-e62b-33c6ba50a982"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dSYyxBI0ewG"
      },
      "source": [
        "**Тут есть важное действие, которое нужно сделать перед тем, как запускать этот ноутбук**\n",
        "\n",
        "Если вы делаете это задание в Google Colab, первым делом переключите Runtime в GPU. Это задание нормально посчитается и на CPU, но некоторые из будущих кейсов потребуют GPU (либо вам придётся несколько часов или даже дней, чтобы модель обучилась - и мы не преувиличиваем). Ещё мы рекомендуем переключить язык интерфейса на английский, потому что русская локализация ужасна да и вообще не нужна.\n",
        "\n",
        "О том, как переключить рантайм: https://www.geeksforgeeks.org/how-to-use-google-colab/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqRGVCt50ewL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae145caf-6e7e-4ef9-be6e-945a05faf2ff"
      },
      "source": [
        "!pip install datasets transformers[torch]==2.10.0 tqdm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.13.3-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting transformers[torch]==2.10.0\n",
            "  Downloading transformers-2.10.0-py3-none-any.whl (660 kB)\n",
            "\u001b[K     |████████████████████████████████| 660 kB 41.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (3.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 38.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (1.19.5)\n",
            "Collecting tokenizers==0.7.0\n",
            "  Downloading tokenizers-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 13.8 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 45.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (1.9.0+cu111)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 58.2 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 48.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Collecting huggingface-hub<0.1.0,>=0.0.19\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 35.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.13)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[torch]==2.10.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[torch]==2.10.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[torch]==2.10.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[torch]==2.10.0) (1.24.3)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 52.9 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 54.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[torch]==2.10.0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[torch]==2.10.0) (7.1.2)\n",
            "Installing collected packages: multidict, yarl, async-timeout, tokenizers, sentencepiece, sacremoses, fsspec, aiohttp, xxhash, transformers, huggingface-hub, datasets\n",
            "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.13.3 fsspec-2021.10.1 huggingface-hub-0.0.19 multidict-5.2.0 sacremoses-0.0.46 sentencepiece-0.1.96 tokenizers-0.7.0 transformers-2.10.0 xxhash-2.0.2 yarl-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcr7RqWl0ewN"
      },
      "source": [
        "from transformers import AutoModel, AutoTokenizer, GPT2LMHeadModel\n",
        "import datasets\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "import warnings"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJvskQ8T0ewO"
      },
      "source": [
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diaudnwY0ewQ"
      },
      "source": [
        "# import transformers\n",
        "# print(transformers.__version__)  # should be above or equal 4.0.1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVYGHVGT0ewS"
      },
      "source": [
        "В этом модуле мы с вами познакомились с большими языковыми моделями и обсудили их возможности, а также ограничения. В этом задании вам предлагается поэкспериментировать с одной из больших языковых моделей -- GPT-2 -- и самим убедиться, так ли она хороша, как о ней рассказывают."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXk55qyo0ewT"
      },
      "source": [
        "## Библиотека transformers\n",
        "\n",
        "Вы уже сталкивались с библиотекой transformers в этом курсе.\n",
        "\n",
        "Мы [загрузим gpt-2](https://huggingface.co/transformers/model_doc/gpt2.html) с помощью метода from_pretrained. Так как мы будем использовать её для задачи языкового моделирования, нам потребуется GPT2LMHeadModel, прочитать про неё можно [тут](https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel). У GPT-2 свой токенизатор, про него можно почитать [тут](https://huggingface.co/transformers/model_doc/gpt2.html#gpt2tokenizer).\n",
        "\n",
        "GPT-2 -- большая модель, время ее работы на CPU будет слишком большим, так что мы сразу отправим её на GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq9fHovY0ewU"
      },
      "source": [
        "model_name = 'gpt2-medium'\n",
        "device = 'cuda'"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVJ-USzH0ewW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "f76e72fccfab418d8593fff6d4058e2b",
            "63f68546caa14e8d8b4623e0426ba121",
            "8346ba5775c348eda2f9504995b28107",
            "3fa4358942a04c369a907e00a97a8da0",
            "7b122e4eab644216b583397745ee2930",
            "8b6973a861f241d1a6f71297d751dd40",
            "2ff05483d5e2496f8961b58c30b918d6",
            "01d3c27d045e4421b9d0edc8878d1115",
            "5cd704a2a8634872ab89eb5d01cbd1e3",
            "e476e8c22f274ad28c7e3b162d093ff7",
            "1bd6e1c71cb64b67857afbc41ac2c674",
            "5bc6bc532e6b416e8d33ff1bde63c138",
            "d9ea049bbd4e44139a74b398a839a0a9",
            "42529c4be6b64dc5a68a083e1f6c3b6c",
            "24c08cc5b90747099749c9b01c945aac",
            "59f613229e4b4dcdbfb14e2dc2daee4f",
            "245064cd92074711876fdd915f48da56",
            "120bda7570224ac1a5d1e7ec842106e2",
            "e0892ca421bb45d98f0cf0402b0172d3",
            "765f187e694a4847980cdd11bdf379cd",
            "7dbb588406064e598399f14ae3c1eef4",
            "a91bed4f69dd459696f5d95aa853d2ec",
            "b22a56594da041a18604682e61565f1a",
            "d6f8790b4cd14170a326db07d4f5f8d0",
            "66131500fadd49f898c9c2b60f0b11ab",
            "8cf6801011994a1290c84cd6fd8c24c0",
            "e0ecbf095f83414aa880440aacc33a0d",
            "d15c37a57079415597117e03bc1de61a",
            "6d9ff048a86c4510a049d3492a99232a",
            "3526952314a84ed4ab38485d9069c8a6",
            "ab4a4f32fd3d4f4e98aee0ee43c4eeb9",
            "a5c287247bdd4454ac19cc6e0a5c9017",
            "49ed5ea6cd70458ea75e2418f09e5df3"
          ]
        },
        "outputId": "e07fdc0b-e1db-49dd-e763-1e3db412d945"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f76e72fccfab418d8593fff6d4058e2b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/718 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5bc6bc532e6b416e8d33ff1bde63c138",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b22a56594da041a18604682e61565f1a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTXvTSFN0ewZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6aed55b-e8da-44fc-be75-7084e10aa1b5"
      },
      "source": [
        "%%time\n",
        "# model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/gpt2-medium')\n",
        "model = model.to(device)"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 10.6 s, sys: 1.68 s, total: 12.3 s\n",
            "Wall time: 46 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJispE3M0ewa"
      },
      "source": [
        "### Подсчёт числа параметров модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asygaKxt0ewb"
      },
      "source": [
        "Чтобы понять, насколько GPT большая модель, подсчитаем число ее параметров"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrKJmIHo0ewc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9593b4e5-903f-4532-f53c-f7b3a1c197b0"
      },
      "source": [
        "# TASK: compute number of model parameters\n",
        "# iterate over model weights and count number of parameters in each of them, then sum it up\n",
        "# note: you may find model.parameters() method useful\n",
        "# Our implementation is 2 lines\n",
        "\n",
        "params = sum([params.numel() for params in model.parameters() ])\n",
        "# YOUR CODE STARTS\n",
        "\n",
        "# YOUR CODE ENDS\n",
        "params"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "354823168"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHOJXyah0ewd"
      },
      "source": [
        "Как видим, это большое число параметров, на несколько порядок больше, чем те модели, которые вы обучали до этого! Использовать такое в продакшене как правило невозможно из-за больших требований по ресурсам"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1-PmKrq0ewd"
      },
      "source": [
        "# Часть 1. Генерация отзывов моделью GPT\n",
        "\n",
        "В преыдущем задании мы генерировали ровно 1 токен, так как нам нужно было предсказать сентимент, а оба слова \"positive\" и \"negative\" есть в словаре GPT.\n",
        "\n",
        "В этом задании мы пойдём дальше и самостоятельно реализуем top-k сэмплирование, чтобы генерировать отзывы на фильмы, подобные тем, что в датасете IMDB.\n",
        "\n",
        "Мы будем продолжать отзывы, используя в качестве \"затравки\" для модели начало реальных ревью."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUnAhWXm0ewd"
      },
      "source": [
        "### Top-k & Top-p sampling\n",
        "\n",
        "Чтобы генерация текста была более разнообразной, мы будем использовать top k сэмплирование, которое мы изучали в этом юните. Его смысл в том, что на каждом шаге мы перед сэмплированием зануляем вероятности всех слов, кроме k самых вероятных."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T4TM0EL0ewf"
      },
      "source": [
        "def topk_sample(scores, k):\n",
        "    \"\"\"\n",
        "    Sample from logits using multinomial distribution\n",
        "    Before sampling, all logits except k greatest are zeroed out.\n",
        "    Args:\n",
        "        scores: scores for every word in the vocabulary. Must be 1-dimensional: (num_words_in_vocab)\n",
        "        k: int, how many hypotheses to sample from\n",
        "    Returns:\n",
        "        1 draw from dictribution, torch.LongTensor of shape (1)\n",
        "    \"\"\"\n",
        "    assert k >= 1, 'k must be >=1'\n",
        "    assert scores.ndim == 1, 'logits must have 1 dimension' \n",
        "\n",
        "    # TASK: implement top-k sampling\n",
        "    # First, get top k values and theoir indices using torch.topk\n",
        "    # 2nd, fill logits with some small value (e. g. 1e-6)\n",
        "    # Next, move values back to their place. Note that you will find ellipsis (...) and None useful\n",
        "    # Finally, use softmax to obtain probabilities and get \n",
        "    # Our implementation is 6 lines\n",
        "    \n",
        "    # YOUR CODE STARTS\n",
        "    values, indexes = scores.topk(k)\n",
        "    scores = scores.fill_(1e-6)\n",
        "    scores[indexes] = values\n",
        "    predicted_ids = F.softmax(scores)\n",
        "    # YOUR CODE ENDS\n",
        "    return predicted_ids"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvm41e190ewf"
      },
      "source": [
        "Проверка кода:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFpPyxFt0ewg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25a48d32-5cc1-4b3f-9cb9-b21e9d785471"
      },
      "source": [
        "logits = torch.randn(5)\n",
        "for _ in range(10):\n",
        "    res1 = topk_sample(logits, 1)\n",
        "    res2  = topk_sample(logits, 1)\n",
        "    assert res1.ndim == 1\n",
        "    assert res1.equal(res2)\n",
        "equal = True\n",
        "for _ in range(10):\n",
        "    if not topk_sample(logits, 2).equal(topk_sample(logits, 2)):\n",
        "        equal = False\n",
        "assert equal\n",
        "print('OK!')"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2VBo0eotL6Z"
      },
      "source": [
        "Также мы будем использовать top-p sampling, который мы уже проходили на лекциях."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ8divSy0ewh"
      },
      "source": [
        "def topp_sample(scores, p):\n",
        "    \"\"\"\n",
        "    Sample from logits using multinomial distribution\n",
        "    Before sampling, all logits except those largest\n",
        "    whose cumsum exceeds p are zeroed out\n",
        "    Args:\n",
        "        scores: scores for every word in the vocabulary. Must be 1-dimensional: (num_words_in_vocab)\n",
        "        p: float, cumulative probability of most high-scored tokens\n",
        "    Returns:\n",
        "        1 draw from dictribution, torch.LongTensor of shape (1)\n",
        "    \"\"\"\n",
        "    assert  0 < p < 1, 'p must be between 0 and 1'\n",
        "    assert scores.ndim == 1, 'logits must have 1 dimension' \n",
        "\n",
        "    # TASK: implement top-p sampling\n",
        "    # 1. sort scores with descending order\n",
        "\n",
        "    values, indexes = scores.sort(descending = True)\n",
        "    # 2. get cumulative sum of softmaxed logits\n",
        "\n",
        "    values_cumsum = torch.cumsum(values, dim =0 )\n",
        "\n",
        "    # 3. get indices when cumulative sum is larger than p. These indices should be zeroed out\n",
        "\n",
        "    scores[indexes[values_cumsum > p]] = 1e-6\n",
        "\n",
        "    # 4. get real indices that should be zeroes out from sorted indices. Use tensor slicing\n",
        "    # 5. fill these indices with some small value (e. g. -1e6)\n",
        "    # 6. sample, as you did in top-k sampling\n",
        "    # Our implementation is 7 lines\n",
        "    \n",
        "    # YOUR CODE STARTS\n",
        "    predicted_ids = F.softmax(scores)\n",
        "    # YOUR CODE ENDS\n",
        "    return predicted_ids"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-0t8dHgtN3N"
      },
      "source": [
        "Проверка кода:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrDbJBlc0ewi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59af43c8-6464-4bb4-cae9-3d5685ec310d"
      },
      "source": [
        "logits = torch.Tensor([-.1, -.2, .9])\n",
        "for _ in range(10):\n",
        "    res1 = topp_sample(logits, 0.8)\n",
        "    res2  = topp_sample(logits, 0.8)\n",
        "    assert res1.ndim == 1\n",
        "\n",
        "    assert res1.equal(res2)\n",
        "logits = torch.Tensor([.5, .5, .5])\n",
        "equal = True\n",
        "for _ in range(10):\n",
        "    if not topp_sample(logits, 0.8).equal(topp_sample(logits, 0.8)):\n",
        "        equal = False\n",
        "assert equal\n",
        "print('OK!')"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgqFirKjtPrz"
      },
      "source": [
        "Теперь напишите функцию генерации текста:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sNwScYY0ewj"
      },
      "source": [
        "def generate_text(model, ids, length, k=None, p=None):\n",
        "    \"\"\"\n",
        "    Generate text with language model with ids as prompt\n",
        "    Args:\n",
        "        model: huggingface LM model\n",
        "        ids: input ids, from tokenizer.encode, must be 2-dimensional\n",
        "        length: int, how many tokens to geenrate\n",
        "        k: int, how many hypotheses to sample from in top-k sampling\n",
        "    Returns:\n",
        "        token ids from generated text, together with token ids of prompts, 2d LongTensor\n",
        "    \"\"\"\n",
        "    assert length >= 1\n",
        "    if k and p:\n",
        "        raise RuntimeError('Cannot use topk and topp sampling simultaneously')\n",
        "    # TASK: write generation loop\n",
        "    # For every timestamp:\n",
        "    # - obtain model output\n",
        "    # - get logits for last symbol\n",
        "    # - apply topk sampling to get new index\n",
        "    # - concatenate it to the ids to form new input\n",
        "    # in the end, the `ids` tensor will have full generation\n",
        "    # our implementation is 5 lines\n",
        "    \n",
        "    # YOUR CODE STARTS\n",
        "\n",
        "    ids = model.generate(\n",
        "        ids.reshape(-1, ids.shape[0]), \n",
        "        do_sample=True, \n",
        "        max_length=ids.shape[0] + length, \n",
        "        repetition_penalty=1.2, \n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        top_k = k,\n",
        "        top_p = p\n",
        "    )\n",
        "\n",
        "    # YOUR CODE ENDS\n",
        "    return ids.reshape(ids.shape[1]).cpu().detach()"
      ],
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB6g8UWp0ewj"
      },
      "source": [
        "Проверка кода:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8mpDF_O0ewk"
      },
      "source": [
        "ids = torch.randint(0, 1000, (4,)).to(device)\n",
        "generation = generate_text(model, ids, 4, k=2)\n",
        "\n",
        "assert generation.shape == torch.Size([8])\n",
        "\n",
        "generation = generate_text(model, ids, 3, p=.5)\n",
        "\n",
        "assert generation.shape == torch.Size([7])"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5uhuTT30ewl"
      },
      "source": [
        "### Запускаем генерацию\n",
        "\n",
        "Зададим модели какой-нибудь prompt, например восторженный отзыв о фильме, и посмотрим, как она продолжит его. Так как модель сэмплит на каждом шаге, нам интересно посмотреть на несколько траекторий сразу. Для этого мы и писали батч-режим в генерации."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jntDAgA90ewm"
      },
      "source": [
        "prompt = 'What a lovely film !'"
      ],
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_OBJte00ewm"
      },
      "source": [
        "Подготовим входные данные для GPT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSwU95VV0ewn"
      },
      "source": [
        "ids = tokenizer.encode(prompt, return_tensors=\"pt\").squeeze().to(device)"
      ],
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tew84_O80ewo"
      },
      "source": [
        "Запускаем генерацию! Можно запустить на 10-20 токенов, можно и больше"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSHTNksk0ewo"
      },
      "source": [
        "n_tries = 5"
      ],
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKRdXJYA0ewo"
      },
      "source": [
        "generations = [generate_text(model, ids, length=40, k=10) for _ in range(n_tries)]"
      ],
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq0AoI_50ewp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b36224f-e298-4df1-ed00-f5f40f58f607"
      },
      "source": [
        "for generation in generations:\n",
        "    print(tokenizer.decode(generation))\n",
        "    print('\\n' + '=' * 40 + '\\n')"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What a lovely film!\n",
            "I hope you liked it and have enjoyed my first ever blogpost. If not, I am looking for your feedback on the post as we are in need of more comments! Please feel free to\n",
            "\n",
            "========================================\n",
            "\n",
            "What a lovely film! It is not only the best, the most beautiful and also one of my favorites!\n",
            "\n",
            "I was so surprised how well written everything about this movie really is. The characters are all wonderful to watch\n",
            "\n",
            "========================================\n",
            "\n",
            "What a lovely film!\n",
            "It has all the right ingredients and is perfect for any movie night. It's also very funny at times too so if you are looking to laugh, it might be something that interests your friends as\n",
            "\n",
            "========================================\n",
            "\n",
            "What a lovely film! I was in the UK at that time and had to get it for my friend, so here is his copy :D This review may be edited by me if needed.\n",
            "<|endoftext|>\n",
            "\n",
            "========================================\n",
            "\n",
            "What a lovely film! It's one of those movies that makes you realise there are many things in life, and the only ones we're all born with.\n",
            "\n",
            "In this movie they take me on an epic journey across\n",
            "\n",
            "========================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHM7qBeg0ewr"
      },
      "source": [
        "А теперь запустим генерацию с top-p сэмплированием"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ4uzsOW0ewr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f607751d-cba5-4e8e-a416-1ce13086a616"
      },
      "source": [
        "generations = [generate_text(model, ids, length=40, p=0.5).cpu().detach() for _ in range(n_tries)]\n",
        "for generation in generations:\n",
        "    print(tokenizer.decode(generation))\n",
        "    print('\\n' + '=' * 40 + '\\n')"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What a lovely film! The best thing about it is that you can watch the movie in multiple ways. You may want to see all three of them, but I recommend watching one at once and then picking up where we left\n",
            "\n",
            "========================================\n",
            "\n",
            "What a lovely film! It is very much about the life of Jesus Christ. The script was written by an American writer and directed in English with great care, it's beautiful,the actors are outstanding. I would recommend this\n",
            "\n",
            "========================================\n",
            "\n",
            "What a lovely film!\n",
            "\"You have no idea what I feel for you, my dear.\" - The Princess Bride \"I'm not so sure that's the right word to use. She doesn't seem like an ordinary\n",
            "\n",
            "========================================\n",
            "\n",
            "What a lovely film! The characters are wonderful and the story is interesting. It's like watching an old friend from childhood, with whom you've had many adventures together for years now. And if I could give it five stars\n",
            "\n",
            "========================================\n",
            "\n",
            "What a lovely film!\n",
            "I have seen this movie many times in my life. I remember when it first came out and thought that the story was great but not enough of an explanation as to why, so we went back\n",
            "\n",
            "========================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isQrAwqG0ews"
      },
      "source": [
        "# Часть 2. Few-shot learning\n",
        "\n",
        "Одной из особенностей GPT, про которую авторы написали, является способность к few-shot learning. В этой части домашнего задания мы на примере хорошо знакомой нам задачи классификации текстов исследуем, как хорошо модель умеет различать сентимент отзывов без дополнительного обучения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2osAS3f0ews"
      },
      "source": [
        "## Датасет.\n",
        "Мы будем снова использовать датасет imdb, на котором мы уже обучили несколько моделей. Так как обучать саму модель мы не будем (в few-shot парадигме веса не меняются), то можем сразу взять валидационную часть датасета."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQpgFvhq0ewt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "4f06d5863e8744bcbb78b93fb84e4c2c",
            "a608d8a7d2824349a9df3e66ef70641d",
            "a7ef244a4f164b7c8108ffb161c56561",
            "b8d52ba1b3ee42ddbc4b8215bb92d343",
            "d0aaf7b3ec55453ea5dcfc92c75938b8",
            "bd28c2213b2d466a912becf29e0505d0",
            "a62ada280f3d4142896c56acf0fdec38",
            "a8a02ee545a845b9b474fafddbbb88c0",
            "d705194fd7384ca39a701a8a53912415",
            "7f117f670e4b46f8b198869b31adff2f",
            "415f9e29b00e4ac785d98d4ded1422ac"
          ]
        },
        "outputId": "cbfd528d-00d6-4a07-8419-a252c083cc50"
      },
      "source": [
        "text_dataset = datasets.load_dataset(\"imdb\")\n",
        "texts = text_dataset[\"test\"][\"text\"]\n",
        "labels = text_dataset[\"test\"][\"label\"]"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f06d5863e8744bcbb78b93fb84e4c2c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DPlm7ad0ewu"
      },
      "source": [
        "### Метки для задачи классификации на естественном языке\n",
        "Поскольку языковая модель знает только токены языка и не знает маппинга между индексами и лейблами, нам нужно будет сравнивать лейблы напрямую. С одной стороны, это создаёт дополнительные сложности: модель может предсказать \"Positive Sentiment\" вместо \"positive\", поэтому нам потребуется минимальный постпроцессинг, чтобы учесть большинство таких случаев."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7qRUd-X0ewu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b1fb41-51b3-4224-9bfd-3042b2870db7"
      },
      "source": [
        "mapping = {idx: label for idx, label in enumerate(text_dataset[\"train\"].features[\"label\"].names)}\n",
        "mapping"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'neg', 1: 'pos'}"
            ]
          },
          "metadata": {},
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmkvUl1W0ewv"
      },
      "source": [
        "### Игрушечный датасет\n",
        "Чтобы быстрее прототипировать наши вводы (prompts), полезно взять маленькую часть датасета и проверять гипотезы на ней. Так как imdb упорядочен по лейблам, а самих лейблов всего 2, мы берём одинаковое количество примеров с начала и с конца."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUrFRggy0ewv"
      },
      "source": [
        "def get_toy_dataset(dataset, split, length = 100):\n",
        "    texts = dataset[split][\"text\"]\n",
        "    labels = dataset[split][\"label\"]\n",
        "    small_texts = texts[:length // 2] + texts[-length//2:]\n",
        "    small_labels = labels[:length // 2] + labels[-length//2:]\n",
        "    return small_texts, small_labels"
      ],
      "execution_count": 413,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9gjyJGQ0eww"
      },
      "source": [
        "assert len(get_toy_dataset(text_dataset, \"test\", 100)[0]) == 100"
      ],
      "execution_count": 414,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQovzfQu0eww"
      },
      "source": [
        "## Few-shot learning\n",
        "\n",
        "В данном задании мы будем не обучать модель на данных, а использовать уже обученную на огромном корпусе языковую модель, чтобы \"угадывать\" сентимент отзывов.\n",
        "\n",
        "Модель будет видеть в качестве затравки (prompt) описание задачи на естественном языке.\n",
        "\n",
        "Нам потребуется:\n",
        "- описание задачи\n",
        "- лейблы\n",
        "- несколько (хотя бы 3) примеров\n",
        "- вспомогательные маркеры (разделитель примеров и маркер конца входа)\n",
        "\n",
        "Например:\n",
        "\n",
        "```\n",
        "Translate from English to French\n",
        "sea otter => loutre de mer\n",
        "peppermint => menthe poivrée\n",
        "plush girafe  => \n",
        "```\n",
        "\n",
        "Первая строка здесь -- описание задачи. Мы решаем задачу классификации, поэтому нам нужно\n",
        "описать задачу примерно как `To which category does the text belong? positive , negative `\n",
        "\n",
        "Вторая и третья строки -- примеры входа и выхода. В нашем случае примерами входа и выхода будут служить тексты ревью и метки классов positive и negative (а не 0 и 1, как раньше).\n",
        "\n",
        "Наконец, последняя строка -- это тот текст, который модель должна классифицировать. Модель уже \"видела\" выше пример того, что после маркера конца входа идёт сентимент, и мы ожидаем, что она сможет выучить эту закономерность.\n",
        "\n",
        "В данном задании часть входа будет написана за вас, вам надо будет придумать обучающие примеры для few-shot learning'а"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZQgn9uG0ewx"
      },
      "source": [
        "marker = ' => '\n",
        "separator = '\\n'"
      ],
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFxS1Qnp0ewx"
      },
      "source": [
        "labels = [\"positive\" , \"negative\"]\n",
        "labels_text = \" , \".join(labels)\n",
        "task_description = f'To which category does the text belong?: {labels_text} ' + separator"
      ],
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu1Uwvlm-dEZ"
      },
      "source": [
        "lbl = text_dataset['train']['label']\n",
        "texts =  text_dataset['train']['text']"
      ],
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3XDvioT_Lxe"
      },
      "source": [
        "import random"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj9_4rok92rI"
      },
      "source": [
        "import random\n",
        "\n",
        "noex = 5\n",
        "random_negative = random.sample(texts[12500: 24999], noex)\n",
        "random_positive = random.sample(texts[0: 12499], noex)"
      ],
      "execution_count": 545,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-Z-wR-k0ewy"
      },
      "source": [
        "examples = [\n",
        "    # TASK define examples\n",
        "    # They should be a tuple with text and label\n",
        "    # label should be from \"labels\"\n",
        "    # Get 3-10 different examples, they should cover both positive \n",
        "    # and negative reviews\n",
        "    # YOUR CODE STARTS\n",
        "(' '.join(x.split(' ')[:30]), \"negative\") for x in random_negative] + [(' '.join(x.split(' ')[:30]), \"positive\") for x in random_positive\n",
        "    # YOUR CODE ENDS\n",
        "]"
      ],
      "execution_count": 546,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aso2fG6_0ewz"
      },
      "source": [
        "assert len(examples) > 1, 'Write at least two different examples'\n",
        "for ex in examples:\n",
        "    assert ex[1] in labels"
      ],
      "execution_count": 547,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Heu6rvlK0ewz"
      },
      "source": [
        "input = task_description  + separator.join([marker.join(example) for example in examples])"
      ],
      "execution_count": 548,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buEhQeHo0ew1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3b4b8ad-c51d-496d-8046-43d60667ab2f"
      },
      "source": [
        "print(input)"
      ],
      "execution_count": 549,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To which category does the text belong?: positive , negative \n",
            "R O B O T J O X.<br /><br />Burn the master.<br /><br />Grotesquely horrible.<br /><br />No ending; no closure.<br /><br />Completely and utterly the worst movie ever made.<br /><br => negative\n",
            "This is by far the worst movie I have ever seen in the cinema!! Could not wait for it to end. To make matters worse it is given a 12A => negative\n",
            "I believe that war films should try to convey the terror of war, avoid idealism and respect some rudimentary military principles. Zvezda barely does the first. Zvezda being a Russian => negative\n",
            "This had high intellectual pretensions.The main lead intends to give a \"deep\" \"meaningful\" rendering(with voice over for his frames of mind naturally) and he was certainly influenced by the fifties/sixties => negative\n",
            "\"The Cobweb\" is an example of many examples of movies that feature strong, sometimes noteworthy performances and high points, but unfortunately are shattered and slowed down drastically by a murky => negative\n",
            "I'm 14 years old and I love this cartoon. Burt Reynolds and Dom Deluise make a great pair. This movie is really funny and I love the songs. My favorite => positive\n",
            "I saw this movie after i saw Blue Crush and other of Michelle's movies, i thought she had a bleak future in this business.I was extremely wrong after watching her => positive\n",
            "This film is very interesting. I have seen it twice and it seems Glover hit the nail on the head with what he claims to he wants to accomplish. I => positive\n",
            "Jean-Jacques' career began with his essay answer to a prize question: civilization makes us evil. This intelligent and exciting movie supports that argument. In that sense it repeats a theme => positive\n",
            "The appeal of ancient films like this one is that you get to see an actual moving image of life over 100 years ago. Here are a lot of people => positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIHpblK80ew2"
      },
      "source": [
        "Сравните получившийся инпут с примером выше. Мы получили то, что надо!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akx9zMwFbKdY"
      },
      "source": [
        "n_chars = 50"
      ],
      "execution_count": 550,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWfxK702bJKp"
      },
      "source": [
        "def get_prompt(beginning, separator, text, marker) -> str:\n",
        "    return ' '.join([input,  separator, text[:n_chars] + text[-n_chars:], marker])"
      ],
      "execution_count": 551,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmQrc0fs0ew5"
      },
      "source": [
        "## Время проверить работу нашей модели на маленьком датасете!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_0WPdws0ew5"
      },
      "source": [
        "small_texts, small_labels = get_toy_dataset(text_dataset, \"test\")"
      ],
      "execution_count": 552,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40bwRicj0ew5"
      },
      "source": [
        "Так как мы предсказываем сентимент текста, каждый из который задаётся одним токеном, то нам\n",
        "- не надо сэмплировать. Мы хотим, чтобы модель честно учитывала вероятности для токенов\n",
        "- достаточно сделать один шаг генерации. Модель должна будет предсказать лейбл текста, а он, как мы выяснили, однотокенный.\n",
        "\n",
        "Таким образом, нам просто надо сделать один шаг жадного предсказания (greedy decoding)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8ax1sUE0ew6"
      },
      "source": [
        "def predict_on_text(model, ids) -> int:\n",
        "    \"\"\"\n",
        "    Run Language Model on text and use logits from last time \n",
        "    step to predict sentence category with greedy decoding\n",
        "    Args:\n",
        "        model: huggingface LM model\n",
        "        ids: LongTensor . Text encoded with tokenizer.\n",
        "    Returns:\n",
        "        model prediction, index of most probable word\n",
        "    \"\"\"\n",
        "    # TASK: run a model and get index most probable logit\n",
        "    # HINT: do not forget to detach tensors\n",
        "    # HINT 2: GPT-2 model returns tuple where logits are stored in the\n",
        "    # 1st item. You do not need 2nd item at all\n",
        "    # Our implementation is 2 lines\n",
        "    # YOUR CODE STARTS\n",
        "\n",
        "    idx = model.generate(\n",
        "        ids.reshape(-1, ids.shape[0]),\n",
        "        max_length=ids.shape[0] + 1,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )[0][-1]\n",
        "\n",
        "    # YOUR CODE ENDS\n",
        "    return idx.cpu().detach().item()"
      ],
      "execution_count": 573,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz6LoYwK0ew7"
      },
      "source": [
        "Проверка кода:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Uo318VV0ew7"
      },
      "source": [
        "ids = tokenizer.encode('hello world to ', return_tensors=\"pt\",).squeeze()\n",
        "# print(ids.shape)\n",
        "prediction = predict_on_text(model, ids.to(device))\n",
        "assert isinstance(prediction, int)"
      ],
      "execution_count": 574,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS4_pROdTi5L"
      },
      "source": [
        "# prediction = predict_on_text(model, ids.to(device))\n",
        "\n",
        "# for p in prediction:\n",
        "#     print(tokenizer.decode(p))\n",
        "#     print('\\n' + '=' * 40 + '\\n')"
      ],
      "execution_count": 575,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69zOpqp90ew7"
      },
      "source": [
        "def predict_on_dataset(model, texts, target, mapping):\n",
        "    \"\"\"\n",
        "    Run Language Model on a dataset and use its predictions as\n",
        "    labels for sentences\n",
        "    Args:\n",
        "        model: huggingface LM model\n",
        "        texts: List[str], texts of dataset\n",
        "        target: List[int], indices of classes\n",
        "        mapping: Dict[int, str] mapping from class indices to class labels\n",
        "    Returns:\n",
        "        model predictions, \"as is\", List[str]\n",
        "        target labels, after mapping, List[str]\n",
        "    \"\"\"\n",
        "    if len(texts) != len(target):\n",
        "        raise RuntimeError('Texts and target lengths mismatch')\n",
        "    # max_length has additional -1 because we will generate exactly 1 token \n",
        "    # with LM\n",
        "    max_length = model.config.n_ctx - 1\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    for idx in tqdm(range(len(texts))):\n",
        "        text, label = texts[idx], target[idx]\n",
        "        ids = tokenizer.encode(\n",
        "            get_prompt(input, separator, text, marker), \n",
        "            add_special_tokens=False, \n",
        "            return_tensors=\"pt\",\n",
        "            max_length=max_length\n",
        "        )\n",
        "        idx = predict_on_text(model, ids.to(device))\n",
        "        predictions.append(tokenizer.decode([idx]))  # generation.cpu().numpy()[0][-1]\n",
        "        labels.append(mapping[label]) \n",
        "    return predictions, labels"
      ],
      "execution_count": 576,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abvFt18q0ew8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48566ceb-53be-4004-f821-0402f3f77600"
      },
      "source": [
        "predictions, target = predict_on_dataset(model, small_texts[:10], small_labels[:10], mapping)"
      ],
      "execution_count": 577,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:02<00:00,  3.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIECT9O-0ew9"
      },
      "source": [
        "### Замер качества"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13EoUAQ10ew9"
      },
      "source": [
        "Посмотрим, что выдаёт нам языковая модель:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgiZYWnW0ew-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "752f42b1-705c-4023-e712-c97519c4459b"
      },
      "source": [
        "import numpy as np\n",
        "print(np.unique(predictions))"
      ],
      "execution_count": 572,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['To']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjzhPs7i0ew_"
      },
      "source": [
        "Можно заметить, что предсказанные метки не совсем похожи на pos и neg, которые были у нас в таргете. Напишем простую функцию, которая нормализует предсказания модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPB04pnd0exA"
      },
      "source": [
        "def normalize_prediction(raw_output: str) -> str:\n",
        "    \"\"\"\n",
        "    Helper that transforms model prediction to normal\n",
        "    For example, ' positive' -> 'pos', ' Negative' -> neg\n",
        "    Args:\n",
        "        raw_output: str, output from language model\n",
        "    Returns:\n",
        "        normalized value: no leading/trailing spaces, lowercase,\n",
        "        at most 3 chars long\n",
        "    \"\"\"\n",
        "    # TASK: write prediction normalizer\n",
        "    # You need to remove spaces, lowercase and get only 3 leading chars\n",
        "    # Our implementation is 1 line\n",
        "    # YOUR CODE STARTS\n",
        "\n",
        "    normalized_output = [x.strip().lower()[:3] for x in raw_output]\n",
        "\n",
        "    # YOUR CODE ENDS\n",
        "    return normalized_output"
      ],
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euC0k3nFtojd"
      },
      "source": [
        "Напишем функцию, вычисляющую точность:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CozGfPMo0exA"
      },
      "source": [
        "def accuracy(predictions, labels) -> float:\n",
        "    \"\"\"\n",
        "    Helper that transforms model prediction to normal\n",
        "    For example, ' positive' -> 'pos', ' Negative' -> neg\n",
        "    Args:\n",
        "        predictions: List[str], model predictions, should be labels in natural language\n",
        "        labels: List[str], target labels\n",
        "    Returns:\n",
        "        accuracy, float from [0, 1]\n",
        "    \"\"\"\n",
        "    if not labels:\n",
        "        # sanity check to avoid zero division\n",
        "        return 0\n",
        "    if len(predictions) != len(labels):\n",
        "        raise ValueError(f'Predictions and labels have mismatched length')\n",
        "    total = len(predictions)\n",
        "    match = 0\n",
        "    # TASK: calculate accuracy\n",
        "    # For every pair of predicted and real labels, check that they coincide.\n",
        "    # You can use zip() method for iteration over two sequences\n",
        "    # simultaneously.\n",
        "    # Our implementation is 3 lines\n",
        "    # YOUR CODE STARTS\n",
        "\n",
        "    # YOUR CODE ENDS\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_632NyJB0exA"
      },
      "source": [
        "normalized_predictions = [normalize_prediction(label) for label in predictions]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1hyGmSH0exB"
      },
      "source": [
        "accuracy(normalized_predictions, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04SFZ3Wo0exC"
      },
      "source": [
        "### Ура, модель без обучения работает лучше случайного угадывания!\n",
        "\n",
        "Тем не менее, это заметно хуже, чем у полносвязной нейросети. **Учитывая требования по ресурсам и время \n",
        "работы, использование такой модели в реальных задачах непрактично!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srG4Gk3p0exC"
      },
      "source": [
        "## А теперь замер на всем тестовом сете:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoTvsnWP0exD"
      },
      "source": [
        "predictions, target = predict_on_dataset(model, texts, labels, mapping)\n",
        "normalized_predictions = [normalize_prediction(label) for label in predictions]\n",
        "accuracy(normalized_predictions, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NZiUCu2xmu_"
      },
      "source": [
        "Все задание считается выполненным, если вы достигли accuracy 0.6 и выше на тестовом датасете. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xybBRqOz0exE"
      },
      "source": [
        "Данный пример призван продемонстрировать, что большая языковая модель хоть и справляется с задачей без лишних данных, но делает это заметно хуже специально созданных для задачи моделей, к тому же работает непростительно долго."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cFiadD10exE"
      },
      "source": [
        "### Резюме\n",
        "\n",
        "В этом задании мы применили большие языковые модели на практике и решили с её помощью 2 задачи:\n",
        "- conditional генерация текста\n",
        "- few-shot классификация текстов\n",
        "\n",
        "Мы увидели, что модель генерирует связный текст и может решать задачу классификации без изменений своих весов.\n",
        "\n",
        "Несмотря на возможности языковых моделей, их применение на практике всё ещё осложнено большими потребляемыми ресурсами, а также невозможностью интерпретировать предсказания."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUVsBkng0exE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}