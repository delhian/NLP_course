{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "gpt_2_text_clf_unresolved.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "04SFZ3Wo0exC"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e82543955d5241f6b12bba5fa8d2bc85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4c13c4e226cb466d9de71e10562298d5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8136d31645dd4f87a2235e0e930f0799",
              "IPY_MODEL_fe6f732483484d3991b32be276d61eae",
              "IPY_MODEL_2fadca88e3a84a238317d7eede123fc1"
            ]
          }
        },
        "4c13c4e226cb466d9de71e10562298d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8136d31645dd4f87a2235e0e930f0799": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fcea639516544712a437db9f931e4795",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_807829fa6de5448fb20079a0bc4fd7fc"
          }
        },
        "fe6f732483484d3991b32be276d61eae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_55876f43364e48789c90da1cbdc328bb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b2bd1fecb01340c8b3890743b8a0b3c2"
          }
        },
        "2fadca88e3a84a238317d7eede123fc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_457042bee2b049b092850f1c8f6b0492",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3/3 [00:00&lt;00:00, 71.13it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f506eeced50a4a92af8e8bca3be5d456"
          }
        },
        "fcea639516544712a437db9f931e4795": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "807829fa6de5448fb20079a0bc4fd7fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "55876f43364e48789c90da1cbdc328bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b2bd1fecb01340c8b3890743b8a0b3c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "457042bee2b049b092850f1c8f6b0492": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f506eeced50a4a92af8e8bca3be5d456": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/delhian/NLP_course/blob/master/Home%20Tasks/gpt_2_text_clf_unresolved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd5wsNhS0ewB"
      },
      "source": [
        "## Возможности и ограничения языковых моделей"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9YkOa3Jwbl-",
        "outputId": "493e0763-a0f9-4586-9c6b-daf61a2bd623"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dSYyxBI0ewG"
      },
      "source": [
        "**Тут есть важное действие, которое нужно сделать перед тем, как запускать этот ноутбук**\n",
        "\n",
        "Если вы делаете это задание в Google Colab, первым делом переключите Runtime в GPU. Это задание нормально посчитается и на CPU, но некоторые из будущих кейсов потребуют GPU (либо вам придётся несколько часов или даже дней, чтобы модель обучилась - и мы не преувиличиваем). Ещё мы рекомендуем переключить язык интерфейса на английский, потому что русская локализация ужасна да и вообще не нужна.\n",
        "\n",
        "О том, как переключить рантайм: https://www.geeksforgeeks.org/how-to-use-google-colab/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqRGVCt50ewL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b886cde-1c40-495c-a5d9-e94e1b8dcf0c"
      },
      "source": [
        "!pip install datasets transformers[torch]==2.10.0 tqdm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.14.0-py3-none-any.whl (290 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 30.1 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 30 kB 22.2 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 40 kB 18.0 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 51 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 61 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████                        | 71 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 81 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 92 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 102 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 112 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 122 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 133 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 143 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 153 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 163 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 174 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 184 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 194 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 204 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 215 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 225 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 235 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 245 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 256 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 266 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 276 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 286 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 290 kB 12.3 MB/s \n",
            "\u001b[?25hCollecting transformers[torch]==2.10.0\n",
            "  Downloading transformers-2.10.0-py3-none-any.whl (660 kB)\n",
            "\u001b[?25l\r\u001b[K     |▌                               | 10 kB 25.8 MB/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 27.8 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30 kB 33.2 MB/s eta 0:00:01\r\u001b[K     |██                              | 40 kB 34.3 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51 kB 35.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 61 kB 38.6 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 71 kB 39.6 MB/s eta 0:00:01\r\u001b[K     |████                            | 81 kB 38.8 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 92 kB 39.4 MB/s eta 0:00:01\r\u001b[K     |█████                           | 102 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 112 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 122 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 133 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 143 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 153 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 163 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 174 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 184 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 194 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 204 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 215 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 225 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 235 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 245 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 256 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 266 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 276 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 286 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 296 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 307 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 317 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 327 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 337 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 348 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 358 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 368 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 378 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 389 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 399 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 409 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 419 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 430 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 440 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 450 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 460 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 471 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 481 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 491 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 501 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 512 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 522 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 532 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 542 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 552 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 563 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 573 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 583 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 593 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 604 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 614 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 624 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 634 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 645 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 655 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 660 kB 40.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 38.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "  Downloading tokenizers-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 38.9 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 43.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (3.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (1.19.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (1.9.0+cu111)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 39.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 48.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 52.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Collecting huggingface-hub<0.1.0,>=0.0.19\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.13)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[torch]==2.10.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[torch]==2.10.0) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[torch]==2.10.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[torch]==2.10.0) (1.24.3)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 49.4 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 51.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[torch]==2.10.0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[torch]==2.10.0) (7.1.2)\n",
            "Installing collected packages: multidict, yarl, async-timeout, tokenizers, sentencepiece, sacremoses, fsspec, aiohttp, xxhash, transformers, huggingface-hub, datasets\n",
            "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.14.0 fsspec-2021.10.1 huggingface-hub-0.0.19 multidict-5.2.0 sacremoses-0.0.46 sentencepiece-0.1.96 tokenizers-0.7.0 transformers-2.10.0 xxhash-2.0.2 yarl-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcr7RqWl0ewN"
      },
      "source": [
        "from transformers import AutoModel, AutoTokenizer, GPT2LMHeadModel\n",
        "import datasets\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "import warnings"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJvskQ8T0ewO"
      },
      "source": [
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diaudnwY0ewQ"
      },
      "source": [
        "# import transformers\n",
        "# print(transformers.__version__)  # should be above or equal 4.0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVYGHVGT0ewS"
      },
      "source": [
        "В этом модуле мы с вами познакомились с большими языковыми моделями и обсудили их возможности, а также ограничения. В этом задании вам предлагается поэкспериментировать с одной из больших языковых моделей -- GPT-2 -- и самим убедиться, так ли она хороша, как о ней рассказывают."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXk55qyo0ewT"
      },
      "source": [
        "## Библиотека transformers\n",
        "\n",
        "Вы уже сталкивались с библиотекой transformers в этом курсе.\n",
        "\n",
        "Мы [загрузим gpt-2](https://huggingface.co/transformers/model_doc/gpt2.html) с помощью метода from_pretrained. Так как мы будем использовать её для задачи языкового моделирования, нам потребуется GPT2LMHeadModel, прочитать про неё можно [тут](https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel). У GPT-2 свой токенизатор, про него можно почитать [тут](https://huggingface.co/transformers/model_doc/gpt2.html#gpt2tokenizer).\n",
        "\n",
        "GPT-2 -- большая модель, время ее работы на CPU будет слишком большим, так что мы сразу отправим её на GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq9fHovY0ewU"
      },
      "source": [
        "model_name = 'gpt2-medium'\n",
        "device = 'cuda'"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVJ-USzH0ewW"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTXvTSFN0ewZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3c421e1-293b-49ec-ed51-a2ad0e08d7e0"
      },
      "source": [
        "%%time\n",
        "# model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/gpt2-medium')\n",
        "model = model.to(device)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 12.4 s, sys: 3.55 s, total: 16 s\n",
            "Wall time: 48.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJispE3M0ewa"
      },
      "source": [
        "### Подсчёт числа параметров модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asygaKxt0ewb"
      },
      "source": [
        "Чтобы понять, насколько GPT большая модель, подсчитаем число ее параметров"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrKJmIHo0ewc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c2579f0-1311-4435-f2a6-fca6d25447ac"
      },
      "source": [
        "# TASK: compute number of model parameters\n",
        "# iterate over model weights and count number of parameters in each of them, then sum it up\n",
        "# note: you may find model.parameters() method useful\n",
        "# Our implementation is 2 lines\n",
        "\n",
        "# YOUR CODE STARTS\n",
        "params = sum([params.numel() for params in model.parameters()])\n",
        "# YOUR CODE ENDS\n",
        "params"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "354823168"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHOJXyah0ewd"
      },
      "source": [
        "Как видим, это большое число параметров, на несколько порядок больше, чем те модели, которые вы обучали до этого! Использовать такое в продакшене как правило невозможно из-за больших требований по ресурсам"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1-PmKrq0ewd"
      },
      "source": [
        "# Часть 1. Генерация отзывов моделью GPT\n",
        "\n",
        "В преыдущем задании мы генерировали ровно 1 токен, так как нам нужно было предсказать сентимент, а оба слова \"positive\" и \"negative\" есть в словаре GPT.\n",
        "\n",
        "В этом задании мы пойдём дальше и самостоятельно реализуем top-k сэмплирование, чтобы генерировать отзывы на фильмы, подобные тем, что в датасете IMDB.\n",
        "\n",
        "Мы будем продолжать отзывы, используя в качестве \"затравки\" для модели начало реальных ревью."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUnAhWXm0ewd"
      },
      "source": [
        "### Top-k & Top-p sampling\n",
        "\n",
        "Чтобы генерация текста была более разнообразной, мы будем использовать top k сэмплирование, которое мы изучали в этом юните. Его смысл в том, что на каждом шаге мы перед сэмплированием зануляем вероятности всех слов, кроме k самых вероятных."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T4TM0EL0ewf"
      },
      "source": [
        "def topk_sample(scores, k):\n",
        "    \"\"\"\n",
        "    Sample from logits using multinomial distribution\n",
        "    Before sampling, all logits except k greatest are zeroed out.\n",
        "    Args:\n",
        "        scores: scores for every word in the vocabulary. Must be 1-dimensional: (num_words_in_vocab)\n",
        "        k: int, how many hypotheses to sample from\n",
        "    Returns:\n",
        "        1 draw from dictribution, torch.LongTensor of shape (1)\n",
        "    \"\"\"\n",
        "    assert k >= 1, 'k must be >=1'\n",
        "    assert scores.ndim == 1, 'logits must have 1 dimension' \n",
        "\n",
        "    # TASK: implement top-k sampling\n",
        "    # First, get top k values and their indices using torch.topk\n",
        "    # 2nd, fill logits with some small value (e. g. 1e-6)\n",
        "    # Next, move values back to their place. Note that you will find ellipsis (...) and None useful\n",
        "    # Finally, use softmax to obtain probabilities and get \n",
        "    # Our implementation is 6 lines\n",
        "    \n",
        "    # YOUR CODE STARTS\n",
        "    scores_new = scores.clone()\n",
        "    values, indexes = scores_new.topk(k)\n",
        "\n",
        "    scores_new = scores_new.fill_(1e-6)\n",
        "    scores_new[indexes] = values\n",
        "    predicted_ids = F.softmax(scores_new, dim=-1)\n",
        "    # predicted_ids = torch.multinomial(predicted_ids, num_samples=1)\n",
        "    return predicted_ids"
      ],
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvm41e190ewf"
      },
      "source": [
        "Проверка кода:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFpPyxFt0ewg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a17de17b-ec08-4091-9574-849b3ec504c6"
      },
      "source": [
        "logits = torch.randn(5)\n",
        "for _ in range(10):\n",
        "    res1 = topk_sample(logits, 1)\n",
        "    res2  = topk_sample(logits, 1)\n",
        "    assert res1.ndim == 1\n",
        "    assert res1.equal(res2)\n",
        "equal = True\n",
        "for _ in range(10):\n",
        "    if not topk_sample(logits, 2).equal(topk_sample(logits, 2)):\n",
        "        equal = False\n",
        "assert equal\n",
        "print('OK!')"
      ],
      "execution_count": 323,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2VBo0eotL6Z"
      },
      "source": [
        "Также мы будем использовать top-p sampling, который мы уже проходили на лекциях."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ8divSy0ewh"
      },
      "source": [
        "def topp_sample(scores, p):\n",
        "    \"\"\"\n",
        "    Sample from logits using multinomial distribution\n",
        "    Before sampling, all logits except those largest\n",
        "    whose cumsum exceeds p are zeroed out\n",
        "    Args:\n",
        "        scores: scores for every word in the vocabulary. Must be 1-dimensional: (num_words_in_vocab)\n",
        "        p: float, cumulative probability of most high-scored tokens\n",
        "    Returns:\n",
        "        1 draw from dictribution, torch.LongTensor of shape (1)\n",
        "    \"\"\"\n",
        "    assert  0 < p < 1, 'p must be between 0 and 1'\n",
        "    assert scores.ndim == 1, 'logits must have 1 dimension' \n",
        "\n",
        "    # TASK: implement top-p sampling\n",
        "    # 1. sort scores with descending order\n",
        "\n",
        "    values, indexes = scores.sort(descending = True)\n",
        "    # 2. get cumulative sum of softmaxed logits\n",
        "\n",
        "    values_cumsum = torch.cumsum(values, dim =0 )\n",
        "\n",
        "    # 3. get indices when cumulative sum is larger than p. These indices should be zeroed out\n",
        "\n",
        "    scores[indexes[values_cumsum > p]] = 1e-6\n",
        "\n",
        "    # 4. get real indices that should be zeroes out from sorted indices. Use tensor slicing\n",
        "    # 5. fill these indices with some small value (e. g. -1e6)\n",
        "    # 6. sample, as you did in top-k sampling\n",
        "    # Our implementation is 7 lines\n",
        "    \n",
        "    # YOUR CODE STARTS\n",
        "    predicted_ids = F.softmax(scores)\n",
        "    # YOUR CODE ENDS\n",
        "    # predicted_ids = torch.multinomial(predicted_ids, num_samples=1)\n",
        "    return predicted_ids"
      ],
      "execution_count": 326,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-0t8dHgtN3N"
      },
      "source": [
        "Проверка кода:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrDbJBlc0ewi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8f409b1-914c-4590-b410-e33f64881eda"
      },
      "source": [
        "logits = torch.Tensor([-.1, -.2, .9])\n",
        "for _ in range(10):\n",
        "    res1 = topp_sample(logits, 0.8)\n",
        "    res2  = topp_sample(logits, 0.8)\n",
        "    assert res1.ndim == 1\n",
        "\n",
        "    assert res1.equal(res2)\n",
        "logits = torch.Tensor([.5, .5, .5])\n",
        "equal = True\n",
        "for _ in range(10):\n",
        "    if not topp_sample(logits, 0.8).equal(topp_sample(logits, 0.8)):\n",
        "        equal = False\n",
        "assert equal\n",
        "print('OK!')"
      ],
      "execution_count": 327,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_FEC40wp9Bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f29050d2-6a60-42da-ee06-3e65a63bfbfc"
      },
      "source": [
        "context_strings = ['How', 'much', 'time','do']\n",
        "# print(' '.join(context_strings))\n",
        "context=torch.tensor([tokenizer.encode(context_strings)]).to(device)\n",
        "\n",
        "output, past = model(context)\n",
        "print(output.shape)\n",
        "\n",
        "token = torch.argmax(output[0, -1, :])\n",
        "print(tokenizer.decode(token.item()))"
      ],
      "execution_count": 330,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4, 50257])\n",
            "you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzdgJa1UwgWq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca62ede7-383c-4407-f860-dd2020519043"
      },
      "source": [
        "context_strings = ['What', 'is']\n",
        "print(' '.join(context_strings), end=' ')\n",
        "ids = torch.tensor([tokenizer.encode(context_strings)]).to(device)\n",
        "# print(ids)\n",
        "length = 50\n",
        "i = 0\n",
        "while i < length:\n",
        "  output, _ = model(ids)\n",
        "  logits = output[0, -1]\n",
        "  i += 1\n",
        "  new_id = torch.argmax(logits).reshape(-1)\n",
        "  ids = torch.cat((ids[0], new_id),).to(device).int()\n",
        "  ids = ids.reshape(1, ids.shape[0])\n",
        "  # print('ids:', ids[0].tolist())\n",
        "  print(tokenizer.decode(new_id), end='')"
      ],
      "execution_count": 334,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is Paid.com is a free service that allows you to pay for your favorite websites using Bitcoin.\n",
            "\n",
            "The service is available in the following countries:\n",
            "\n",
            "United States\n",
            "\n",
            "United Kingdom\n",
            "\n",
            "Canada\n",
            "\n",
            "Australia\n",
            "\n",
            "New Zealand\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgqFirKjtPrz"
      },
      "source": [
        "Теперь напишите функцию генерации текста:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sNwScYY0ewj"
      },
      "source": [
        "def generate_text(model, ids, length, k=None, p=None):\n",
        "    \"\"\"\n",
        "    Generate text with language model with ids as prompt\n",
        "    Args:\n",
        "        model: huggingface LM model\n",
        "        ids: input ids, from tokenizer.encode, must be 2-dimensional\n",
        "        length: int, how many tokens to geenrate\n",
        "        k: int, how many hypotheses to sample from in top-k sampling\n",
        "    Returns:\n",
        "        token ids from generated text, together with token ids of prompts, 2d LongTensor\n",
        "    \"\"\"\n",
        "    assert length >= 1\n",
        "    if k and p:\n",
        "        raise RuntimeError('Cannot use topk and topp sampling simultaneously')\n",
        "    # TASK: write generation loop\n",
        "    # For every timestamp:\n",
        "    # - obtain model output\n",
        "    # - get logits for last symbol\n",
        "    # - apply topk sampling to get new index\n",
        "    # - concatenate it to the ids to form new input\n",
        "    # in the end, the `ids` tensor will have full generation\n",
        "    # our implementation is 5 lines\n",
        "    \n",
        "    # YOUR CODE STARTS\n",
        "\n",
        "    # i = 0\n",
        "    # while i < length:\n",
        "    #   output, _ = model(ids)\n",
        "    #   logits = output[0, -1]\n",
        "    #   i += 1\n",
        "    #   new_id = torch.argmax(logits).reshape(-1)\n",
        "    #   ids = torch.cat((ids[0], new_id),).to(device).int()\n",
        "    #   ids = ids.reshape(1, ids.shape[0])\n",
        "    ids = model.generate(\n",
        "        ids, \n",
        "        do_sample=True, \n",
        "        max_length=ids.shape[1] + length, \n",
        "        repetition_penalty=1.2, \n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        top_k = k,\n",
        "        top_p = p\n",
        "    )\n",
        "\n",
        "    # YOUR CODE ENDS\n",
        "    return ids.cpu().detach()"
      ],
      "execution_count": 358,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB6g8UWp0ewj"
      },
      "source": [
        "Проверка кода:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8mpDF_O0ewk"
      },
      "source": [
        "ids = torch.randint(0, 1000, (1, 4)).to(device)\n",
        "generation = generate_text(model, ids, 4, k=2)\n",
        "\n",
        "assert generation.shape == torch.Size([1, 8])\n",
        "\n",
        "generation = generate_text(model, ids, 3, p=.5)\n",
        "\n",
        "assert generation.shape == torch.Size([1, 7])"
      ],
      "execution_count": 360,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5uhuTT30ewl"
      },
      "source": [
        "### Запускаем генерацию\n",
        "\n",
        "Зададим модели какой-нибудь prompt, например восторженный отзыв о фильме, и посмотрим, как она продолжит его. Так как модель сэмплит на каждом шаге, нам интересно посмотреть на несколько траекторий сразу. Для этого мы и писали батч-режим в генерации."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jntDAgA90ewm"
      },
      "source": [
        "prompt = 'What a lovely film !'"
      ],
      "execution_count": 361,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_OBJte00ewm"
      },
      "source": [
        "Подготовим входные данные для GPT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSwU95VV0ewn"
      },
      "source": [
        "ids = tokenizer.encode(prompt, return_tensors=\"pt\").squeeze().to(device)"
      ],
      "execution_count": 362,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tew84_O80ewo"
      },
      "source": [
        "Запускаем генерацию! Можно запустить на 10-20 токенов, можно и больше"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSHTNksk0ewo"
      },
      "source": [
        "n_tries = 5"
      ],
      "execution_count": 364,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHM7qBeg0ewr"
      },
      "source": [
        "А теперь запустим генерацию с top-p сэмплированием"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ4uzsOW0ewr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9e59752-c198-4779-a43b-a1d12d54358e"
      },
      "source": [
        "generations = [generate_text(model, ids, length=40, p=0.5).squeeze().cpu().detach() for _ in range(n_tries)]\n",
        "for generation in generations:\n",
        "    print(tokenizer.decode(generation))\n",
        "    print('\\n' + '=' * 40 + '\\n')"
      ],
      "execution_count": 372,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What a lovely film! The sound is wonderful, the camera movement and lighting are superb. I'm really impressed with this movie!\n",
            "\n",
            "Rated 5 out of 10 by Jody from Fantastic story/storytelling - beautiful set\n",
            "\n",
            "========================================\n",
            "\n",
            "What a lovely film! I love the characters, and it's great to see how much they have grown. The only problem is that there are too many of them!\n",
            "\n",
            "This review may contain affiliate links which won't\n",
            "\n",
            "========================================\n",
            "\n",
            "What a lovely film! It's not about the story but it is all that I remember from my childhood. The soundtrack and sound effects are beautiful, especially in this case.\"\n",
            "\n",
            "– David Karpowitz, Los Angeles\n",
            "\n",
            "========================================\n",
            "\n",
            "What a lovely film! It is very funny and has some great moments, especially the one with Mabel.\n",
            "\n",
            "The best part of this movie was that it's really not about what happened to her or even why she\n",
            "\n",
            "========================================\n",
            "\n",
            "What a lovely film! I am so happy to see it again. It is very well done, and the ending makes me laugh out loud!\"\n",
            "\n",
            "- Rona K., USA\n",
            "\n",
            "\n",
            " \"I have seen this\n",
            "\n",
            "========================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isQrAwqG0ews"
      },
      "source": [
        "# Часть 2. Few-shot learning\n",
        "\n",
        "Одной из особенностей GPT, про которую авторы написали, является способность к few-shot learning. В этой части домашнего задания мы на примере хорошо знакомой нам задачи классификации текстов исследуем, как хорошо модель умеет различать сентимент отзывов без дополнительного обучения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2osAS3f0ews"
      },
      "source": [
        "## Датасет.\n",
        "Мы будем снова использовать датасет imdb, на котором мы уже обучили несколько моделей. Так как обучать саму модель мы не будем (в few-shot парадигме веса не меняются), то можем сразу взять валидационную часть датасета."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQpgFvhq0ewt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65,
          "referenced_widgets": [
            "e82543955d5241f6b12bba5fa8d2bc85",
            "4c13c4e226cb466d9de71e10562298d5",
            "8136d31645dd4f87a2235e0e930f0799",
            "fe6f732483484d3991b32be276d61eae",
            "2fadca88e3a84a238317d7eede123fc1",
            "fcea639516544712a437db9f931e4795",
            "807829fa6de5448fb20079a0bc4fd7fc",
            "55876f43364e48789c90da1cbdc328bb",
            "b2bd1fecb01340c8b3890743b8a0b3c2",
            "457042bee2b049b092850f1c8f6b0492",
            "f506eeced50a4a92af8e8bca3be5d456"
          ]
        },
        "outputId": "8d98f3f6-83e5-4707-d025-6df57a9e32bc"
      },
      "source": [
        "text_dataset = datasets.load_dataset(\"imdb\")\n",
        "texts = text_dataset[\"test\"][\"text\"]\n",
        "labels = text_dataset[\"test\"][\"label\"]"
      ],
      "execution_count": 373,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e82543955d5241f6b12bba5fa8d2bc85",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DPlm7ad0ewu"
      },
      "source": [
        "### Метки для задачи классификации на естественном языке\n",
        "Поскольку языковая модель знает только токены языка и не знает маппинга между индексами и лейблами, нам нужно будет сравнивать лейблы напрямую. С одной стороны, это создаёт дополнительные сложности: модель может предсказать \"Positive Sentiment\" вместо \"positive\", поэтому нам потребуется минимальный постпроцессинг, чтобы учесть большинство таких случаев."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7qRUd-X0ewu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b1f856b-3192-42f6-94b1-7c5e9f2c5f5e"
      },
      "source": [
        "mapping = {idx: label for idx, label in enumerate(text_dataset[\"train\"].features[\"label\"].names)}\n",
        "mapping"
      ],
      "execution_count": 374,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'neg', 1: 'pos'}"
            ]
          },
          "metadata": {},
          "execution_count": 374
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmkvUl1W0ewv"
      },
      "source": [
        "### Игрушечный датасет\n",
        "Чтобы быстрее прототипировать наши вводы (prompts), полезно взять маленькую часть датасета и проверять гипотезы на ней. Так как imdb упорядочен по лейблам, а самих лейблов всего 2, мы берём одинаковое количество примеров с начала и с конца."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUrFRggy0ewv"
      },
      "source": [
        "def get_toy_dataset(dataset, split, length = 100):\n",
        "    texts = dataset[split][\"text\"]\n",
        "    labels = dataset[split][\"label\"]\n",
        "    small_texts = texts[:length // 2] + texts[-length//2:]\n",
        "    small_labels = labels[:length // 2] + labels[-length//2:]\n",
        "    return small_texts, small_labels"
      ],
      "execution_count": 375,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9gjyJGQ0eww"
      },
      "source": [
        "assert len(get_toy_dataset(text_dataset, \"test\", 100)[0]) == 100"
      ],
      "execution_count": 376,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQovzfQu0eww"
      },
      "source": [
        "## Few-shot learning\n",
        "\n",
        "В данном задании мы будем не обучать модель на данных, а использовать уже обученную на огромном корпусе языковую модель, чтобы \"угадывать\" сентимент отзывов.\n",
        "\n",
        "Модель будет видеть в качестве затравки (prompt) описание задачи на естественном языке.\n",
        "\n",
        "Нам потребуется:\n",
        "- описание задачи\n",
        "- лейблы\n",
        "- несколько (хотя бы 3) примеров\n",
        "- вспомогательные маркеры (разделитель примеров и маркер конца входа)\n",
        "\n",
        "Например:\n",
        "\n",
        "```\n",
        "Translate from English to French\n",
        "sea otter => loutre de mer\n",
        "peppermint => menthe poivrée\n",
        "plush girafe  => \n",
        "```\n",
        "\n",
        "Первая строка здесь -- описание задачи. Мы решаем задачу классификации, поэтому нам нужно\n",
        "описать задачу примерно как `To which category does the text belong? positive , negative `\n",
        "\n",
        "Вторая и третья строки -- примеры входа и выхода. В нашем случае примерами входа и выхода будут служить тексты ревью и метки классов positive и negative (а не 0 и 1, как раньше).\n",
        "\n",
        "Наконец, последняя строка -- это тот текст, который модель должна классифицировать. Модель уже \"видела\" выше пример того, что после маркера конца входа идёт сентимент, и мы ожидаем, что она сможет выучить эту закономерность.\n",
        "\n",
        "В данном задании часть входа будет написана за вас, вам надо будет придумать обучающие примеры для few-shot learning'а"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZQgn9uG0ewx"
      },
      "source": [
        "marker = ' => '\n",
        "separator = '\\n'"
      ],
      "execution_count": 377,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFxS1Qnp0ewx"
      },
      "source": [
        "labels = [\"positive\" , \"negative\"]\n",
        "labels_text = \" , \".join(labels)\n",
        "task_description = f'To which category does the text belong?: {labels_text} ' + separator"
      ],
      "execution_count": 378,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu1Uwvlm-dEZ"
      },
      "source": [
        "lbl = text_dataset['train']['label']\n",
        "texts =  text_dataset['train']['text']"
      ],
      "execution_count": 379,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj9_4rok92rI"
      },
      "source": [
        "import random\n",
        "\n",
        "random_negative = random.sample(texts[12500: 24999], noex)\n",
        "random_positive = random.sample(texts[0: 12499], noex)"
      ],
      "execution_count": 380,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-Z-wR-k0ewy"
      },
      "source": [
        "max_len = 120\n",
        "noex = 4\n",
        "\n",
        "random_negatives = random.sample([x for x in texts[12500: 24999] if len(x) < max_len], noex)\n",
        "random_positives = random.sample([x for x in texts[0: 12499] if len(x) < max_len], noex)\n",
        "\n",
        "    # TASK define examples\n",
        "    # They should be a tuple with text and label\n",
        "    # label should be from \"labels\"\n",
        "    # Get 3-10 different examples, they should cover both positive \n",
        "    # and negative reviews\n",
        "    # YOUR CODE STARTS\n",
        "examples = [(' '.join(x.split(' ')), \"negative\") for x in random_negatives] + [(' '.join(x.split(' ')[:30]), \"positive\") for x in random_positives]\n",
        "    # YOUR CODE ENDS"
      ],
      "execution_count": 642,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hc9gbfFYBJ1O",
        "outputId": "d8502537-f5da-4195-e555-2f28c407be4e"
      },
      "source": [
        "examples"
      ],
      "execution_count": 643,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('no comment - stupid movie, acting average or worse... screenplay - no sense at all... SKIP IT!',\n",
              "  'negative'),\n",
              " (\"You'd better choose Paul Verhoeven's even if you have watched it.\",\n",
              "  'negative'),\n",
              " (\"I wouldn't rent this one even on dollar rental night.\", 'negative'),\n",
              " ('A rating of \"1\" does not begin to express how dull, depressing and relentlessly bad this movie is.',\n",
              "  'negative'),\n",
              " ('Adrian Pasdar is excellent is this film. He makes a fascinating woman.',\n",
              "  'positive'),\n",
              " (\"I don't know why I like this movie so well, but I never get tired of watching it.\",\n",
              "  'positive'),\n",
              " ('This movie will always be a Broadway and Movie classic, as long as there are still people who sing, dance, and act.',\n",
              "  'positive'),\n",
              " ('This is the definitive movie version of Hamlet. Branagh cuts nothing, but there are no wasted moments.',\n",
              "  'positive')]"
            ]
          },
          "metadata": {},
          "execution_count": 643
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aso2fG6_0ewz"
      },
      "source": [
        "labels = [\"positive\" , \"negative\"]\n",
        "\n",
        "assert len(examples) > 1, 'Write at least two different examples'\n",
        "for ex in examples:\n",
        "    assert ex[1] in labels"
      ],
      "execution_count": 644,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Heu6rvlK0ewz"
      },
      "source": [
        "input = task_description  + separator.join([marker.join(example) for example in examples])"
      ],
      "execution_count": 645,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buEhQeHo0ew1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16c0304f-b373-4bb4-956c-2e3e8a27e064"
      },
      "source": [
        "print(input)"
      ],
      "execution_count": 646,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To which category does the text belong?: positive , negative \n",
            "no comment - stupid movie, acting average or worse... screenplay - no sense at all... SKIP IT! => negative\n",
            "You'd better choose Paul Verhoeven's even if you have watched it. => negative\n",
            "I wouldn't rent this one even on dollar rental night. => negative\n",
            "A rating of \"1\" does not begin to express how dull, depressing and relentlessly bad this movie is. => negative\n",
            "Adrian Pasdar is excellent is this film. He makes a fascinating woman. => positive\n",
            "I don't know why I like this movie so well, but I never get tired of watching it. => positive\n",
            "This movie will always be a Broadway and Movie classic, as long as there are still people who sing, dance, and act. => positive\n",
            "This is the definitive movie version of Hamlet. Branagh cuts nothing, but there are no wasted moments. => positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIHpblK80ew2"
      },
      "source": [
        "Сравните получившийся инпут с примером выше. Мы получили то, что надо!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akx9zMwFbKdY"
      },
      "source": [
        "n_chars = 50"
      ],
      "execution_count": 647,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWfxK702bJKp"
      },
      "source": [
        "def get_prompt(beginning, separator, text, marker) -> str:\n",
        "    return ' '.join([input,  separator, text[:n_chars] + text[-n_chars:], marker])"
      ],
      "execution_count": 648,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmQrc0fs0ew5"
      },
      "source": [
        "## Время проверить работу нашей модели на маленьком датасете!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_0WPdws0ew5"
      },
      "source": [
        "small_texts, small_labels = get_toy_dataset(text_dataset, \"test\")"
      ],
      "execution_count": 649,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40bwRicj0ew5"
      },
      "source": [
        "Так как мы предсказываем сентимент текста, каждый из который задаётся одним токеном, то нам\n",
        "- не надо сэмплировать. Мы хотим, чтобы модель честно учитывала вероятности для токенов\n",
        "- достаточно сделать один шаг генерации. Модель должна будет предсказать лейбл текста, а он, как мы выяснили, однотокенный.\n",
        "\n",
        "Таким образом, нам просто надо сделать один шаг жадного предсказания (greedy decoding)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8ax1sUE0ew6"
      },
      "source": [
        "def predict_on_text(model, ids) -> int:\n",
        "    \"\"\"\n",
        "    Run Language Model on text and use logits from last time \n",
        "    step to predict sentence category with greedy decoding\n",
        "    Args:\n",
        "        model: huggingface LM model\n",
        "        ids: LongTensor . Text encoded with tokenizer.\n",
        "    Returns:\n",
        "        model prediction, index of most probable word\n",
        "    \"\"\"\n",
        "    # TASK: run a model and get index most probable logit\n",
        "    # HINT: do not forget to detach tensors\n",
        "    # HINT 2: GPT-2 model returns tuple where logits are stored in the\n",
        "    # 1st item. You do not need 2nd item at all\n",
        "    # Our implementation is 2 lines\n",
        "    # YOUR CODE STARTS\n",
        "\n",
        "    idx = model.generate(\n",
        "        ids.reshape(-1, ids.shape[0]),\n",
        "        max_length=ids.shape[0] + 1,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )[0][-1]\n",
        "\n",
        "    # YOUR CODE ENDS\n",
        "    return idx.cpu().detach().item()"
      ],
      "execution_count": 650,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz6LoYwK0ew7"
      },
      "source": [
        "Проверка кода:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Uo318VV0ew7"
      },
      "source": [
        "ids = tokenizer.encode('hello world to ', return_tensors=\"pt\",).squeeze()\n",
        "# print(ids.shape)\n",
        "prediction = predict_on_text(model, ids.to(device))\n",
        "assert isinstance(prediction, int)"
      ],
      "execution_count": 651,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69zOpqp90ew7"
      },
      "source": [
        "def predict_on_dataset(model, texts, target, mapping):\n",
        "    \"\"\"\n",
        "    Run Language Model on a dataset and use its predictions as\n",
        "    labels for sentences\n",
        "    Args:\n",
        "        model: huggingface LM model\n",
        "        texts: List[str], texts of dataset\n",
        "        target: List[int], indices of classes\n",
        "        mapping: Dict[int, str] mapping from class indices to class labels\n",
        "    Returns:\n",
        "        model predictions, \"as is\", List[str]\n",
        "        target labels, after mapping, List[str]\n",
        "    \"\"\"\n",
        "    print(len(texts) , len(target))\n",
        "    if len(texts) != len(target):\n",
        "        raise RuntimeError('Texts and target lengths mismatch')\n",
        "    # max_length has additional -1 because we will generate exactly 1 token \n",
        "    # with LM\n",
        "    max_length = model.config.n_ctx - 1\n",
        "    predictions = []\n",
        "    labels = []\n",
        "\n",
        "    for idx in tqdm(range(len(texts))):\n",
        "        text, label = texts[idx], target[idx]\n",
        "\n",
        "        ids = tokenizer.encode(\n",
        "            get_prompt(input, separator, text, marker), \n",
        "            add_special_tokens=False, \n",
        "            return_tensors=\"pt\",\n",
        "            max_length=max_length\n",
        "        ).squeeze()\n",
        "        idx = predict_on_text(model, ids.to(device))\n",
        "\n",
        "        predictions.append(tokenizer.decode([idx]))  # generation.cpu().numpy()[0][-1]\n",
        "        labels.append(mapping[label]) \n",
        "    return predictions, labels"
      ],
      "execution_count": 652,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abvFt18q0ew8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f94c8270-68e0-4629-c807-c245999ed0f5"
      },
      "source": [
        "predictions, target = predict_on_dataset(model, small_texts, small_labels, mapping)"
      ],
      "execution_count": 653,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:15<00:00,  6.55it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReMfA06zE-p8",
        "outputId": "bc955bf6-2445-438b-9e3e-8d897f623904"
      },
      "source": [
        "import numpy as np\n",
        "np.unique(predictions)"
      ],
      "execution_count": 654,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([' negative', ' positive'], dtype='<U9')"
            ]
          },
          "metadata": {},
          "execution_count": 654
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIECT9O-0ew9"
      },
      "source": [
        "### Замер качества"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13EoUAQ10ew9"
      },
      "source": [
        "Посмотрим, что выдаёт нам языковая модель:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjzhPs7i0ew_"
      },
      "source": [
        "Можно заметить, что предсказанные метки не совсем похожи на pos и neg, которые были у нас в таргете. Напишем простую функцию, которая нормализует предсказания модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPB04pnd0exA"
      },
      "source": [
        "def normalize_prediction(raw_output: str) -> str:\n",
        "    \"\"\"\n",
        "    Helper that transforms model prediction to normal\n",
        "    For example, ' positive' -> 'pos', ' Negative' -> neg\n",
        "    Args:\n",
        "        raw_output: str, output from language model\n",
        "    Returns:\n",
        "        normalized value: no leading/trailing spaces, lowercase,\n",
        "        at most 3 chars long\n",
        "    \"\"\"\n",
        "    # TASK: write prediction normalizer\n",
        "    # You need to remove spaces, lowercase and get only 3 leading chars\n",
        "    # Our implementation is 1 line\n",
        "    # YOUR CODE STARTS\n",
        "\n",
        "    # YOUR CODE ENDS\n",
        "    return raw_output.strip().lower()[:3]"
      ],
      "execution_count": 655,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euC0k3nFtojd"
      },
      "source": [
        "Напишем функцию, вычисляющую точность:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CozGfPMo0exA"
      },
      "source": [
        "def accuracy(predictions, labels) -> float:\n",
        "    \"\"\"\n",
        "    Helper that transforms model prediction to normal\n",
        "    For example, ' positive' -> 'pos', ' Negative' -> neg\n",
        "    Args:\n",
        "        predictions: List[str], model predictions, should be labels in natural language\n",
        "        labels: List[str], target labels\n",
        "    Returns:\n",
        "        accuracy, float from [0, 1]\n",
        "    \"\"\"\n",
        "    if not labels:\n",
        "        # sanity check to avoid zero division\n",
        "        return 0\n",
        "    if len(predictions) != len(labels):\n",
        "        raise ValueError(f'Predictions and labels have mismatched length')\n",
        "    total = len(predictions)\n",
        "    match = 0\n",
        "    # TASK: calculate accuracy\n",
        "    # For every pair of predicted and real labels, check that they coincide.\n",
        "    # You can use zip() method for iteration over two sequences\n",
        "    # simultaneously.\n",
        "    # Our implementation is 3 lines\n",
        "    # YOUR CODE STARTS\n",
        "\n",
        "    match = len([1 for (x,y) in zip(predictions, labels) if x == y])\n",
        "    # YOUR CODE ENDS\n",
        "    return match/total"
      ],
      "execution_count": 656,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_632NyJB0exA"
      },
      "source": [
        "normalized_predictions = [normalize_prediction(label) for label in predictions]"
      ],
      "execution_count": 657,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1hyGmSH0exB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ff75dee-db65-4bc9-8a7c-1325ccfbdc2a"
      },
      "source": [
        "accuracy(normalized_predictions, target)"
      ],
      "execution_count": 658,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.71"
            ]
          },
          "metadata": {},
          "execution_count": 658
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04SFZ3Wo0exC"
      },
      "source": [
        "### Ура, модель без обучения работает лучше случайного угадывания!\n",
        "\n",
        "Тем не менее, это заметно хуже, чем у полносвязной нейросети. **Учитывая требования по ресурсам и время \n",
        "работы, использование такой модели в реальных задачах непрактично!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srG4Gk3p0exC"
      },
      "source": [
        "## А теперь замер на всем тестовом сете:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kww3w-XIZ8KD"
      },
      "source": [
        "labels = text_dataset[\"test\"][\"label\"]"
      ],
      "execution_count": 659,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoTvsnWP0exD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53aadcc0-2c21-4c60-a5b4-f534eea349a0"
      },
      "source": [
        "predictions, target = predict_on_dataset(model, texts, labels, mapping)\n",
        "normalized_predictions = [normalize_prediction(label) for label in predictions]\n",
        "accuracy(normalized_predictions, target)"
      ],
      "execution_count": 660,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000 25000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25000/25000 [1:02:45<00:00,  6.64it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.67928"
            ]
          },
          "metadata": {},
          "execution_count": 660
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NZiUCu2xmu_"
      },
      "source": [
        "Все задание считается выполненным, если вы достигли accuracy 0.6 и выше на тестовом датасете. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xybBRqOz0exE"
      },
      "source": [
        "Данный пример призван продемонстрировать, что большая языковая модель хоть и справляется с задачей без лишних данных, но делает это заметно хуже специально созданных для задачи моделей, к тому же работает непростительно долго."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cFiadD10exE"
      },
      "source": [
        "### Резюме\n",
        "\n",
        "В этом задании мы применили большие языковые модели на практике и решили с её помощью 2 задачи:\n",
        "- conditional генерация текста\n",
        "- few-shot классификация текстов\n",
        "\n",
        "Мы увидели, что модель генерирует связный текст и может решать задачу классификации без изменений своих весов.\n",
        "\n",
        "Несмотря на возможности языковых моделей, их применение на практике всё ещё осложнено большими потребляемыми ресурсами, а также невозможностью интерпретировать предсказания."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUVsBkng0exE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}