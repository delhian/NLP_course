{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "gpt_2_text_clf_unresolved.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "04SFZ3Wo0exC"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/delhian/NLP_course/blob/master/Home%20Tasks/gpt_2_text_clf_unresolved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd5wsNhS0ewB"
      },
      "source": [
        "## Возможности и ограничения языковых моделей"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9YkOa3Jwbl-",
        "outputId": "bd5056a5-7c40-4bb4-c88b-7830e7ffaf24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dSYyxBI0ewG"
      },
      "source": [
        "**Тут есть важное действие, которое нужно сделать перед тем, как запускать этот ноутбук**\n",
        "\n",
        "Если вы делаете это задание в Google Colab, первым делом переключите Runtime в GPU. Это задание нормально посчитается и на CPU, но некоторые из будущих кейсов потребуют GPU (либо вам придётся несколько часов или даже дней, чтобы модель обучилась - и мы не преувиличиваем). Ещё мы рекомендуем переключить язык интерфейса на английский, потому что русская локализация ужасна да и вообще не нужна.\n",
        "\n",
        "О том, как переключить рантайм: https://www.geeksforgeeks.org/how-to-use-google-colab/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqRGVCt50ewL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "615e5a4e-cd12-4d70-90e0-54fdf150f147"
      },
      "source": [
        "!pip install datasets transformers[torch]==2.10.0 tqdm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.13.3)\n",
            "Requirement already satisfied: transformers[torch]==2.10.0 in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (0.7.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (0.1.96)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (3.3.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (0.0.46)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (2.23.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (1.9.0+cu111)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0,>=0.0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.19)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.7.4.post0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.10.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.13)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[torch]==2.10.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[torch]==2.10.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[torch]==2.10.0) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[torch]==2.10.0) (3.0.4)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (3.0.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (5.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[torch]==2.10.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[torch]==2.10.0) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcr7RqWl0ewN"
      },
      "source": [
        "from transformers import AutoModel, AutoTokenizer, GPT2LMHeadModel\n",
        "import datasets\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "import warnings"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJvskQ8T0ewO"
      },
      "source": [
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diaudnwY0ewQ"
      },
      "source": [
        "# import transformers\n",
        "# print(transformers.__version__)  # should be above or equal 4.0.1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVYGHVGT0ewS"
      },
      "source": [
        "В этом модуле мы с вами познакомились с большими языковыми моделями и обсудили их возможности, а также ограничения. В этом задании вам предлагается поэкспериментировать с одной из больших языковых моделей -- GPT-2 -- и самим убедиться, так ли она хороша, как о ней рассказывают."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXk55qyo0ewT"
      },
      "source": [
        "## Библиотека transformers\n",
        "\n",
        "Вы уже сталкивались с библиотекой transformers в этом курсе.\n",
        "\n",
        "Мы [загрузим gpt-2](https://huggingface.co/transformers/model_doc/gpt2.html) с помощью метода from_pretrained. Так как мы будем использовать её для задачи языкового моделирования, нам потребуется GPT2LMHeadModel, прочитать про неё можно [тут](https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel). У GPT-2 свой токенизатор, про него можно почитать [тут](https://huggingface.co/transformers/model_doc/gpt2.html#gpt2tokenizer).\n",
        "\n",
        "GPT-2 -- большая модель, время ее работы на CPU будет слишком большим, так что мы сразу отправим её на GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq9fHovY0ewU"
      },
      "source": [
        "model_name = 'gpt2-medium'\n",
        "device = 'cuda'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVJ-USzH0ewW"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTXvTSFN0ewZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1aee345-03c3-479b-e152-45e2b59b38c1"
      },
      "source": [
        "%%time\n",
        "# model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/gpt2-medium')\n",
        "# model = model.to(device)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 9.23 s, sys: 3.03 s, total: 12.3 s\n",
            "Wall time: 18.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJispE3M0ewa"
      },
      "source": [
        "### Подсчёт числа параметров модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asygaKxt0ewb"
      },
      "source": [
        "Чтобы понять, насколько GPT большая модель, подсчитаем число ее параметров"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrKJmIHo0ewc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28a9f459-36ed-45c3-ef9c-ee60eee1386e"
      },
      "source": [
        "# TASK: compute number of model parameters\n",
        "# iterate over model weights and count number of parameters in each of them, then sum it up\n",
        "# note: you may find model.parameters() method useful\n",
        "# Our implementation is 2 lines\n",
        "\n",
        "params = sum([params.numel() for params in model.parameters() ])\n",
        "# YOUR CODE STARTS\n",
        "\n",
        "# YOUR CODE ENDS\n",
        "params"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "354823168"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHOJXyah0ewd"
      },
      "source": [
        "Как видим, это большое число параметров, на несколько порядок больше, чем те модели, которые вы обучали до этого! Использовать такое в продакшене как правило невозможно из-за больших требований по ресурсам"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1-PmKrq0ewd"
      },
      "source": [
        "# Часть 1. Генерация отзывов моделью GPT\n",
        "\n",
        "В преыдущем задании мы генерировали ровно 1 токен, так как нам нужно было предсказать сентимент, а оба слова \"positive\" и \"negative\" есть в словаре GPT.\n",
        "\n",
        "В этом задании мы пойдём дальше и самостоятельно реализуем top-k сэмплирование, чтобы генерировать отзывы на фильмы, подобные тем, что в датасете IMDB.\n",
        "\n",
        "Мы будем продолжать отзывы, используя в качестве \"затравки\" для модели начало реальных ревью."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUnAhWXm0ewd"
      },
      "source": [
        "### Top-k & Top-p sampling\n",
        "\n",
        "Чтобы генерация текста была более разнообразной, мы будем использовать top k сэмплирование, которое мы изучали в этом юните. Его смысл в том, что на каждом шаге мы перед сэмплированием зануляем вероятности всех слов, кроме k самых вероятных."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T4TM0EL0ewf"
      },
      "source": [
        "def topk_sample(scores, k):\n",
        "    \"\"\"\n",
        "    Sample from logits using multinomial distribution\n",
        "    Before sampling, all logits except k greatest are zeroed out.\n",
        "    Args:\n",
        "        scores: scores for every word in the vocabulary. Must be 1-dimensional: (num_words_in_vocab)\n",
        "        k: int, how many hypotheses to sample from\n",
        "    Returns:\n",
        "        1 draw from dictribution, torch.LongTensor of shape (1)\n",
        "    \"\"\"\n",
        "    assert k >= 1, 'k must be >=1'\n",
        "    assert scores.ndim == 1, 'logits must have 1 dimension' \n",
        "\n",
        "    # TASK: implement top-k sampling\n",
        "    # First, get top k values and theoir indices using torch.topk\n",
        "\n",
        "\n",
        "\n",
        "    # 2nd, fill logits with some small value (e. g. 1e-6)\n",
        "    # Next, move values back to their place. Note that you will find ellipsis (...) and None useful\n",
        "    # Finally, use softmax to obtain probabilities and get \n",
        "    # Our implementation is 6 lines\n",
        "    \n",
        "    # YOUR CODE STARTS\n",
        "    k= 2\n",
        "    values, indexes = scores.topk(k)\n",
        "\n",
        "    scores = scores.fill_(1e-6)\n",
        "    scores[indexes] = values\n",
        "\n",
        "    predicted_ids = F.softmax(scores)\n",
        "    # YOUR CODE ENDS\n",
        "    return predicted_ids"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvm41e190ewf"
      },
      "source": [
        "Проверка кода:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFpPyxFt0ewg",
        "outputId": "0055cb2d-9d6a-4478-82f5-11e96e4d98bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "logits = torch.randn(5)\n",
        "for _ in range(10):\n",
        "    res1 = topk_sample(logits, 1)\n",
        "    res2  = topk_sample(logits, 1)\n",
        "    assert res1.ndim == 1\n",
        "    assert res1.equal(res2)\n",
        "equal = True\n",
        "for _ in range(10):\n",
        "    if not topk_sample(logits, 2).equal(topk_sample(logits, 2)):\n",
        "        equal = False\n",
        "assert equal\n",
        "print('OK!')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2VBo0eotL6Z"
      },
      "source": [
        "Также мы будем использовать top-p sampling, который мы уже проходили на лекциях."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ8divSy0ewh"
      },
      "source": [
        "def topp_sample(scores, p):\n",
        "    \"\"\"\n",
        "    Sample from logits using multinomial distribution\n",
        "    Before sampling, all logits except those largest\n",
        "    whose cumsum exceeds p are zeroed out\n",
        "    Args:\n",
        "        scores: scores for every word in the vocabulary. Must be 1-dimensional: (num_words_in_vocab)\n",
        "        p: float, cumulative probability of most high-scored tokens\n",
        "    Returns:\n",
        "        1 draw from dictribution, torch.LongTensor of shape (1)\n",
        "    \"\"\"\n",
        "    assert  0 < p < 1, 'p must be between 0 and 1'\n",
        "    assert scores.ndim == 1, 'logits must have 1 dimension' \n",
        "\n",
        "    # TASK: implement top-p sampling\n",
        "    # 1. sort scores with descending order\n",
        "    # 2. get cumulative sum of softmaxed logits\n",
        "    # 3. get indices when cumulative sum is larger than p. These indices should be zeroed out\n",
        "    # 4. get real indices that should be zeroes out from sorted indices. Use tensor slicing\n",
        "    # 5. fill these indices with some small value (e. g. -1e6)\n",
        "    # 6. sample, as you did in top-k sampling\n",
        "    # Our implementation is 7 lines\n",
        "    \n",
        "    # YOUR CODE STARTS\n",
        "\n",
        "    # YOUR CODE ENDS\n",
        "    return predicted_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-0t8dHgtN3N"
      },
      "source": [
        "Проверка кода:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrDbJBlc0ewi"
      },
      "source": [
        "logits = torch.Tensor([-.1, -.2, .9])\n",
        "for _ in range(10):\n",
        "    res1 = topp_sample(logits, 0.8)\n",
        "    res2  = topp_sample(logits, 0.8)\n",
        "    assert res1.ndim == 0\n",
        "\n",
        "    assert res1.equal(res2)\n",
        "logits = torch.Tensor([.5, .5, .5])\n",
        "not_equal = False\n",
        "for _ in range(10):\n",
        "    if not topp_sample(logits, 0.8).equal(topp_sample(logits, 0.8)):\n",
        "        not_equal = True\n",
        "assert not_equal\n",
        "print('OK!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgqFirKjtPrz"
      },
      "source": [
        "Теперь напишите функцию генерации текста:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sNwScYY0ewj"
      },
      "source": [
        "def generate_text(model, ids, length, k=None, p=None):\n",
        "    \"\"\"\n",
        "    Generate text with language model with ids as prompt\n",
        "    Args:\n",
        "        model: huggingface LM model\n",
        "        ids: input ids, from tokenizer.encode, must be 2-dimensional\n",
        "        length: int, how many tokens to geenrate\n",
        "        k: int, how many hypotheses to sample from in top-k sampling\n",
        "    Returns:\n",
        "        token ids from generated text, together with token ids of prompts, 2d LongTensor\n",
        "    \"\"\"\n",
        "    assert length >= 1\n",
        "    if k and p:\n",
        "        raise RuntimeError('Cannot use topk and topp sampling simultaneously')\n",
        "    # TASK: write generation loop\n",
        "    # For every timestamp:\n",
        "    # - obtain model output\n",
        "    # - get logits for last symbol\n",
        "    # - apply topk sampling to get new index\n",
        "    # - concatenate it to the ids to form new input\n",
        "    # in the end, the `ids` tensor will have full generation\n",
        "    # our implementation is 5 lines\n",
        "    \n",
        "    # YOUR CODE STARTS\n",
        "\n",
        "    # YOUR CODE ENDS\n",
        "    return ids.cpu().detach()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB6g8UWp0ewj"
      },
      "source": [
        "Проверка кода:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8mpDF_O0ewk"
      },
      "source": [
        "ids = torch.randint(0, 1000, (4,)).to(device)\n",
        "generation = generate_text(model, ids, 4, k=2)\n",
        "\n",
        "assert generation.shape == torch.Size([8])\n",
        "\n",
        "generation = generate_text(model, ids, 3, p=.5)\n",
        "\n",
        "assert generation.shape == torch.Size([7])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5uhuTT30ewl"
      },
      "source": [
        "### Запускаем генерацию\n",
        "\n",
        "Зададим модели какой-нибудь prompt, например восторженный отзыв о фильме, и посмотрим, как она продолжит его. Так как модель сэмплит на каждом шаге, нам интересно посмотреть на несколько траекторий сразу. Для этого мы и писали батч-режим в генерации."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jntDAgA90ewm"
      },
      "source": [
        "prompt = 'What a lovely film !'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_OBJte00ewm"
      },
      "source": [
        "Подготовим входные данные для GPT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSwU95VV0ewn"
      },
      "source": [
        "ids = tokenizer.encode(prompt, return_tensors=\"pt\").squeeze().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tew84_O80ewo"
      },
      "source": [
        "Запускаем генерацию! Можно запустить на 10-20 токенов, можно и больше"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSHTNksk0ewo"
      },
      "source": [
        "n_tries = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKRdXJYA0ewo"
      },
      "source": [
        "generations = [generate_text(model, ids, length=40, k=10).cpu().detach() for _ in range(n_tries)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq0AoI_50ewp"
      },
      "source": [
        "for generation in generations:\n",
        "    print(tokenizer.decode(generation))\n",
        "    print('\\n' + '=' * 40 + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHM7qBeg0ewr"
      },
      "source": [
        "А теперь запустим генерацию с top-p сэмплированием"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ4uzsOW0ewr"
      },
      "source": [
        "generations = [generate_text(model, ids, length=4, p=0.5).cpu().detach() for _ in range(n_tries)]\n",
        "for generation in generations:\n",
        "    print(tokenizer.decode(generation))\n",
        "    print('\\n' + '=' * 40 + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isQrAwqG0ews"
      },
      "source": [
        "# Часть 2. Few-shot learning\n",
        "\n",
        "Одной из особенностей GPT, про которую авторы написали, является способность к few-shot learning. В этой части домашнего задания мы на примере хорошо знакомой нам задачи классификации текстов исследуем, как хорошо модель умеет различать сентимент отзывов без дополнительного обучения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2osAS3f0ews"
      },
      "source": [
        "## Датасет.\n",
        "Мы будем снова использовать датасет imdb, на котором мы уже обучили несколько моделей. Так как обучать саму модель мы не будем (в few-shot парадигме веса не меняются), то можем сразу взять валидационную часть датасета."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQpgFvhq0ewt"
      },
      "source": [
        "text_dataset = datasets.load_dataset(\"imdb\")\n",
        "texts = text_dataset[\"test\"][\"text\"]\n",
        "labels = text_dataset[\"test\"][\"label\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DPlm7ad0ewu"
      },
      "source": [
        "### Метки для задачи классификации на естественном языке\n",
        "Поскольку языковая модель знает только токены языка и не знает маппинга между индексами и лейблами, нам нужно будет сравнивать лейблы напрямую. С одной стороны, это создаёт дополнительные сложности: модель может предсказать \"Positive Sentiment\" вместо \"positive\", поэтому нам потребуется минимальный постпроцессинг, чтобы учесть большинство таких случаев."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7qRUd-X0ewu"
      },
      "source": [
        "mapping = {idx: label for idx, label in enumerate(text_dataset[\"train\"].features[\"label\"].names)}\n",
        "mapping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmkvUl1W0ewv"
      },
      "source": [
        "### Игрушечный датасет\n",
        "Чтобы быстрее прототипировать наши вводы (prompts), полезно взять маленькую часть датасета и проверять гипотезы на ней. Так как imdb упорядочен по лейблам, а самих лейблов всего 2, мы берём одинаковое количество примеров с начала и с конца."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUrFRggy0ewv"
      },
      "source": [
        "def get_toy_dataset(dataset, split, length = 100):\n",
        "    texts = dataset[split][\"text\"]\n",
        "    labels = dataset[split][\"label\"]\n",
        "    small_texts = texts[:length // 2] + texts[-length//2:]\n",
        "    small_labels = labels[:length // 2] + labels[-length//2:]\n",
        "    return small_texts, small_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9gjyJGQ0eww"
      },
      "source": [
        "assert len(get_toy_dataset(text_dataset, \"test\", 100)[0]) == 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQovzfQu0eww"
      },
      "source": [
        "## Few-shot learning\n",
        "\n",
        "В данном задании мы будем не обучать модель на данных, а использовать уже обученную на огромном корпусе языковую модель, чтобы \"угадывать\" сентимент отзывов.\n",
        "\n",
        "Модель будет видеть в качестве затравки (prompt) описание задачи на естественном языке.\n",
        "\n",
        "Нам потребуется:\n",
        "- описание задачи\n",
        "- лейблы\n",
        "- несколько (хотя бы 3) примеров\n",
        "- вспомогательные маркеры (разделитель примеров и маркер конца входа)\n",
        "\n",
        "Например:\n",
        "\n",
        "```\n",
        "Translate from English to French\n",
        "sea otter => loutre de mer\n",
        "peppermint => menthe poivrée\n",
        "plush girafe  => \n",
        "```\n",
        "\n",
        "Первая строка здесь -- описание задачи. Мы решаем задачу классификации, поэтому нам нужно\n",
        "описать задачу примерно как `To which category does the text belong? positive , negative `\n",
        "\n",
        "Вторая и третья строки -- примеры входа и выхода. В нашем случае примерами входа и выхода будут служить тексты ревью и метки классов positive и negative (а не 0 и 1, как раньше).\n",
        "\n",
        "Наконец, последняя строка -- это тот текст, который модель должна классифицировать. Модель уже \"видела\" выше пример того, что после маркера конца входа идёт сентимент, и мы ожидаем, что она сможет выучить эту закономерность.\n",
        "\n",
        "В данном задании часть входа будет написана за вас, вам надо будет придумать обучающие примеры для few-shot learning'а"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZQgn9uG0ewx"
      },
      "source": [
        "marker = ' => '\n",
        "separator = '\\n'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFxS1Qnp0ewx"
      },
      "source": [
        "labels = [\"positive\" , \"negative\"]\n",
        "labels_text = \" , \".join(labels)\n",
        "task_description = f'To which category does the text belong?: {labels_text} ' + separator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-Z-wR-k0ewy"
      },
      "source": [
        "examples = [\n",
        "    # TASK define examples\n",
        "    # They should be a tuple with text and label\n",
        "    # label should be from \"labels\"\n",
        "    # Get 3-10 different examples, they should cover both positive \n",
        "    # and negative reviews\n",
        "    # YOUR CODE STARTS\n",
        "\n",
        "    # YOUR CODE ENDS\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aso2fG6_0ewz"
      },
      "source": [
        "assert len(examples) > 1, 'Write at least two different examples'\n",
        "for ex in examples:\n",
        "    assert ex[1] in labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Heu6rvlK0ewz"
      },
      "source": [
        "input = task_description  + separator.join([marker.join(example) for example in examples])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buEhQeHo0ew1"
      },
      "source": [
        "print(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIHpblK80ew2"
      },
      "source": [
        "Сравните получившийся инпут с примером выше. Мы получили то, что надо!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xds7hkfQ0ew3"
      },
      "source": [
        "n_chars = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Slih0Fur0ew3"
      },
      "source": [
        "def get_prompt(beginning, separator, text, marker) -> str:\n",
        "    return ' '.join([input,  separator, text[:n_chars] + text[-n_chars:], marker])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmQrc0fs0ew5"
      },
      "source": [
        "## Время проверить работу нашей модели на маленьком датасете!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_0WPdws0ew5"
      },
      "source": [
        "small_texts, small_labels = get_toy_dataset(text_dataset, \"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40bwRicj0ew5"
      },
      "source": [
        "Так как мы предсказываем сентимент текста, каждый из который задаётся одним токеном, то нам\n",
        "- не надо сэмплировать. Мы хотим, чтобы модель честно учитывала вероятности для токенов\n",
        "- достаточно сделать один шаг генерации. Модель должна будет предсказать лейбл текста, а он, как мы выяснили, однотокенный.\n",
        "\n",
        "Таким образом, нам просто надо сделать один шаг жадного предсказания (greedy decoding)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8ax1sUE0ew6"
      },
      "source": [
        "def predict_on_text(model, ids) -> int:\n",
        "    \"\"\"\n",
        "    Run Language Model on text and use logits from last time \n",
        "    step to predict sentence category with greedy decoding\n",
        "    Args:\n",
        "        model: huggingface LM model\n",
        "        ids: LongTensor . Text encoded with tokenizer.\n",
        "    Returns:\n",
        "        model prediction, index of most probable word\n",
        "    \"\"\"\n",
        "    # TASK: run a model and get index most probable logit\n",
        "    # HINT: do not forget to detach tensors\n",
        "    # HINT 2: GPT-2 model returns tuple where logits are stored in the\n",
        "    # 1st item. You do not need 2nd item at all\n",
        "    # Our implementation is 2 lines\n",
        "    # YOUR CODE STARTS\n",
        "\n",
        "    # YOUR CODE ENDS\n",
        "    return idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz6LoYwK0ew7"
      },
      "source": [
        "Проверка кода:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Uo318VV0ew7"
      },
      "source": [
        "ids = tokenizer.encode('hello world !', return_tensors=\"pt\",)\n",
        "prediction = predict_on_text(model, ids.to(device))\n",
        "assert isinstance(prediction, int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69zOpqp90ew7"
      },
      "source": [
        "def predict_on_dataset(model, texts, target, mapping):\n",
        "    \"\"\"\n",
        "    Run Language Model on a dataset and use its predictions as\n",
        "    labels for sentences\n",
        "    Args:\n",
        "        model: huggingface LM model\n",
        "        texts: List[str], texts of dataset\n",
        "        target: List[int], indices of classes\n",
        "        mapping: Dict[int, str] mapping from class indices to class labels\n",
        "    Returns:\n",
        "        model predictions, \"as is\", List[str]\n",
        "        target labels, after mapping, List[str]\n",
        "    \"\"\"\n",
        "    if len(texts) != len(target):\n",
        "        raise RuntimeError('Texts and target lengths mismatch')\n",
        "    # max_length has additional -1 because we will generate exactly 1 token \n",
        "    # with LM\n",
        "    max_length = model.config.n_ctx - 1\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    for idx in tqdm(range(len(texts))):\n",
        "        text, label = texts[idx], target[idx]\n",
        "        ids = tokenizer.encode(\n",
        "            get_prompt(input, separator, text, marker), \n",
        "            add_special_tokens=False, \n",
        "            return_tensors=\"pt\",\n",
        "            max_length=max_length\n",
        "        )\n",
        "        idx = predict_on_text(model, ids.to(device))\n",
        "        predictions.append(tokenizer.decode([idx]))  # generation.cpu().numpy()[0][-1]\n",
        "        labels.append(mapping[label]) \n",
        "    return predictions, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abvFt18q0ew8"
      },
      "source": [
        "predictions, target = predict_on_dataset(model, small_texts, small_labels, mapping)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIECT9O-0ew9"
      },
      "source": [
        "### Замер качества"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13EoUAQ10ew9"
      },
      "source": [
        "Посмотрим, что выдаёт нам языковая модель:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgiZYWnW0ew-"
      },
      "source": [
        "print(predictions[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjzhPs7i0ew_"
      },
      "source": [
        "Можно заметить, что предсказанные метки не совсем похожи на pos и neg, которые были у нас в таргете. Напишем простую функцию, которая нормализует предсказания модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPB04pnd0exA"
      },
      "source": [
        "def normalize_prediction(raw_output: str) -> str:\n",
        "    \"\"\"\n",
        "    Helper that transforms model prediction to normal\n",
        "    For example, ' positive' -> 'pos', ' Negative' -> neg\n",
        "    Args:\n",
        "        raw_output: str, output from language model\n",
        "    Returns:\n",
        "        normalized value: no leading/trailing spaces, lowercase,\n",
        "        at most 3 chars long\n",
        "    \"\"\"\n",
        "    # TASK: write prediction normalizer\n",
        "    # You need to remove spaces, lowercase and get only 3 leading chars\n",
        "    # Our implementation is 1 line\n",
        "    # YOUR CODE STARTS\n",
        "\n",
        "    # YOUR CODE ENDS\n",
        "    return normalized_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euC0k3nFtojd"
      },
      "source": [
        "Напишем функцию, вычисляющую точность:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CozGfPMo0exA"
      },
      "source": [
        "def accuracy(predictions, labels) -> float:\n",
        "    \"\"\"\n",
        "    Helper that transforms model prediction to normal\n",
        "    For example, ' positive' -> 'pos', ' Negative' -> neg\n",
        "    Args:\n",
        "        predictions: List[str], model predictions, should be labels in natural language\n",
        "        labels: List[str], target labels\n",
        "    Returns:\n",
        "        accuracy, float from [0, 1]\n",
        "    \"\"\"\n",
        "    if not labels:\n",
        "        # sanity check to avoid zero division\n",
        "        return 0\n",
        "    if len(predictions) != len(labels):\n",
        "        raise ValueError(f'Predictions and labels have mismatched length')\n",
        "    total = len(predictions)\n",
        "    match = 0\n",
        "    # TASK: calculate accuracy\n",
        "    # For every pair of predicted and real labels, check that they coincide.\n",
        "    # You can use zip() method for iteration over two sequences\n",
        "    # simultaneously.\n",
        "    # Our implementation is 3 lines\n",
        "    # YOUR CODE STARTS\n",
        "\n",
        "    # YOUR CODE ENDS\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_632NyJB0exA"
      },
      "source": [
        "normalized_predictions = [normalize_prediction(label) for label in predictions]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1hyGmSH0exB"
      },
      "source": [
        "accuracy(normalized_predictions, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04SFZ3Wo0exC"
      },
      "source": [
        "### Ура, модель без обучения работает лучше случайного угадывания!\n",
        "\n",
        "Тем не менее, это заметно хуже, чем у полносвязной нейросети. **Учитывая требования по ресурсам и время \n",
        "работы, использование такой модели в реальных задачах непрактично!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srG4Gk3p0exC"
      },
      "source": [
        "## А теперь замер на всем тестовом сете:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoTvsnWP0exD"
      },
      "source": [
        "predictions, target = predict_on_dataset(model, texts, labels, mapping)\n",
        "normalized_predictions = [normalize_prediction(label) for label in predictions]\n",
        "accuracy(normalized_predictions, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NZiUCu2xmu_"
      },
      "source": [
        "Все задание считается выполненным, если вы достигли accuracy 0.6 и выше на тестовом датасете. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xybBRqOz0exE"
      },
      "source": [
        "Данный пример призван продемонстрировать, что большая языковая модель хоть и справляется с задачей без лишних данных, но делает это заметно хуже специально созданных для задачи моделей, к тому же работает непростительно долго."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cFiadD10exE"
      },
      "source": [
        "### Резюме\n",
        "\n",
        "В этом задании мы применили большие языковые модели на практике и решили с её помощью 2 задачи:\n",
        "- conditional генерация текста\n",
        "- few-shot классификация текстов\n",
        "\n",
        "Мы увидели, что модель генерирует связный текст и может решать задачу классификации без изменений своих весов.\n",
        "\n",
        "Несмотря на возможности языковых моделей, их применение на практике всё ещё осложнено большими потребляемыми ресурсами, а также невозможностью интерпретировать предсказания."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUVsBkng0exE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}