{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "gpt_2_text_clf_unresolved.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "04SFZ3Wo0exC"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f01b16eef69342d492415f6df7bf729a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_58feca82d9fa4f3aa04ed33d07c31c5d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_eb458ed0d48b40bd98bdf0139691548d",
              "IPY_MODEL_844579f72e3c4dffa61eb8043e43dad0",
              "IPY_MODEL_a0c3110250494508a8ee5b0ca69df3e7"
            ]
          }
        },
        "58feca82d9fa4f3aa04ed33d07c31c5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eb458ed0d48b40bd98bdf0139691548d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dbffdbbd90164c5a8f54c64710c57189",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1c9275e91048451b89359b4965360994"
          }
        },
        "844579f72e3c4dffa61eb8043e43dad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3507c19ba3934bda8bc4dd0972df6032",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 718,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 718,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e0e5231f6e8442d9a5234e7f082b822c"
          }
        },
        "a0c3110250494508a8ee5b0ca69df3e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e3d20bed7f674daaaa9f72ac61487c32",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 718/718 [00:00&lt;00:00, 14.9kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ecf147c2644b48cebc4a19760c0b88db"
          }
        },
        "dbffdbbd90164c5a8f54c64710c57189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1c9275e91048451b89359b4965360994": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3507c19ba3934bda8bc4dd0972df6032": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e0e5231f6e8442d9a5234e7f082b822c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e3d20bed7f674daaaa9f72ac61487c32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ecf147c2644b48cebc4a19760c0b88db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e2a422ce234e425aa05e893bc077f874": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4d8f18c0fc784232ba69eb0cb24c51f2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_da5325ecaa854aac942422af79b25d79",
              "IPY_MODEL_4327c2680daf46a5832c319af176e757",
              "IPY_MODEL_af5c2fd38c9541debace9dfbf68bd424"
            ]
          }
        },
        "4d8f18c0fc784232ba69eb0cb24c51f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "da5325ecaa854aac942422af79b25d79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7315fa2d3ad741d3a940aaf841511793",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6a480a83eab34cfa9ea9af3bf307300f"
          }
        },
        "4327c2680daf46a5832c319af176e757": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_91723812eeb64678afbec2cf4bf7d443",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1042301,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1042301,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7e9b74fba76c431d88b86dcc42e21e9f"
          }
        },
        "af5c2fd38c9541debace9dfbf68bd424": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_237da895ef9b40318a34f08823a65950",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 759kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_35581157ea38434cb96fe75decc9c713"
          }
        },
        "7315fa2d3ad741d3a940aaf841511793": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6a480a83eab34cfa9ea9af3bf307300f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "91723812eeb64678afbec2cf4bf7d443": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7e9b74fba76c431d88b86dcc42e21e9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "237da895ef9b40318a34f08823a65950": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "35581157ea38434cb96fe75decc9c713": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f7c9d2da910d4f149aab902a5eed4ff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_70aef096e13a498eac8a91a01538e405",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4034eb644c994db6a2740222f755b3a6",
              "IPY_MODEL_7821344c89484fcf9270b1cd51209112",
              "IPY_MODEL_cf4fd73ce4d84559911362a3400a408f"
            ]
          }
        },
        "70aef096e13a498eac8a91a01538e405": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4034eb644c994db6a2740222f755b3a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7b094c8896444e50a837c394257ef3d0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b02aaca377364041bf648a0ba47642af"
          }
        },
        "7821344c89484fcf9270b1cd51209112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_071ca8dcce5546c59c49ac09a22e19e5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_626efccb25774cca817e1753826b1202"
          }
        },
        "cf4fd73ce4d84559911362a3400a408f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_520bafadfc594f6fae060c517e627d45",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 820kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2f86cc2dd87441898b271ebf752daaa4"
          }
        },
        "7b094c8896444e50a837c394257ef3d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b02aaca377364041bf648a0ba47642af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "071ca8dcce5546c59c49ac09a22e19e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "626efccb25774cca817e1753826b1202": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "520bafadfc594f6fae060c517e627d45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2f86cc2dd87441898b271ebf752daaa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7b1586e43d89401b8256a43e67d46390": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f327ac1bc4b9424ba6656b22e4e2f4ea",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b8ab6abe4d77455f9ea9c13c99cb4ddd",
              "IPY_MODEL_c035d7a941414972b1ddb39983c819c4",
              "IPY_MODEL_4118658228f641d0997f32da0c1a1761"
            ]
          }
        },
        "f327ac1bc4b9424ba6656b22e4e2f4ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b8ab6abe4d77455f9ea9c13c99cb4ddd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2b37e4a4a8064382848a36846e247419",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e115b978acba4994accb62f387d4d608"
          }
        },
        "c035d7a941414972b1ddb39983c819c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c4f1aca251994686841c1bb3b5e9720f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1520013706,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1520013706,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_44a21df0d4194fae94689b22ca577f6b"
          }
        },
        "4118658228f641d0997f32da0c1a1761": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f0b3ba032ef64fe5b4c4931c96cde32d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.52G/1.52G [01:19&lt;00:00, 18.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b618355482de471d889e5bee9ad04e65"
          }
        },
        "2b37e4a4a8064382848a36846e247419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e115b978acba4994accb62f387d4d608": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c4f1aca251994686841c1bb3b5e9720f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "44a21df0d4194fae94689b22ca577f6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f0b3ba032ef64fe5b4c4931c96cde32d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b618355482de471d889e5bee9ad04e65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/delhian/NLP_course/blob/master/Home%20Tasks/gpt_2_text_clf_unresolved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd5wsNhS0ewB"
      },
      "source": [
        "## Возможности и ограничения языковых моделей"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dSYyxBI0ewG"
      },
      "source": [
        "**Тут есть важное действие, которое нужно сделать перед тем, как запускать этот ноутбук**\n",
        "\n",
        "Если вы делаете это задание в Google Colab, первым делом переключите Runtime в GPU. Это задание нормально посчитается и на CPU, но некоторые из будущих кейсов потребуют GPU (либо вам придётся несколько часов или даже дней, чтобы модель обучилась - и мы не преувиличиваем). Ещё мы рекомендуем переключить язык интерфейса на английский, потому что русская локализация ужасна да и вообще не нужна.\n",
        "\n",
        "О том, как переключить рантайм: https://www.geeksforgeeks.org/how-to-use-google-colab/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqRGVCt50ewL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f14d5ac-ca16-40b2-8fa0-b42c8fcc0635"
      },
      "source": [
        "!pip install datasets transformers[torch]==2.10.0 tqdm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.13.2-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting transformers[torch]==2.10.0\n",
            "  Downloading transformers-2.10.0-py3-none-any.whl (660 kB)\n",
            "\u001b[K     |████████████████████████████████| 660 kB 37.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (3.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (1.19.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 33.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (2.23.0)\n",
            "Collecting tokenizers==0.7.0\n",
            "  Downloading tokenizers-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 36.1 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 46.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (2019.12.20)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (1.9.0+cu111)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.10.0-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 47.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<0.1.0,>=0.0.19\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 46.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 29.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.13)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[torch]==2.10.0) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[torch]==2.10.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[torch]==2.10.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[torch]==2.10.0) (1.24.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 47.9 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 40.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[torch]==2.10.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[torch]==2.10.0) (1.0.1)\n",
            "Installing collected packages: multidict, yarl, async-timeout, tokenizers, sentencepiece, sacremoses, fsspec, aiohttp, xxhash, transformers, huggingface-hub, datasets\n",
            "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.13.2 fsspec-2021.10.0 huggingface-hub-0.0.19 multidict-5.2.0 sacremoses-0.0.46 sentencepiece-0.1.96 tokenizers-0.7.0 transformers-2.10.0 xxhash-2.0.2 yarl-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcr7RqWl0ewN"
      },
      "source": [
        "from transformers import AutoModel, AutoTokenizer, GPT2LMHeadModel\n",
        "import datasets\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "import warnings"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJvskQ8T0ewO"
      },
      "source": [
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diaudnwY0ewQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6bec34f-00e7-4d95-b84f-90118fc4ddd4"
      },
      "source": [
        "import transformers\n",
        "print(transformers.__version__)  # should be above or equal 4.0.1"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVYGHVGT0ewS"
      },
      "source": [
        "В этом модуле мы с вами познакомились с большими языковыми моделями и обсудили их возможности, а также ограничения. В этом задании вам предлагается поэкспериментировать с одной из больших языковых моделей -- GPT-2 -- и самим убедиться, так ли она хороша, как о ней рассказывают."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXk55qyo0ewT"
      },
      "source": [
        "## Библиотека transformers\n",
        "\n",
        "Вы уже сталкивались с библиотекой transformers в этом курсе.\n",
        "\n",
        "Мы [загрузим gpt-2](https://huggingface.co/transformers/model_doc/gpt2.html) с помощью метода from_pretrained. Так как мы будем использовать её для задачи языкового моделирования, нам потребуется GPT2LMHeadModel, прочитать про неё можно [тут](https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel). У GPT-2 свой токенизатор, про него можно почитать [тут](https://huggingface.co/transformers/model_doc/gpt2.html#gpt2tokenizer).\n",
        "\n",
        "GPT-2 -- большая модель, время ее работы на CPU будет слишком большим, так что мы сразу отправим её на GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq9fHovY0ewU"
      },
      "source": [
        "model_name = 'gpt2-medium'\n",
        "device = 'cuda'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVJ-USzH0ewW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "f01b16eef69342d492415f6df7bf729a",
            "58feca82d9fa4f3aa04ed33d07c31c5d",
            "eb458ed0d48b40bd98bdf0139691548d",
            "844579f72e3c4dffa61eb8043e43dad0",
            "a0c3110250494508a8ee5b0ca69df3e7",
            "dbffdbbd90164c5a8f54c64710c57189",
            "1c9275e91048451b89359b4965360994",
            "3507c19ba3934bda8bc4dd0972df6032",
            "e0e5231f6e8442d9a5234e7f082b822c",
            "e3d20bed7f674daaaa9f72ac61487c32",
            "ecf147c2644b48cebc4a19760c0b88db",
            "e2a422ce234e425aa05e893bc077f874",
            "4d8f18c0fc784232ba69eb0cb24c51f2",
            "da5325ecaa854aac942422af79b25d79",
            "4327c2680daf46a5832c319af176e757",
            "af5c2fd38c9541debace9dfbf68bd424",
            "7315fa2d3ad741d3a940aaf841511793",
            "6a480a83eab34cfa9ea9af3bf307300f",
            "91723812eeb64678afbec2cf4bf7d443",
            "7e9b74fba76c431d88b86dcc42e21e9f",
            "237da895ef9b40318a34f08823a65950",
            "35581157ea38434cb96fe75decc9c713",
            "f7c9d2da910d4f149aab902a5eed4ff3",
            "70aef096e13a498eac8a91a01538e405",
            "4034eb644c994db6a2740222f755b3a6",
            "7821344c89484fcf9270b1cd51209112",
            "cf4fd73ce4d84559911362a3400a408f",
            "7b094c8896444e50a837c394257ef3d0",
            "b02aaca377364041bf648a0ba47642af",
            "071ca8dcce5546c59c49ac09a22e19e5",
            "626efccb25774cca817e1753826b1202",
            "520bafadfc594f6fae060c517e627d45",
            "2f86cc2dd87441898b271ebf752daaa4"
          ]
        },
        "outputId": "0ef64540-afe1-4621-e490-09b88302ab69"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f01b16eef69342d492415f6df7bf729a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/718 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2a422ce234e425aa05e893bc077f874",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7c9d2da910d4f149aab902a5eed4ff3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTXvTSFN0ewZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "7b1586e43d89401b8256a43e67d46390",
            "f327ac1bc4b9424ba6656b22e4e2f4ea",
            "b8ab6abe4d77455f9ea9c13c99cb4ddd",
            "c035d7a941414972b1ddb39983c819c4",
            "4118658228f641d0997f32da0c1a1761",
            "2b37e4a4a8064382848a36846e247419",
            "e115b978acba4994accb62f387d4d608",
            "c4f1aca251994686841c1bb3b5e9720f",
            "44a21df0d4194fae94689b22ca577f6b",
            "f0b3ba032ef64fe5b4c4931c96cde32d",
            "b618355482de471d889e5bee9ad04e65"
          ]
        },
        "outputId": "0532ad75-ad8b-4ade-a344-efa20008c120"
      },
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model = model.to(device)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b1586e43d89401b8256a43e67d46390",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJispE3M0ewa"
      },
      "source": [
        "### Подсчёт числа параметров модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asygaKxt0ewb"
      },
      "source": [
        "Чтобы понять, насколько GPT большая модель, подсчитаем число ее параметров"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrKJmIHo0ewc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23248cd5-492e-4a4a-f4d7-d341769c6513"
      },
      "source": [
        "# TASK: compute number of model parameters\n",
        "# iterate over model weights and count number of parameters in each of them, then sum it up\n",
        "# note: you may find model.parameters() method useful\n",
        "# Our implementation is 2 lines\n",
        "\n",
        "params = sum([params.numel() for params in model.parameters() ])\n",
        "# YOUR CODE STARTS\n",
        "\n",
        "# YOUR CODE ENDS\n",
        "params"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "354823168"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHOJXyah0ewd"
      },
      "source": [
        "Как видим, это большое число параметров, на несколько порядок больше, чем те модели, которые вы обучали до этого! Использовать такое в продакшене как правило невозможно из-за больших требований по ресурсам"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1-PmKrq0ewd"
      },
      "source": [
        "# Часть 1. Генерация отзывов моделью GPT\n",
        "\n",
        "В преыдущем задании мы генерировали ровно 1 токен, так как нам нужно было предсказать сентимент, а оба слова \"positive\" и \"negative\" есть в словаре GPT.\n",
        "\n",
        "В этом задании мы пойдём дальше и самостоятельно реализуем top-k сэмплирование, чтобы генерировать отзывы на фильмы, подобные тем, что в датасете IMDB.\n",
        "\n",
        "Мы будем продолжать отзывы, используя в качестве \"затравки\" для модели начало реальных ревью."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUnAhWXm0ewd"
      },
      "source": [
        "### Top-k & Top-p sampling\n",
        "\n",
        "Чтобы генерация текста была более разнообразной, мы будем использовать top k сэмплирование, которое мы изучали в этом юните. Его смысл в том, что на каждом шаге мы перед сэмплированием зануляем вероятности всех слов, кроме k самых вероятных."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T4TM0EL0ewf"
      },
      "source": [
        "def topk_sample(scores, k):\n",
        "    \"\"\"\n",
        "    Sample from logits using multinomial distribution\n",
        "    Before sampling, all logits except k greatest are zeroed out.\n",
        "    Args:\n",
        "        scores: scores for every word in the vocabulary. Must be 1-dimensional: (num_words_in_vocab)\n",
        "        k: int, how many hypotheses to sample from\n",
        "    Returns:\n",
        "        1 draw from dictribution, torch.LongTensor of shape (1)\n",
        "    \"\"\"\n",
        "    assert k >= 1, 'k must be >=1'\n",
        "    assert scores.ndim == 1, 'logits must have 1 dimension' \n",
        "\n",
        "    # TASK: implement top-k sampling\n",
        "    # First, get top k values and theoir indices using torch.topk\n",
        "    # 2nd, fill logits with some small value (e. g. 1e-6)\n",
        "    # Next, move values back to their place. Note that you will find ellipsis (...) and None useful\n",
        "    # Finally, use softmax to obtain probabilities and get \n",
        "    # Our implementation is 6 lines\n",
        "    \n",
        "    # YOUR CODE STARTS\n",
        "\n",
        "    # YOUR CODE ENDS\n",
        "    return predicted_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvm41e190ewf"
      },
      "source": [
        "Проверка кода:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFpPyxFt0ewg"
      },
      "source": [
        "logits = torch.randn(5)\n",
        "for _ in range(10):\n",
        "    res1 = topk_sample(logits, 1)\n",
        "    res2  = topk_sample(logits, 1)\n",
        "    assert res1.ndim == 0\n",
        "\n",
        "    assert res1.equal(res2)\n",
        "not_equal = False\n",
        "for _ in range(10):\n",
        "    if not topk_sample(logits, 2).equal(topk_sample(logits, 2)):\n",
        "        not_equal = True\n",
        "assert not_equal\n",
        "print('OK!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2VBo0eotL6Z"
      },
      "source": [
        "Также мы будем использовать top-p sampling, который мы уже проходили на лекциях."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ8divSy0ewh"
      },
      "source": [
        "def topp_sample(scores, p):\n",
        "    \"\"\"\n",
        "    Sample from logits using multinomial distribution\n",
        "    Before sampling, all logits except those largest\n",
        "    whose cumsum exceeds p are zeroed out\n",
        "    Args:\n",
        "        scores: scores for every word in the vocabulary. Must be 1-dimensional: (num_words_in_vocab)\n",
        "        p: float, cumulative probability of most high-scored tokens\n",
        "    Returns:\n",
        "        1 draw from dictribution, torch.LongTensor of shape (1)\n",
        "    \"\"\"\n",
        "    assert  0 < p < 1, 'p must be between 0 and 1'\n",
        "    assert scores.ndim == 1, 'logits must have 1 dimension' \n",
        "\n",
        "    # TASK: implement top-p sampling\n",
        "    # 1. sort scores with descending order\n",
        "    # 2. get cumulative sum of softmaxed logits\n",
        "    # 3. get indices when cumulative sum is larger than p. These indices should be zeroed out\n",
        "    # 4. get real indices that should be zeroes out from sorted indices. Use tensor slicing\n",
        "    # 5. fill these indices with some small value (e. g. -1e6)\n",
        "    # 6. sample, as you did in top-k sampling\n",
        "    # Our implementation is 7 lines\n",
        "    \n",
        "    # YOUR CODE STARTS\n",
        "\n",
        "    # YOUR CODE ENDS\n",
        "    return predicted_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-0t8dHgtN3N"
      },
      "source": [
        "Проверка кода:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrDbJBlc0ewi"
      },
      "source": [
        "logits = torch.Tensor([-.1, -.2, .9])\n",
        "for _ in range(10):\n",
        "    res1 = topp_sample(logits, 0.8)\n",
        "    res2  = topp_sample(logits, 0.8)\n",
        "    assert res1.ndim == 0\n",
        "\n",
        "    assert res1.equal(res2)\n",
        "logits = torch.Tensor([.5, .5, .5])\n",
        "not_equal = False\n",
        "for _ in range(10):\n",
        "    if not topp_sample(logits, 0.8).equal(topp_sample(logits, 0.8)):\n",
        "        not_equal = True\n",
        "assert not_equal\n",
        "print('OK!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgqFirKjtPrz"
      },
      "source": [
        "Теперь напишите функцию генерации текста:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sNwScYY0ewj"
      },
      "source": [
        "def generate_text(model, ids, length, k=None, p=None):\n",
        "    \"\"\"\n",
        "    Generate text with language model with ids as prompt\n",
        "    Args:\n",
        "        model: huggingface LM model\n",
        "        ids: input ids, from tokenizer.encode, must be 2-dimensional\n",
        "        length: int, how many tokens to geenrate\n",
        "        k: int, how many hypotheses to sample from in top-k sampling\n",
        "    Returns:\n",
        "        token ids from generated text, together with token ids of prompts, 2d LongTensor\n",
        "    \"\"\"\n",
        "    assert length >= 1\n",
        "    if k and p:\n",
        "        raise RuntimeError('Cannot use topk and topp sampling simultaneously')\n",
        "    # TASK: write generation loop\n",
        "    # For every timestamp:\n",
        "    # - obtain model output\n",
        "    # - get logits for last symbol\n",
        "    # - apply topk sampling to get new index\n",
        "    # - concatenate it to the ids to form new input\n",
        "    # in the end, the `ids` tensor will have full generation\n",
        "    # our implementation is 5 lines\n",
        "    \n",
        "    # YOUR CODE STARTS\n",
        "\n",
        "    # YOUR CODE ENDS\n",
        "    return ids.cpu().detach()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB6g8UWp0ewj"
      },
      "source": [
        "Проверка кода:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8mpDF_O0ewk"
      },
      "source": [
        "ids = torch.randint(0, 1000, (4,)).to(device)\n",
        "generation = generate_text(model, ids, 4, k=2)\n",
        "\n",
        "assert generation.shape == torch.Size([8])\n",
        "\n",
        "generation = generate_text(model, ids, 3, p=.5)\n",
        "\n",
        "assert generation.shape == torch.Size([7])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5uhuTT30ewl"
      },
      "source": [
        "### Запускаем генерацию\n",
        "\n",
        "Зададим модели какой-нибудь prompt, например восторженный отзыв о фильме, и посмотрим, как она продолжит его. Так как модель сэмплит на каждом шаге, нам интересно посмотреть на несколько траекторий сразу. Для этого мы и писали батч-режим в генерации."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jntDAgA90ewm"
      },
      "source": [
        "prompt = 'What a lovely film !'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_OBJte00ewm"
      },
      "source": [
        "Подготовим входные данные для GPT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSwU95VV0ewn"
      },
      "source": [
        "ids = tokenizer.encode(prompt, return_tensors=\"pt\").squeeze().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tew84_O80ewo"
      },
      "source": [
        "Запускаем генерацию! Можно запустить на 10-20 токенов, можно и больше"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSHTNksk0ewo"
      },
      "source": [
        "n_tries = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKRdXJYA0ewo"
      },
      "source": [
        "generations = [generate_text(model, ids, length=40, k=10).cpu().detach() for _ in range(n_tries)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq0AoI_50ewp"
      },
      "source": [
        "for generation in generations:\n",
        "    print(tokenizer.decode(generation))\n",
        "    print('\\n' + '=' * 40 + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHM7qBeg0ewr"
      },
      "source": [
        "А теперь запустим генерацию с top-p сэмплированием"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ4uzsOW0ewr"
      },
      "source": [
        "generations = [generate_text(model, ids, length=4, p=0.5).cpu().detach() for _ in range(n_tries)]\n",
        "for generation in generations:\n",
        "    print(tokenizer.decode(generation))\n",
        "    print('\\n' + '=' * 40 + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isQrAwqG0ews"
      },
      "source": [
        "# Часть 2. Few-shot learning\n",
        "\n",
        "Одной из особенностей GPT, про которую авторы написали, является способность к few-shot learning. В этой части домашнего задания мы на примере хорошо знакомой нам задачи классификации текстов исследуем, как хорошо модель умеет различать сентимент отзывов без дополнительного обучения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2osAS3f0ews"
      },
      "source": [
        "## Датасет.\n",
        "Мы будем снова использовать датасет imdb, на котором мы уже обучили несколько моделей. Так как обучать саму модель мы не будем (в few-shot парадигме веса не меняются), то можем сразу взять валидационную часть датасета."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQpgFvhq0ewt"
      },
      "source": [
        "text_dataset = datasets.load_dataset(\"imdb\")\n",
        "texts = text_dataset[\"test\"][\"text\"]\n",
        "labels = text_dataset[\"test\"][\"label\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DPlm7ad0ewu"
      },
      "source": [
        "### Метки для задачи классификации на естественном языке\n",
        "Поскольку языковая модель знает только токены языка и не знает маппинга между индексами и лейблами, нам нужно будет сравнивать лейблы напрямую. С одной стороны, это создаёт дополнительные сложности: модель может предсказать \"Positive Sentiment\" вместо \"positive\", поэтому нам потребуется минимальный постпроцессинг, чтобы учесть большинство таких случаев."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7qRUd-X0ewu"
      },
      "source": [
        "mapping = {idx: label for idx, label in enumerate(text_dataset[\"train\"].features[\"label\"].names)}\n",
        "mapping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmkvUl1W0ewv"
      },
      "source": [
        "### Игрушечный датасет\n",
        "Чтобы быстрее прототипировать наши вводы (prompts), полезно взять маленькую часть датасета и проверять гипотезы на ней. Так как imdb упорядочен по лейблам, а самих лейблов всего 2, мы берём одинаковое количество примеров с начала и с конца."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUrFRggy0ewv"
      },
      "source": [
        "def get_toy_dataset(dataset, split, length = 100):\n",
        "    texts = dataset[split][\"text\"]\n",
        "    labels = dataset[split][\"label\"]\n",
        "    small_texts = texts[:length // 2] + texts[-length//2:]\n",
        "    small_labels = labels[:length // 2] + labels[-length//2:]\n",
        "    return small_texts, small_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9gjyJGQ0eww"
      },
      "source": [
        "assert len(get_toy_dataset(text_dataset, \"test\", 100)[0]) == 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQovzfQu0eww"
      },
      "source": [
        "## Few-shot learning\n",
        "\n",
        "В данном задании мы будем не обучать модель на данных, а использовать уже обученную на огромном корпусе языковую модель, чтобы \"угадывать\" сентимент отзывов.\n",
        "\n",
        "Модель будет видеть в качестве затравки (prompt) описание задачи на естественном языке.\n",
        "\n",
        "Нам потребуется:\n",
        "- описание задачи\n",
        "- лейблы\n",
        "- несколько (хотя бы 3) примеров\n",
        "- вспомогательные маркеры (разделитель примеров и маркер конца входа)\n",
        "\n",
        "Например:\n",
        "\n",
        "```\n",
        "Translate from English to French\n",
        "sea otter => loutre de mer\n",
        "peppermint => menthe poivrée\n",
        "plush girafe  => \n",
        "```\n",
        "\n",
        "Первая строка здесь -- описание задачи. Мы решаем задачу классификации, поэтому нам нужно\n",
        "описать задачу примерно как `To which category does the text belong? positive , negative `\n",
        "\n",
        "Вторая и третья строки -- примеры входа и выхода. В нашем случае примерами входа и выхода будут служить тексты ревью и метки классов positive и negative (а не 0 и 1, как раньше).\n",
        "\n",
        "Наконец, последняя строка -- это тот текст, который модель должна классифицировать. Модель уже \"видела\" выше пример того, что после маркера конца входа идёт сентимент, и мы ожидаем, что она сможет выучить эту закономерность.\n",
        "\n",
        "В данном задании часть входа будет написана за вас, вам надо будет придумать обучающие примеры для few-shot learning'а"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZQgn9uG0ewx"
      },
      "source": [
        "marker = ' => '\n",
        "separator = '\\n'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFxS1Qnp0ewx"
      },
      "source": [
        "labels = [\"positive\" , \"negative\"]\n",
        "labels_text = \" , \".join(labels)\n",
        "task_description = f'To which category does the text belong?: {labels_text} ' + separator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-Z-wR-k0ewy"
      },
      "source": [
        "examples = [\n",
        "    # TASK define examples\n",
        "    # They should be a tuple with text and label\n",
        "    # label should be from \"labels\"\n",
        "    # Get 3-10 different examples, they should cover both positive \n",
        "    # and negative reviews\n",
        "    # YOUR CODE STARTS\n",
        "\n",
        "    # YOUR CODE ENDS\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aso2fG6_0ewz"
      },
      "source": [
        "assert len(examples) > 1, 'Write at least two different examples'\n",
        "for ex in examples:\n",
        "    assert ex[1] in labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Heu6rvlK0ewz"
      },
      "source": [
        "input = task_description  + separator.join([marker.join(example) for example in examples])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buEhQeHo0ew1"
      },
      "source": [
        "print(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIHpblK80ew2"
      },
      "source": [
        "Сравните получившийся инпут с примером выше. Мы получили то, что надо!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xds7hkfQ0ew3"
      },
      "source": [
        "n_chars = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Slih0Fur0ew3"
      },
      "source": [
        "def get_prompt(beginning, separator, text, marker) -> str:\n",
        "    return ' '.join([input,  separator, text[:n_chars] + text[-n_chars:], marker])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmQrc0fs0ew5"
      },
      "source": [
        "## Время проверить работу нашей модели на маленьком датасете!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_0WPdws0ew5"
      },
      "source": [
        "small_texts, small_labels = get_toy_dataset(text_dataset, \"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40bwRicj0ew5"
      },
      "source": [
        "Так как мы предсказываем сентимент текста, каждый из который задаётся одним токеном, то нам\n",
        "- не надо сэмплировать. Мы хотим, чтобы модель честно учитывала вероятности для токенов\n",
        "- достаточно сделать один шаг генерации. Модель должна будет предсказать лейбл текста, а он, как мы выяснили, однотокенный.\n",
        "\n",
        "Таким образом, нам просто надо сделать один шаг жадного предсказания (greedy decoding)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8ax1sUE0ew6"
      },
      "source": [
        "def predict_on_text(model, ids) -> int:\n",
        "    \"\"\"\n",
        "    Run Language Model on text and use logits from last time \n",
        "    step to predict sentence category with greedy decoding\n",
        "    Args:\n",
        "        model: huggingface LM model\n",
        "        ids: LongTensor . Text encoded with tokenizer.\n",
        "    Returns:\n",
        "        model prediction, index of most probable word\n",
        "    \"\"\"\n",
        "    # TASK: run a model and get index most probable logit\n",
        "    # HINT: do not forget to detach tensors\n",
        "    # HINT 2: GPT-2 model returns tuple where logits are stored in the\n",
        "    # 1st item. You do not need 2nd item at all\n",
        "    # Our implementation is 2 lines\n",
        "    # YOUR CODE STARTS\n",
        "\n",
        "    # YOUR CODE ENDS\n",
        "    return idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz6LoYwK0ew7"
      },
      "source": [
        "Проверка кода:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Uo318VV0ew7"
      },
      "source": [
        "ids = tokenizer.encode('hello world !', return_tensors=\"pt\",)\n",
        "prediction = predict_on_text(model, ids.to(device))\n",
        "assert isinstance(prediction, int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69zOpqp90ew7"
      },
      "source": [
        "def predict_on_dataset(model, texts, target, mapping):\n",
        "    \"\"\"\n",
        "    Run Language Model on a dataset and use its predictions as\n",
        "    labels for sentences\n",
        "    Args:\n",
        "        model: huggingface LM model\n",
        "        texts: List[str], texts of dataset\n",
        "        target: List[int], indices of classes\n",
        "        mapping: Dict[int, str] mapping from class indices to class labels\n",
        "    Returns:\n",
        "        model predictions, \"as is\", List[str]\n",
        "        target labels, after mapping, List[str]\n",
        "    \"\"\"\n",
        "    if len(texts) != len(target):\n",
        "        raise RuntimeError('Texts and target lengths mismatch')\n",
        "    # max_length has additional -1 because we will generate exactly 1 token \n",
        "    # with LM\n",
        "    max_length = model.config.n_ctx - 1\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    for idx in tqdm(range(len(texts))):\n",
        "        text, label = texts[idx], target[idx]\n",
        "        ids = tokenizer.encode(\n",
        "            get_prompt(input, separator, text, marker), \n",
        "            add_special_tokens=False, \n",
        "            return_tensors=\"pt\",\n",
        "            max_length=max_length\n",
        "        )\n",
        "        idx = predict_on_text(model, ids.to(device))\n",
        "        predictions.append(tokenizer.decode([idx]))  # generation.cpu().numpy()[0][-1]\n",
        "        labels.append(mapping[label]) \n",
        "    return predictions, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abvFt18q0ew8"
      },
      "source": [
        "predictions, target = predict_on_dataset(model, small_texts, small_labels, mapping)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIECT9O-0ew9"
      },
      "source": [
        "### Замер качества"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13EoUAQ10ew9"
      },
      "source": [
        "Посмотрим, что выдаёт нам языковая модель:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgiZYWnW0ew-"
      },
      "source": [
        "print(predictions[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjzhPs7i0ew_"
      },
      "source": [
        "Можно заметить, что предсказанные метки не совсем похожи на pos и neg, которые были у нас в таргете. Напишем простую функцию, которая нормализует предсказания модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPB04pnd0exA"
      },
      "source": [
        "def normalize_prediction(raw_output: str) -> str:\n",
        "    \"\"\"\n",
        "    Helper that transforms model prediction to normal\n",
        "    For example, ' positive' -> 'pos', ' Negative' -> neg\n",
        "    Args:\n",
        "        raw_output: str, output from language model\n",
        "    Returns:\n",
        "        normalized value: no leading/trailing spaces, lowercase,\n",
        "        at most 3 chars long\n",
        "    \"\"\"\n",
        "    # TASK: write prediction normalizer\n",
        "    # You need to remove spaces, lowercase and get only 3 leading chars\n",
        "    # Our implementation is 1 line\n",
        "    # YOUR CODE STARTS\n",
        "\n",
        "    # YOUR CODE ENDS\n",
        "    return normalized_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euC0k3nFtojd"
      },
      "source": [
        "Напишем функцию, вычисляющую точность:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CozGfPMo0exA"
      },
      "source": [
        "def accuracy(predictions, labels) -> float:\n",
        "    \"\"\"\n",
        "    Helper that transforms model prediction to normal\n",
        "    For example, ' positive' -> 'pos', ' Negative' -> neg\n",
        "    Args:\n",
        "        predictions: List[str], model predictions, should be labels in natural language\n",
        "        labels: List[str], target labels\n",
        "    Returns:\n",
        "        accuracy, float from [0, 1]\n",
        "    \"\"\"\n",
        "    if not labels:\n",
        "        # sanity check to avoid zero division\n",
        "        return 0\n",
        "    if len(predictions) != len(labels):\n",
        "        raise ValueError(f'Predictions and labels have mismatched length')\n",
        "    total = len(predictions)\n",
        "    match = 0\n",
        "    # TASK: calculate accuracy\n",
        "    # For every pair of predicted and real labels, check that they coincide.\n",
        "    # You can use zip() method for iteration over two sequences\n",
        "    # simultaneously.\n",
        "    # Our implementation is 3 lines\n",
        "    # YOUR CODE STARTS\n",
        "\n",
        "    # YOUR CODE ENDS\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_632NyJB0exA"
      },
      "source": [
        "normalized_predictions = [normalize_prediction(label) for label in predictions]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1hyGmSH0exB"
      },
      "source": [
        "accuracy(normalized_predictions, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04SFZ3Wo0exC"
      },
      "source": [
        "### Ура, модель без обучения работает лучше случайного угадывания!\n",
        "\n",
        "Тем не менее, это заметно хуже, чем у полносвязной нейросети. **Учитывая требования по ресурсам и время \n",
        "работы, использование такой модели в реальных задачах непрактично!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srG4Gk3p0exC"
      },
      "source": [
        "## А теперь замер на всем тестовом сете:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoTvsnWP0exD"
      },
      "source": [
        "predictions, target = predict_on_dataset(model, texts, labels, mapping)\n",
        "normalized_predictions = [normalize_prediction(label) for label in predictions]\n",
        "accuracy(normalized_predictions, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NZiUCu2xmu_"
      },
      "source": [
        "Все задание считается выполненным, если вы достигли accuracy 0.6 и выше на тестовом датасете. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xybBRqOz0exE"
      },
      "source": [
        "Данный пример призван продемонстрировать, что большая языковая модель хоть и справляется с задачей без лишних данных, но делает это заметно хуже специально созданных для задачи моделей, к тому же работает непростительно долго."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cFiadD10exE"
      },
      "source": [
        "### Резюме\n",
        "\n",
        "В этом задании мы применили большие языковые модели на практике и решили с её помощью 2 задачи:\n",
        "- conditional генерация текста\n",
        "- few-shot классификация текстов\n",
        "\n",
        "Мы увидели, что модель генерирует связный текст и может решать задачу классификации без изменений своих весов.\n",
        "\n",
        "Несмотря на возможности языковых моделей, их применение на практике всё ещё осложнено большими потребляемыми ресурсами, а также невозможностью интерпретировать предсказания."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUVsBkng0exE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}