{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "gpt_2_text_clf_unresolved.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "04SFZ3Wo0exC"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f76e72fccfab418d8593fff6d4058e2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_63f68546caa14e8d8b4623e0426ba121",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8346ba5775c348eda2f9504995b28107",
              "IPY_MODEL_3fa4358942a04c369a907e00a97a8da0",
              "IPY_MODEL_7b122e4eab644216b583397745ee2930"
            ]
          }
        },
        "63f68546caa14e8d8b4623e0426ba121": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8346ba5775c348eda2f9504995b28107": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8b6973a861f241d1a6f71297d751dd40",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2ff05483d5e2496f8961b58c30b918d6"
          }
        },
        "3fa4358942a04c369a907e00a97a8da0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_01d3c27d045e4421b9d0edc8878d1115",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 718,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 718,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5cd704a2a8634872ab89eb5d01cbd1e3"
          }
        },
        "7b122e4eab644216b583397745ee2930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e476e8c22f274ad28c7e3b162d093ff7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 718/718 [00:00&lt;00:00, 16.6kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1bd6e1c71cb64b67857afbc41ac2c674"
          }
        },
        "8b6973a861f241d1a6f71297d751dd40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2ff05483d5e2496f8961b58c30b918d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "01d3c27d045e4421b9d0edc8878d1115": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5cd704a2a8634872ab89eb5d01cbd1e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e476e8c22f274ad28c7e3b162d093ff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1bd6e1c71cb64b67857afbc41ac2c674": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5bc6bc532e6b416e8d33ff1bde63c138": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d9ea049bbd4e44139a74b398a839a0a9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_42529c4be6b64dc5a68a083e1f6c3b6c",
              "IPY_MODEL_24c08cc5b90747099749c9b01c945aac",
              "IPY_MODEL_59f613229e4b4dcdbfb14e2dc2daee4f"
            ]
          }
        },
        "d9ea049bbd4e44139a74b398a839a0a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "42529c4be6b64dc5a68a083e1f6c3b6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_245064cd92074711876fdd915f48da56",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_120bda7570224ac1a5d1e7ec842106e2"
          }
        },
        "24c08cc5b90747099749c9b01c945aac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e0892ca421bb45d98f0cf0402b0172d3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1042301,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1042301,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_765f187e694a4847980cdd11bdf379cd"
          }
        },
        "59f613229e4b4dcdbfb14e2dc2daee4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7dbb588406064e598399f14ae3c1eef4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 732kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a91bed4f69dd459696f5d95aa853d2ec"
          }
        },
        "245064cd92074711876fdd915f48da56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "120bda7570224ac1a5d1e7ec842106e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e0892ca421bb45d98f0cf0402b0172d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "765f187e694a4847980cdd11bdf379cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7dbb588406064e598399f14ae3c1eef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a91bed4f69dd459696f5d95aa853d2ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b22a56594da041a18604682e61565f1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d6f8790b4cd14170a326db07d4f5f8d0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_66131500fadd49f898c9c2b60f0b11ab",
              "IPY_MODEL_8cf6801011994a1290c84cd6fd8c24c0",
              "IPY_MODEL_e0ecbf095f83414aa880440aacc33a0d"
            ]
          }
        },
        "d6f8790b4cd14170a326db07d4f5f8d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "66131500fadd49f898c9c2b60f0b11ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d15c37a57079415597117e03bc1de61a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6d9ff048a86c4510a049d3492a99232a"
          }
        },
        "8cf6801011994a1290c84cd6fd8c24c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3526952314a84ed4ab38485d9069c8a6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ab4a4f32fd3d4f4e98aee0ee43c4eeb9"
          }
        },
        "e0ecbf095f83414aa880440aacc33a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a5c287247bdd4454ac19cc6e0a5c9017",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 1.04MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_49ed5ea6cd70458ea75e2418f09e5df3"
          }
        },
        "d15c37a57079415597117e03bc1de61a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6d9ff048a86c4510a049d3492a99232a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3526952314a84ed4ab38485d9069c8a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ab4a4f32fd3d4f4e98aee0ee43c4eeb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a5c287247bdd4454ac19cc6e0a5c9017": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "49ed5ea6cd70458ea75e2418f09e5df3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "351c2b8e34e74bbcaefa0968af72d59a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5ee9dd2414af469daf3f6170aac55c73",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dbc739fb20cc4f4585a4eb564d73e3db",
              "IPY_MODEL_a614925853a64ddf84d85932de74dc59",
              "IPY_MODEL_97cdd5881e83418eb5df002be6629b72"
            ]
          }
        },
        "5ee9dd2414af469daf3f6170aac55c73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dbc739fb20cc4f4585a4eb564d73e3db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_34d2c89aac1a4ce5b27437fbce43aa3b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ac41c14ad1d94f838c71572dc90f1e0b"
          }
        },
        "a614925853a64ddf84d85932de74dc59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5394d2de59334e68bd4f3d0d0a03dc15",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_80cc440d1c3c41c7b198535bfab9fd7f"
          }
        },
        "97cdd5881e83418eb5df002be6629b72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cee0c9f35e574dfc9890d7510b8e5cf1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3/3 [00:00&lt;00:00, 51.10it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_edb0cf897b6246049b85684bb61c71c4"
          }
        },
        "34d2c89aac1a4ce5b27437fbce43aa3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ac41c14ad1d94f838c71572dc90f1e0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5394d2de59334e68bd4f3d0d0a03dc15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "80cc440d1c3c41c7b198535bfab9fd7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cee0c9f35e574dfc9890d7510b8e5cf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "edb0cf897b6246049b85684bb61c71c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/delhian/NLP_course/blob/master/Home%20Tasks/gpt_2_text_clf_unresolved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd5wsNhS0ewB"
      },
      "source": [
        "## Возможности и ограничения языковых моделей"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9YkOa3Jwbl-",
        "outputId": "931a3ecb-61d4-438e-e62b-33c6ba50a982"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dSYyxBI0ewG"
      },
      "source": [
        "**Тут есть важное действие, которое нужно сделать перед тем, как запускать этот ноутбук**\n",
        "\n",
        "Если вы делаете это задание в Google Colab, первым делом переключите Runtime в GPU. Это задание нормально посчитается и на CPU, но некоторые из будущих кейсов потребуют GPU (либо вам придётся несколько часов или даже дней, чтобы модель обучилась - и мы не преувиличиваем). Ещё мы рекомендуем переключить язык интерфейса на английский, потому что русская локализация ужасна да и вообще не нужна.\n",
        "\n",
        "О том, как переключить рантайм: https://www.geeksforgeeks.org/how-to-use-google-colab/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqRGVCt50ewL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae145caf-6e7e-4ef9-be6e-945a05faf2ff"
      },
      "source": [
        "!pip install datasets transformers[torch]==2.10.0 tqdm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.13.3-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting transformers[torch]==2.10.0\n",
            "  Downloading transformers-2.10.0-py3-none-any.whl (660 kB)\n",
            "\u001b[K     |████████████████████████████████| 660 kB 41.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (3.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 38.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (1.19.5)\n",
            "Collecting tokenizers==0.7.0\n",
            "  Downloading tokenizers-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 13.8 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 45.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from transformers[torch]==2.10.0) (1.9.0+cu111)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 58.2 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 48.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Collecting huggingface-hub<0.1.0,>=0.0.19\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 35.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.13)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[torch]==2.10.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[torch]==2.10.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[torch]==2.10.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[torch]==2.10.0) (1.24.3)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 52.9 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 54.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[torch]==2.10.0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[torch]==2.10.0) (7.1.2)\n",
            "Installing collected packages: multidict, yarl, async-timeout, tokenizers, sentencepiece, sacremoses, fsspec, aiohttp, xxhash, transformers, huggingface-hub, datasets\n",
            "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.13.3 fsspec-2021.10.1 huggingface-hub-0.0.19 multidict-5.2.0 sacremoses-0.0.46 sentencepiece-0.1.96 tokenizers-0.7.0 transformers-2.10.0 xxhash-2.0.2 yarl-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcr7RqWl0ewN"
      },
      "source": [
        "from transformers import AutoModel, AutoTokenizer, GPT2LMHeadModel\n",
        "import datasets\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "import warnings"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJvskQ8T0ewO"
      },
      "source": [
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diaudnwY0ewQ"
      },
      "source": [
        "# import transformers\n",
        "# print(transformers.__version__)  # should be above or equal 4.0.1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVYGHVGT0ewS"
      },
      "source": [
        "В этом модуле мы с вами познакомились с большими языковыми моделями и обсудили их возможности, а также ограничения. В этом задании вам предлагается поэкспериментировать с одной из больших языковых моделей -- GPT-2 -- и самим убедиться, так ли она хороша, как о ней рассказывают."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXk55qyo0ewT"
      },
      "source": [
        "## Библиотека transformers\n",
        "\n",
        "Вы уже сталкивались с библиотекой transformers в этом курсе.\n",
        "\n",
        "Мы [загрузим gpt-2](https://huggingface.co/transformers/model_doc/gpt2.html) с помощью метода from_pretrained. Так как мы будем использовать её для задачи языкового моделирования, нам потребуется GPT2LMHeadModel, прочитать про неё можно [тут](https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel). У GPT-2 свой токенизатор, про него можно почитать [тут](https://huggingface.co/transformers/model_doc/gpt2.html#gpt2tokenizer).\n",
        "\n",
        "GPT-2 -- большая модель, время ее работы на CPU будет слишком большим, так что мы сразу отправим её на GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq9fHovY0ewU"
      },
      "source": [
        "model_name = 'gpt2-medium'\n",
        "device = 'cuda'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVJ-USzH0ewW",
        "outputId": "e07fdc0b-e1db-49dd-e763-1e3db412d945",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "f76e72fccfab418d8593fff6d4058e2b",
            "63f68546caa14e8d8b4623e0426ba121",
            "8346ba5775c348eda2f9504995b28107",
            "3fa4358942a04c369a907e00a97a8da0",
            "7b122e4eab644216b583397745ee2930",
            "8b6973a861f241d1a6f71297d751dd40",
            "2ff05483d5e2496f8961b58c30b918d6",
            "01d3c27d045e4421b9d0edc8878d1115",
            "5cd704a2a8634872ab89eb5d01cbd1e3",
            "e476e8c22f274ad28c7e3b162d093ff7",
            "1bd6e1c71cb64b67857afbc41ac2c674",
            "5bc6bc532e6b416e8d33ff1bde63c138",
            "d9ea049bbd4e44139a74b398a839a0a9",
            "42529c4be6b64dc5a68a083e1f6c3b6c",
            "24c08cc5b90747099749c9b01c945aac",
            "59f613229e4b4dcdbfb14e2dc2daee4f",
            "245064cd92074711876fdd915f48da56",
            "120bda7570224ac1a5d1e7ec842106e2",
            "e0892ca421bb45d98f0cf0402b0172d3",
            "765f187e694a4847980cdd11bdf379cd",
            "7dbb588406064e598399f14ae3c1eef4",
            "a91bed4f69dd459696f5d95aa853d2ec",
            "b22a56594da041a18604682e61565f1a",
            "d6f8790b4cd14170a326db07d4f5f8d0",
            "66131500fadd49f898c9c2b60f0b11ab",
            "8cf6801011994a1290c84cd6fd8c24c0",
            "e0ecbf095f83414aa880440aacc33a0d",
            "d15c37a57079415597117e03bc1de61a",
            "6d9ff048a86c4510a049d3492a99232a",
            "3526952314a84ed4ab38485d9069c8a6",
            "ab4a4f32fd3d4f4e98aee0ee43c4eeb9",
            "a5c287247bdd4454ac19cc6e0a5c9017",
            "49ed5ea6cd70458ea75e2418f09e5df3"
          ]
        }
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f76e72fccfab418d8593fff6d4058e2b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/718 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5bc6bc532e6b416e8d33ff1bde63c138",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b22a56594da041a18604682e61565f1a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTXvTSFN0ewZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "556b8666-2446-40e2-eeab-b74e577a6f22"
      },
      "source": [
        "%%time\n",
        "# model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/gpt2-medium')\n",
        "# model = model.to(device)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 9.32 s, sys: 1.98 s, total: 11.3 s\n",
            "Wall time: 24.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJispE3M0ewa"
      },
      "source": [
        "### Подсчёт числа параметров модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asygaKxt0ewb"
      },
      "source": [
        "Чтобы понять, насколько GPT большая модель, подсчитаем число ее параметров"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrKJmIHo0ewc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad78c025-69c2-4747-b5a0-ea77cc709483"
      },
      "source": [
        "# TASK: compute number of model parameters\n",
        "# iterate over model weights and count number of parameters in each of them, then sum it up\n",
        "# note: you may find model.parameters() method useful\n",
        "# Our implementation is 2 lines\n",
        "\n",
        "params = sum([params.numel() for params in model.parameters() ])\n",
        "# YOUR CODE STARTS\n",
        "\n",
        "# YOUR CODE ENDS\n",
        "params"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "354823168"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHOJXyah0ewd"
      },
      "source": [
        "Как видим, это большое число параметров, на несколько порядок больше, чем те модели, которые вы обучали до этого! Использовать такое в продакшене как правило невозможно из-за больших требований по ресурсам"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1-PmKrq0ewd"
      },
      "source": [
        "# Часть 1. Генерация отзывов моделью GPT\n",
        "\n",
        "В преыдущем задании мы генерировали ровно 1 токен, так как нам нужно было предсказать сентимент, а оба слова \"positive\" и \"negative\" есть в словаре GPT.\n",
        "\n",
        "В этом задании мы пойдём дальше и самостоятельно реализуем top-k сэмплирование, чтобы генерировать отзывы на фильмы, подобные тем, что в датасете IMDB.\n",
        "\n",
        "Мы будем продолжать отзывы, используя в качестве \"затравки\" для модели начало реальных ревью."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUnAhWXm0ewd"
      },
      "source": [
        "### Top-k & Top-p sampling\n",
        "\n",
        "Чтобы генерация текста была более разнообразной, мы будем использовать top k сэмплирование, которое мы изучали в этом юните. Его смысл в том, что на каждом шаге мы перед сэмплированием зануляем вероятности всех слов, кроме k самых вероятных."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T4TM0EL0ewf"
      },
      "source": [
        "def topk_sample(scores, k):\n",
        "    \"\"\"\n",
        "    Sample from logits using multinomial distribution\n",
        "    Before sampling, all logits except k greatest are zeroed out.\n",
        "    Args:\n",
        "        scores: scores for every word in the vocabulary. Must be 1-dimensional: (num_words_in_vocab)\n",
        "        k: int, how many hypotheses to sample from\n",
        "    Returns:\n",
        "        1 draw from dictribution, torch.LongTensor of shape (1)\n",
        "    \"\"\"\n",
        "    assert k >= 1, 'k must be >=1'\n",
        "    assert scores.ndim == 1, 'logits must have 1 dimension' \n",
        "\n",
        "    # TASK: implement top-k sampling\n",
        "    # First, get top k values and theoir indices using torch.topk\n",
        "    # 2nd, fill logits with some small value (e. g. 1e-6)\n",
        "    # Next, move values back to their place. Note that you will find ellipsis (...) and None useful\n",
        "    # Finally, use softmax to obtain probabilities and get \n",
        "    # Our implementation is 6 lines\n",
        "    \n",
        "    # YOUR CODE STARTS\n",
        "    values, indexes = scores.topk(k)\n",
        "    scores = scores.fill_(1e-6)\n",
        "    scores[indexes] = values\n",
        "    predicted_ids = F.softmax(scores)\n",
        "    # YOUR CODE ENDS\n",
        "    return predicted_ids"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvm41e190ewf"
      },
      "source": [
        "Проверка кода:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFpPyxFt0ewg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8728474-56a7-45a8-cd38-7b6ad017de1d"
      },
      "source": [
        "logits = torch.randn(5)\n",
        "for _ in range(10):\n",
        "    res1 = topk_sample(logits, 1)\n",
        "    res2  = topk_sample(logits, 1)\n",
        "    assert res1.ndim == 1\n",
        "    assert res1.equal(res2)\n",
        "equal = True\n",
        "for _ in range(10):\n",
        "    if not topk_sample(logits, 2).equal(topk_sample(logits, 2)):\n",
        "        equal = False\n",
        "assert equal\n",
        "print('OK!')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2VBo0eotL6Z"
      },
      "source": [
        "Также мы будем использовать top-p sampling, который мы уже проходили на лекциях."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ8divSy0ewh"
      },
      "source": [
        "def topp_sample(scores, p):\n",
        "    \"\"\"\n",
        "    Sample from logits using multinomial distribution\n",
        "    Before sampling, all logits except those largest\n",
        "    whose cumsum exceeds p are zeroed out\n",
        "    Args:\n",
        "        scores: scores for every word in the vocabulary. Must be 1-dimensional: (num_words_in_vocab)\n",
        "        p: float, cumulative probability of most high-scored tokens\n",
        "    Returns:\n",
        "        1 draw from dictribution, torch.LongTensor of shape (1)\n",
        "    \"\"\"\n",
        "    assert  0 < p < 1, 'p must be between 0 and 1'\n",
        "    assert scores.ndim == 1, 'logits must have 1 dimension' \n",
        "\n",
        "    # TASK: implement top-p sampling\n",
        "    # 1. sort scores with descending order\n",
        "\n",
        "    values, indexes = scores.sort(descending = True)\n",
        "    # 2. get cumulative sum of softmaxed logits\n",
        "\n",
        "    values_cumsum = torch.cumsum(values, dim =0 )\n",
        "\n",
        "    # 3. get indices when cumulative sum is larger than p. These indices should be zeroed out\n",
        "\n",
        "    scores[indexes[values_cumsum > p]] = 1e-6\n",
        "\n",
        "    # 4. get real indices that should be zeroes out from sorted indices. Use tensor slicing\n",
        "    # 5. fill these indices with some small value (e. g. -1e6)\n",
        "    # 6. sample, as you did in top-k sampling\n",
        "    # Our implementation is 7 lines\n",
        "    \n",
        "    # YOUR CODE STARTS\n",
        "    predicted_ids = F.softmax(scores)\n",
        "    # YOUR CODE ENDS\n",
        "    return predicted_ids"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-0t8dHgtN3N"
      },
      "source": [
        "Проверка кода:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrDbJBlc0ewi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecb1fc67-875e-40bb-ad17-bf3bb05ba746"
      },
      "source": [
        "logits = torch.Tensor([-.1, -.2, .9])\n",
        "for _ in range(10):\n",
        "    res1 = topp_sample(logits, 0.8)\n",
        "    res2  = topp_sample(logits, 0.8)\n",
        "    assert res1.ndim == 1\n",
        "\n",
        "    assert res1.equal(res2)\n",
        "logits = torch.Tensor([.5, .5, .5])\n",
        "equal = True\n",
        "for _ in range(10):\n",
        "    if not topp_sample(logits, 0.8).equal(topp_sample(logits, 0.8)):\n",
        "        equal = False\n",
        "assert equal\n",
        "print('OK!')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgqFirKjtPrz"
      },
      "source": [
        "Теперь напишите функцию генерации текста:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sNwScYY0ewj"
      },
      "source": [
        "def generate_text(model, ids, length, k=None, p=None):\n",
        "    \"\"\"\n",
        "    Generate text with language model with ids as prompt\n",
        "    Args:\n",
        "        model: huggingface LM model\n",
        "        ids: input ids, from tokenizer.encode, must be 2-dimensional\n",
        "        length: int, how many tokens to geenrate\n",
        "        k: int, how many hypotheses to sample from in top-k sampling\n",
        "    Returns:\n",
        "        token ids from generated text, together with token ids of prompts, 2d LongTensor\n",
        "    \"\"\"\n",
        "    assert length >= 1\n",
        "    if k and p:\n",
        "        raise RuntimeError('Cannot use topk and topp sampling simultaneously')\n",
        "    # TASK: write generation loop\n",
        "    # For every timestamp:\n",
        "    # - obtain model output\n",
        "    # - get logits for last symbol\n",
        "    # - apply topk sampling to get new index\n",
        "    # - concatenate it to the ids to form new input\n",
        "    # in the end, the `ids` tensor will have full generation\n",
        "    # our implementation is 5 lines\n",
        "    \n",
        "    # YOUR CODE STARTS\n",
        "\n",
        "    ids = model.generate(\n",
        "        ids.reshape(-1, ids.shape[0]), \n",
        "        do_sample=True, \n",
        "        max_length=ids.shape[0] + length, \n",
        "        repetition_penalty=1.2, \n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        top_k = k,\n",
        "        top_p = p\n",
        "    )\n",
        "\n",
        "    # YOUR CODE ENDS\n",
        "    return ids.reshape(ids.shape[1]).cpu().detach()"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB6g8UWp0ewj"
      },
      "source": [
        "Проверка кода:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8mpDF_O0ewk"
      },
      "source": [
        "ids = torch.randint(0, 1000, (4,))\n",
        "generation = generate_text(model, ids, 4, k=2)\n",
        "\n",
        "assert generation.shape == torch.Size([8])\n",
        "\n",
        "generation = generate_text(model, ids, 3, p=.5)\n",
        "\n",
        "assert generation.shape == torch.Size([7])"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5uhuTT30ewl"
      },
      "source": [
        "### Запускаем генерацию\n",
        "\n",
        "Зададим модели какой-нибудь prompt, например восторженный отзыв о фильме, и посмотрим, как она продолжит его. Так как модель сэмплит на каждом шаге, нам интересно посмотреть на несколько траекторий сразу. Для этого мы и писали батч-режим в генерации."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jntDAgA90ewm"
      },
      "source": [
        "prompt = 'What a lovely film !'"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_OBJte00ewm"
      },
      "source": [
        "Подготовим входные данные для GPT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSwU95VV0ewn"
      },
      "source": [
        "ids = tokenizer.encode(prompt, return_tensors=\"pt\").squeeze()"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tew84_O80ewo"
      },
      "source": [
        "Запускаем генерацию! Можно запустить на 10-20 токенов, можно и больше"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSHTNksk0ewo"
      },
      "source": [
        "n_tries = 5"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKRdXJYA0ewo"
      },
      "source": [
        "generations = [generate_text(model, ids, length=40, k=10).cpu().detach() for _ in range(n_tries)]"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq0AoI_50ewp",
        "outputId": "75fa8eb0-7c1a-47a9-d087-f35ac6b87a66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for generation in generations:\n",
        "    print(tokenizer.decode(generation))\n",
        "    print('\\n' + '=' * 40 + '\\n')"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What a lovely film! I loved the story line and characters. You have all kinds of wonderful things going on, like an American woman who is being held hostage with her children by two men in black suits while she's eating\n",
            "\n",
            "========================================\n",
            "\n",
            "What a lovely film! I love it. And now i can say that the only reason to watch this movie is if you are in London! :)\n",
            "\n",
            "…<|endoftext|>\n",
            "\n",
            "========================================\n",
            "\n",
            "What a lovely film! This is my favourite of his films, so I am looking forward to the sequel!\n",
            "\n",
            "I'm really glad that you are doing this for me. My husband and kids have never been able get\n",
            "\n",
            "========================================\n",
            "\n",
            "What a lovely film!\n",
            " The soundtrack to the movie is very good, but it does not come across as catchy at first. However this time there are plenty more songs that can be played during any scene with just one\n",
            "\n",
            "========================================\n",
            "\n",
            "What a lovely film! I can't wait to see how the rest of his career pans out.\"\n",
            "\n",
            "- The New Yorker, December 18th, 2006\n",
            " \"The best thing about this movie is that it's so\n",
            "\n",
            "========================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHM7qBeg0ewr"
      },
      "source": [
        "А теперь запустим генерацию с top-p сэмплированием"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ4uzsOW0ewr",
        "outputId": "d8bd5a29-b51c-4f59-9b30-213a9a2f8426",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "generations = [generate_text(model, ids, length=40, p=0.5).cpu().detach() for _ in range(n_tries)]\n",
        "for generation in generations:\n",
        "    print(tokenizer.decode(generation))\n",
        "    print('\\n' + '=' * 40 + '\\n')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What a lovely film! I love it. It's not only the story of what happened but also how this beautiful woman was treated and why she died.\n",
            "\n",
            "This is my favorite movie ever! The ending (with an\n",
            "\n",
            "========================================\n",
            "\n",
            "What a lovely film!\n",
            "\n",
            "\n",
            "This is one of the best films I have seen in years. It's funny, touching and has some very interesting moments which make it all worth watching! This was my first time seeing this\n",
            "\n",
            "========================================\n",
            "\n",
            "What a lovely film! I love the way it shows how much you care for someone who has just been taken away from you. It is such an amazing story and so touching, that if there was ever another movie like this\n",
            "\n",
            "========================================\n",
            "\n",
            "What a lovely film! The only thing I can say is that it doesn't do justice to the fact, at least in my opinion. There are some very good scenes and there's not too much of them ;-) But\n",
            "\n",
            "========================================\n",
            "\n",
            "What a lovely film! It is very funny and charming.\n",
            "\n",
            "I love it, I'm going to buy the next one in this series.\n",
            " \"<|endoftext|>\n",
            "\n",
            "========================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isQrAwqG0ews"
      },
      "source": [
        "# Часть 2. Few-shot learning\n",
        "\n",
        "Одной из особенностей GPT, про которую авторы написали, является способность к few-shot learning. В этой части домашнего задания мы на примере хорошо знакомой нам задачи классификации текстов исследуем, как хорошо модель умеет различать сентимент отзывов без дополнительного обучения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2osAS3f0ews"
      },
      "source": [
        "## Датасет.\n",
        "Мы будем снова использовать датасет imdb, на котором мы уже обучили несколько моделей. Так как обучать саму модель мы не будем (в few-shot парадигме веса не меняются), то можем сразу взять валидационную часть датасета."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQpgFvhq0ewt",
        "outputId": "cf5e0c21-defb-4d0a-e105-800f9ef4c442",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "351c2b8e34e74bbcaefa0968af72d59a",
            "5ee9dd2414af469daf3f6170aac55c73",
            "dbc739fb20cc4f4585a4eb564d73e3db",
            "a614925853a64ddf84d85932de74dc59",
            "97cdd5881e83418eb5df002be6629b72",
            "34d2c89aac1a4ce5b27437fbce43aa3b",
            "ac41c14ad1d94f838c71572dc90f1e0b",
            "5394d2de59334e68bd4f3d0d0a03dc15",
            "80cc440d1c3c41c7b198535bfab9fd7f",
            "cee0c9f35e574dfc9890d7510b8e5cf1",
            "edb0cf897b6246049b85684bb61c71c4"
          ]
        }
      },
      "source": [
        "text_dataset = datasets.load_dataset(\"imdb\")\n",
        "texts = text_dataset[\"test\"][\"text\"]\n",
        "labels = text_dataset[\"test\"][\"label\"]"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "351c2b8e34e74bbcaefa0968af72d59a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DPlm7ad0ewu"
      },
      "source": [
        "### Метки для задачи классификации на естественном языке\n",
        "Поскольку языковая модель знает только токены языка и не знает маппинга между индексами и лейблами, нам нужно будет сравнивать лейблы напрямую. С одной стороны, это создаёт дополнительные сложности: модель может предсказать \"Positive Sentiment\" вместо \"positive\", поэтому нам потребуется минимальный постпроцессинг, чтобы учесть большинство таких случаев."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7qRUd-X0ewu",
        "outputId": "878c86f1-9bab-44fb-f4c4-a1df2f1c4200",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mapping = {idx: label for idx, label in enumerate(text_dataset[\"train\"].features[\"label\"].names)}\n",
        "mapping"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'neg', 1: 'pos'}"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmkvUl1W0ewv"
      },
      "source": [
        "### Игрушечный датасет\n",
        "Чтобы быстрее прототипировать наши вводы (prompts), полезно взять маленькую часть датасета и проверять гипотезы на ней. Так как imdb упорядочен по лейблам, а самих лейблов всего 2, мы берём одинаковое количество примеров с начала и с конца."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUrFRggy0ewv"
      },
      "source": [
        "def get_toy_dataset(dataset, split, length = 100):\n",
        "    texts = dataset[split][\"text\"]\n",
        "    labels = dataset[split][\"label\"]\n",
        "    small_texts = texts[:length // 2] + texts[-length//2:]\n",
        "    small_labels = labels[:length // 2] + labels[-length//2:]\n",
        "    return small_texts, small_labels"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9gjyJGQ0eww"
      },
      "source": [
        "assert len(get_toy_dataset(text_dataset, \"test\", 100)[0]) == 100"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQovzfQu0eww"
      },
      "source": [
        "## Few-shot learning\n",
        "\n",
        "В данном задании мы будем не обучать модель на данных, а использовать уже обученную на огромном корпусе языковую модель, чтобы \"угадывать\" сентимент отзывов.\n",
        "\n",
        "Модель будет видеть в качестве затравки (prompt) описание задачи на естественном языке.\n",
        "\n",
        "Нам потребуется:\n",
        "- описание задачи\n",
        "- лейблы\n",
        "- несколько (хотя бы 3) примеров\n",
        "- вспомогательные маркеры (разделитель примеров и маркер конца входа)\n",
        "\n",
        "Например:\n",
        "\n",
        "```\n",
        "Translate from English to French\n",
        "sea otter => loutre de mer\n",
        "peppermint => menthe poivrée\n",
        "plush girafe  => \n",
        "```\n",
        "\n",
        "Первая строка здесь -- описание задачи. Мы решаем задачу классификации, поэтому нам нужно\n",
        "описать задачу примерно как `To which category does the text belong? positive , negative `\n",
        "\n",
        "Вторая и третья строки -- примеры входа и выхода. В нашем случае примерами входа и выхода будут служить тексты ревью и метки классов positive и negative (а не 0 и 1, как раньше).\n",
        "\n",
        "Наконец, последняя строка -- это тот текст, который модель должна классифицировать. Модель уже \"видела\" выше пример того, что после маркера конца входа идёт сентимент, и мы ожидаем, что она сможет выучить эту закономерность.\n",
        "\n",
        "В данном задании часть входа будет написана за вас, вам надо будет придумать обучающие примеры для few-shot learning'а"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZQgn9uG0ewx"
      },
      "source": [
        "marker = ' => '\n",
        "separator = '\\n'"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFxS1Qnp0ewx"
      },
      "source": [
        "labels = [\"positive\" , \"negative\"]\n",
        "labels_text = \" , \".join(labels)\n",
        "task_description = f'To which category does the text belong?: {labels_text} ' + separator"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu1Uwvlm-dEZ"
      },
      "source": [
        "lbl = text_dataset['train']['label']\n",
        "texts =  text_dataset['train']['text']"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3XDvioT_Lxe"
      },
      "source": [
        "import random"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj9_4rok92rI"
      },
      "source": [
        "noex = 3\n",
        "random_negative = random.sample(texts[12500: 24999], noex)\n",
        "random_positive = random.sample(texts[0: 12499], noex)"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-Z-wR-k0ewy"
      },
      "source": [
        "examples = [\n",
        "    # TASK define examples\n",
        "    # They should be a tuple with text and label\n",
        "    # label should be from \"labels\"\n",
        "    # Get 3-10 different examples, they should cover both positive \n",
        "    # and negative reviews\n",
        "    # YOUR CODE STARTS\n",
        "(x, \"negative\") for x in random_negative] + [(x, \"positive\") for x in random_positive\n",
        "    # YOUR CODE ENDS\n",
        "]"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aso2fG6_0ewz"
      },
      "source": [
        "assert len(examples) > 1, 'Write at least two different examples'\n",
        "for ex in examples:\n",
        "    assert ex[1] in labels"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Heu6rvlK0ewz"
      },
      "source": [
        "input = task_description  + separator.join([marker.join(example) for example in examples])"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buEhQeHo0ew1",
        "outputId": "875ad9a4-87f8-4351-8fd0-8eadb60c9dd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(input)"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To which category does the text belong?: positive , negative \n",
            "I decided to watch this serial after seeing the endless adverts for it on the BBC in the weeks prior to it starting. I watched it despite the fact that I don't like the pretentious kind of stuff that Alan Hollinghurst writes (sorry to his fans but I think we have a case of the emperor's new clothes with this author's work). I admit that the acting is excellent, it is beautifully shot and I was reasonably entertained by it - however- I found that the storyline was extremely thin and after watching all three episodes feel very unsatisfied with this rather empty production. The 'explicit' gay sex that the media droned on about has all been done before on TV - several times - so it was nothing very shocking I'm afraid. Full marks for production values but low ones for storyline/content I'm afraid. => negative\n",
            "Waiting to go inside the theathre with tickets in my hand, I expected an interesting sci-fi fantasy movie which could finally feed my appetite of movies regarding robot-technology, instead I went disappointed by each aspect of it, once more proving that stunning special effects can't help a boring plot, which by my opinion was the worse in this year. Acting in this movie also dissatisfied me, Will Smith didn't show anything new in this movie, yet I never saw his acting to change since \"Men In Black\" which was his only success by my opinion. He had to retire since than, not spoiling his name with titles like \"I,Robot\" and \"Men In Black 2\". 4/10 => negative\n",
            "The Perfectly Stupid Weapon. I think the guys dancing at the beginning of one of Steven Segal's movies was intented to mock Jeff doing his forms to dance music at the beginning of this stupid movie. The plot is predictable, the fights were fair and Jeff acts about as well as the sofa he beats with some sort of weapon in one scene. => negative\n",
            "David Beckham is a British soccer star and the husband of Victoria Beckham (\"Posh Spice\" of the Spice Girls). His trademark is a goal shot that curves across the pitch and into the net. The soccer equivalent of an unhittable curve ball in baseball. \"Bend it like Beckham\" means making that type of spectacular shot. Apart from that, and a little shrine to him in the main character's bedroom and a faux-cameo at the very end, the movie has nothing to do with him.<br /><br />The movie is full of little soccer in-jokes, such as the present that one of the characters' parents give her of a jersey with the number 9 on it (property of the great Mia Hamm, to those in the know), references to \"Posh 'n' Becks,\" the video homage to the WUSA one of the characters plays for a disbelieving friend (\"They *have* that??\"), lesbian gags, sports-bra gags, and so on.<br /><br />The story is about a teenage girl in England who idolizes Beckham and wants to be a soccer star. She has a real gift, but the two seemingly insurmountable obstacles she must overcome are the absence of a professional women's league in the UK (hence their fascination with our WUSA), and her parents, who are Indian immigrants set in very old-fashioned ways that do not allow daughters, among other things, to engage in contact sports. The girl's family are portrayed as figures of ironic fun, but with great affection -- think My Big Fat Greek Wedding. The girl loves and respects them enough to go through sitcom hell to conceal her growing soccer stardom from them. => positive\n",
            "So many wonderful actresses in one film serve as a practical invitation to the local movie house so I duly responded. Here are some remarks..<br /><br />Vanessa Redgrave is great even while lying in bed. She also looks very old and I don't think this is achieved with much make-up which is a good thing for the film but a sad thing for us cinema-goers. I think her aging got a bit harsh in recent years. Claire Danes continues her welcome return to the movies and exudes a definite warmth. Mamie Gummer's resemblance to her mother Merly Streep both in terms of physical appearance and acting style is so striking that I lost my concentration to the film for a couple of minutes after her entrance. She is surprisingly good; however such a resemblance has the danger of working against her favor. I agree with a previous comment: Natasha Richardson definitely had some plastic job done to her face. She certainly does not look like how I remember her from previous films (\"Nell\" for example.) Both she and Toni Collette sadly do not make much impression partly because they do not look convincing as sisters. Their interplay is weak. Toni Collette additionally is way too old for her character. Glenn Close and Meryl Streep had to have more screen time. Streep's performance actually is little more than a cameo. Her scenes on the other hand have bigger emotional resonance than the rest of the film. Eileen Atkins provides some welcome dry wit, especially in her second role as an imaginary nighttime companion to Redgrave's character. As for the men; Hugh Dancy enlivenes the film considerably even though he gives a broader performance than needed. As a matter of fact as soon as he exits the story it starts to drag. It is also to his credit that he manages to create the exact necessary sense of boyish charm in the viewer. Patrick Wilson on the other hand is a complete void at the center of the film. He also has the misfortune that the script is insufficient in explaining why three people (one of them a man) are so much smitten by this man. The backstory to this should have been developed more.<br /><br />The cinematography is excellent as expected. However the main summer house set failed to convince me. It does not look natural on the top of that rocky hill, particularly with its grass patch in the front. A bit too cardboard like.<br /><br />Overall, the film is a classy production, but a seen-it-all-before, cried-at-it-all-before feeling took over me during most of its duration and consequently it failed to make the kind of impact on me that I expected from a tearjerker. However, it still managed to make me thoughtful about the passing of time, about one's expectations from life and the extent to which these are fulfilled or not. Worth trying at least on DVD if not at the movies... => positive\n",
            "This highly underrated film is (to me) what good writing in a movie should be all about. Kasdan takes the search for meaning in our lives and lays it out for all to see and wonder at. The movie is about the divides people create to insulate themselves from the violence and hatred and bigotry of everyday life. <br /><br /> Along the way we are asked question after question about life. Davis (Steve Martin with a great beard) asks himself 'Is my making a violent movie (and by extension our enjoyment of it) causing the violence in society?' Claire asks \"What kind of world throws away something as precious as a human life?' Mack is not immune as he asks 'Is it possible to pass beyond the bounds of race and (an even harder step) finance? These are of course not quoted from the film, but generalities. Others ask their questions too, and to be honest it raises more than it answers.<br /><br /> But that is the nature of life. We strive all our lives to find answers to questions we will never totally answer, and in certain cases have to make answers fit to our own needs and desires. As humans we thrive on questions we cannot answer. Some answers are real. Claire and Mack come to realize that even though they could take the easy road and let the state take the baby, their finding it placed the responsibility for her life in their hands. Some answers are not. Davis `Sees the Light' and decides not to make violent films, but the next day turns around and dismisses his epiphany as subordinate to his art.<br /><br /> We all seek answers. This movie does not answer them for; it simply reminds you to keep looking for the answers.<br /><br /> => positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIHpblK80ew2"
      },
      "source": [
        "Сравните получившийся инпут с примером выше. Мы получили то, что надо!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xds7hkfQ0ew3"
      },
      "source": [
        "n_chars = 50"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Slih0Fur0ew3"
      },
      "source": [
        "def get_prompt(beginning, separator, text, marker) -> str:\n",
        "    return ' '.join([input,  separator, text[:n_chars] + text[-n_chars:], marker])"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmQrc0fs0ew5"
      },
      "source": [
        "## Время проверить работу нашей модели на маленьком датасете!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_0WPdws0ew5"
      },
      "source": [
        "small_texts, small_labels = get_toy_dataset(text_dataset, \"test\")"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40bwRicj0ew5"
      },
      "source": [
        "Так как мы предсказываем сентимент текста, каждый из который задаётся одним токеном, то нам\n",
        "- не надо сэмплировать. Мы хотим, чтобы модель честно учитывала вероятности для токенов\n",
        "- достаточно сделать один шаг генерации. Модель должна будет предсказать лейбл текста, а он, как мы выяснили, однотокенный.\n",
        "\n",
        "Таким образом, нам просто надо сделать один шаг жадного предсказания (greedy decoding)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8ax1sUE0ew6"
      },
      "source": [
        "def predict_on_text(model, ids) -> int:\n",
        "    \"\"\"\n",
        "    Run Language Model on text and use logits from last time \n",
        "    step to predict sentence category with greedy decoding\n",
        "    Args:\n",
        "        model: huggingface LM model\n",
        "        ids: LongTensor . Text encoded with tokenizer.\n",
        "    Returns:\n",
        "        model prediction, index of most probable word\n",
        "    \"\"\"\n",
        "    # TASK: run a model and get index most probable logit\n",
        "    # HINT: do not forget to detach tensors\n",
        "    # HINT 2: GPT-2 model returns tuple where logits are stored in the\n",
        "    # 1st item. You do not need 2nd item at all\n",
        "    # Our implementation is 2 lines\n",
        "    # YOUR CODE STARTS\n",
        "\n",
        "    # YOUR CODE ENDS\n",
        "    return idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz6LoYwK0ew7"
      },
      "source": [
        "Проверка кода:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Uo318VV0ew7"
      },
      "source": [
        "ids = tokenizer.encode('hello world !', return_tensors=\"pt\",)\n",
        "prediction = predict_on_text(model, ids.to(device))\n",
        "assert isinstance(prediction, int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69zOpqp90ew7"
      },
      "source": [
        "def predict_on_dataset(model, texts, target, mapping):\n",
        "    \"\"\"\n",
        "    Run Language Model on a dataset and use its predictions as\n",
        "    labels for sentences\n",
        "    Args:\n",
        "        model: huggingface LM model\n",
        "        texts: List[str], texts of dataset\n",
        "        target: List[int], indices of classes\n",
        "        mapping: Dict[int, str] mapping from class indices to class labels\n",
        "    Returns:\n",
        "        model predictions, \"as is\", List[str]\n",
        "        target labels, after mapping, List[str]\n",
        "    \"\"\"\n",
        "    if len(texts) != len(target):\n",
        "        raise RuntimeError('Texts and target lengths mismatch')\n",
        "    # max_length has additional -1 because we will generate exactly 1 token \n",
        "    # with LM\n",
        "    max_length = model.config.n_ctx - 1\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    for idx in tqdm(range(len(texts))):\n",
        "        text, label = texts[idx], target[idx]\n",
        "        ids = tokenizer.encode(\n",
        "            get_prompt(input, separator, text, marker), \n",
        "            add_special_tokens=False, \n",
        "            return_tensors=\"pt\",\n",
        "            max_length=max_length\n",
        "        )\n",
        "        idx = predict_on_text(model, ids.to(device))\n",
        "        predictions.append(tokenizer.decode([idx]))  # generation.cpu().numpy()[0][-1]\n",
        "        labels.append(mapping[label]) \n",
        "    return predictions, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abvFt18q0ew8"
      },
      "source": [
        "predictions, target = predict_on_dataset(model, small_texts, small_labels, mapping)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIECT9O-0ew9"
      },
      "source": [
        "### Замер качества"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13EoUAQ10ew9"
      },
      "source": [
        "Посмотрим, что выдаёт нам языковая модель:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgiZYWnW0ew-"
      },
      "source": [
        "print(predictions[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjzhPs7i0ew_"
      },
      "source": [
        "Можно заметить, что предсказанные метки не совсем похожи на pos и neg, которые были у нас в таргете. Напишем простую функцию, которая нормализует предсказания модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPB04pnd0exA"
      },
      "source": [
        "def normalize_prediction(raw_output: str) -> str:\n",
        "    \"\"\"\n",
        "    Helper that transforms model prediction to normal\n",
        "    For example, ' positive' -> 'pos', ' Negative' -> neg\n",
        "    Args:\n",
        "        raw_output: str, output from language model\n",
        "    Returns:\n",
        "        normalized value: no leading/trailing spaces, lowercase,\n",
        "        at most 3 chars long\n",
        "    \"\"\"\n",
        "    # TASK: write prediction normalizer\n",
        "    # You need to remove spaces, lowercase and get only 3 leading chars\n",
        "    # Our implementation is 1 line\n",
        "    # YOUR CODE STARTS\n",
        "\n",
        "    # YOUR CODE ENDS\n",
        "    return normalized_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euC0k3nFtojd"
      },
      "source": [
        "Напишем функцию, вычисляющую точность:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CozGfPMo0exA"
      },
      "source": [
        "def accuracy(predictions, labels) -> float:\n",
        "    \"\"\"\n",
        "    Helper that transforms model prediction to normal\n",
        "    For example, ' positive' -> 'pos', ' Negative' -> neg\n",
        "    Args:\n",
        "        predictions: List[str], model predictions, should be labels in natural language\n",
        "        labels: List[str], target labels\n",
        "    Returns:\n",
        "        accuracy, float from [0, 1]\n",
        "    \"\"\"\n",
        "    if not labels:\n",
        "        # sanity check to avoid zero division\n",
        "        return 0\n",
        "    if len(predictions) != len(labels):\n",
        "        raise ValueError(f'Predictions and labels have mismatched length')\n",
        "    total = len(predictions)\n",
        "    match = 0\n",
        "    # TASK: calculate accuracy\n",
        "    # For every pair of predicted and real labels, check that they coincide.\n",
        "    # You can use zip() method for iteration over two sequences\n",
        "    # simultaneously.\n",
        "    # Our implementation is 3 lines\n",
        "    # YOUR CODE STARTS\n",
        "\n",
        "    # YOUR CODE ENDS\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_632NyJB0exA"
      },
      "source": [
        "normalized_predictions = [normalize_prediction(label) for label in predictions]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1hyGmSH0exB"
      },
      "source": [
        "accuracy(normalized_predictions, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04SFZ3Wo0exC"
      },
      "source": [
        "### Ура, модель без обучения работает лучше случайного угадывания!\n",
        "\n",
        "Тем не менее, это заметно хуже, чем у полносвязной нейросети. **Учитывая требования по ресурсам и время \n",
        "работы, использование такой модели в реальных задачах непрактично!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srG4Gk3p0exC"
      },
      "source": [
        "## А теперь замер на всем тестовом сете:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoTvsnWP0exD"
      },
      "source": [
        "predictions, target = predict_on_dataset(model, texts, labels, mapping)\n",
        "normalized_predictions = [normalize_prediction(label) for label in predictions]\n",
        "accuracy(normalized_predictions, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NZiUCu2xmu_"
      },
      "source": [
        "Все задание считается выполненным, если вы достигли accuracy 0.6 и выше на тестовом датасете. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xybBRqOz0exE"
      },
      "source": [
        "Данный пример призван продемонстрировать, что большая языковая модель хоть и справляется с задачей без лишних данных, но делает это заметно хуже специально созданных для задачи моделей, к тому же работает непростительно долго."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cFiadD10exE"
      },
      "source": [
        "### Резюме\n",
        "\n",
        "В этом задании мы применили большие языковые модели на практике и решили с её помощью 2 задачи:\n",
        "- conditional генерация текста\n",
        "- few-shot классификация текстов\n",
        "\n",
        "Мы увидели, что модель генерирует связный текст и может решать задачу классификации без изменений своих весов.\n",
        "\n",
        "Несмотря на возможности языковых моделей, их применение на практике всё ещё осложнено большими потребляемыми ресурсами, а также невозможностью интерпретировать предсказания."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUVsBkng0exE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}