{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seminar_3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_zLNARu9cYDv",
        "yKD4GgGpfybD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7e6d9e59ee9b451aa04430d9ce516c55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c88d90bdc888487eb7fe9451946a4739",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_34533ef1fadd463c9598ebf8f53a7c8c",
              "IPY_MODEL_a35400a0cf8246de85384acdc1eb755c",
              "IPY_MODEL_f469c10af54a4a70bdfa0f2b3ccbf173"
            ]
          }
        },
        "c88d90bdc888487eb7fe9451946a4739": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "34533ef1fadd463c9598ebf8f53a7c8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e8faeb40de2449f88818310aa80d2a7e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: ",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_83e0e05539af499f886e0103677ffc8c"
          }
        },
        "a35400a0cf8246de85384acdc1eb755c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_77619f3e630e4f93b024c99e77a98f12",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2603,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2603,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0731ec5d3da2418fbba88c50f3f636c7"
          }
        },
        "f469c10af54a4a70bdfa0f2b3ccbf173": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c1bfb722a8f447a49905cf1b8d0a65f9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9.52k/? [00:00&lt;00:00, 216kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_abe4ebc9760d4063be50cf77b5f6697d"
          }
        },
        "e8faeb40de2449f88818310aa80d2a7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "83e0e05539af499f886e0103677ffc8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "77619f3e630e4f93b024c99e77a98f12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0731ec5d3da2418fbba88c50f3f636c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c1bfb722a8f447a49905cf1b8d0a65f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "abe4ebc9760d4063be50cf77b5f6697d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/delhian/NLP_course/blob/master/week3/seminar_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFIUUYWtVMRB"
      },
      "source": [
        "# Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C6hPcSsPF42"
      },
      "source": [
        "In this lecture we will get insight into very popular NLP task - Named Entity Recognition.<br>Our goal is to:\n",
        "- build a good baseline solution\n",
        "- modify the data markup\n",
        "- learn how to solve this problem using neural network methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iilmDaCFVOXX"
      },
      "source": [
        "In first part we will explore how to get fast solution of this task, how to exlore metrics and how to convert labeling.<br>\n",
        "In the second part we will look how we can solve this task by using different architectures and measure them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsLFL7s0VRsK"
      },
      "source": [
        "What we will learn:\n",
        "- non neural approaches for NER-task;\n",
        "- measure quality of model for NER-task;\n",
        "- different markup for NER-task;\n",
        "- data preparation for neural network solution of NER;\n",
        "- using different neural approaches for NER;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIKjsBqIVU2x"
      },
      "source": [
        "# Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rrym_4yiVWok"
      },
      "source": [
        "## Solving NER task without Neural netowrks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdYixEjkRfrC"
      },
      "source": [
        "!pip install datasets > /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI6yaeHZRjbc"
      },
      "source": [
        "import pytest\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import LongTensor, FloatTensor\n",
        "from torch.nn import functional as F\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from torch.utils.data import Dataset\n",
        "from torch.optim import Adam\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilQEoIuGW6YB"
      },
      "source": [
        "### look at the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoASj4VoRrqN"
      },
      "source": [
        "For this task we will use common NER-dataset which is always included in all benchmarks, when scientists measure quality of SOTA solutions for NER.<br>\n",
        "The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on four types of named entities: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzSRKAzLRpT_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "7e6d9e59ee9b451aa04430d9ce516c55",
            "c88d90bdc888487eb7fe9451946a4739",
            "34533ef1fadd463c9598ebf8f53a7c8c",
            "a35400a0cf8246de85384acdc1eb755c",
            "f469c10af54a4a70bdfa0f2b3ccbf173",
            "e8faeb40de2449f88818310aa80d2a7e",
            "83e0e05539af499f886e0103677ffc8c",
            "77619f3e630e4f93b024c99e77a98f12",
            "0731ec5d3da2418fbba88c50f3f636c7",
            "c1bfb722a8f447a49905cf1b8d0a65f9",
            "abe4ebc9760d4063be50cf77b5f6697d"
          ]
        },
        "outputId": "071fb257-ad53-4a8a-cd4c-c94d89d3b92b"
      },
      "source": [
        "dataset_base = load_dataset(\"conll2003\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e6d9e59ee9b451aa04430d9ce516c55",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.60k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ded9672edbc84192b98246f74c9926b3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.78k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset conll2003/conll2003 (download: 4.63 MiB, generated: 9.78 MiB, post-processed: Unknown size, total: 14.41 MiB) to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14dd4bfdeb9d435a99a5aee16ee73fa0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e7652102570400ca0f0c441ecc90a58",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/650k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac750d48dc874576b6476597fadf86f1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/163k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7476b38216046f2ad2d114ece8801ed",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/146k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41cb0ef7af024ddeb0c6e9600ad00838",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e85a07f782164b249eba20ba4ff6cd53",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "171045a9e1e841bbbc01152da509a1eb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "371d01a436ed42449503778e07e52d10",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset conll2003 downloaded and prepared to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b65b4feefa2742a39449f045dd912940",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju3RL22uR_Tl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "785eb771-b42d-4b74-ecb5-fdc3135533a5"
      },
      "source": [
        "dataset_base['train']"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "    num_rows: 14041\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SejZA2vpSmUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0384269c-544d-4c80-8baf-0c14a5d52847"
      },
      "source": [
        "import json\n",
        "mapping_ = {v: k for k, v in dataset_base[\"train\"].features[\"ner_tags\"].feature._str2int.items()}\n",
        "\n",
        "with open('mapping.json', 'w') as f:\n",
        "  json.dump(mapping_, f)\n",
        "mapping_"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'O',\n",
              " 1: 'B-PER',\n",
              " 2: 'I-PER',\n",
              " 3: 'B-ORG',\n",
              " 4: 'I-ORG',\n",
              " 5: 'B-LOC',\n",
              " 6: 'I-LOC',\n",
              " 7: 'B-MISC',\n",
              " 8: 'I-MISC'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3dkmzC6TGh9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26fbb419-f4e0-461c-cf65-f02d6670db4a"
      },
      "source": [
        "for i in range(10):\n",
        "  print(i + 1, ' '.join(dataset_base[\"train\"]['tokens'][i]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 EU rejects German call to boycott British lamb .\n",
            "2 Peter Blackburn\n",
            "3 BRUSSELS 1996-08-22\n",
            "4 The European Commission said on Thursday it disagreed with German advice to consumers to shun British lamb until scientists determine whether mad cow disease can be transmitted to sheep .\n",
            "5 Germany 's representative to the European Union 's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .\n",
            "6 \" We do n't support any such recommendation because we do n't see any grounds for it , \" the Commission 's chief spokesman Nikolaus van der Pas told a news briefing .\n",
            "7 He said further scientific study was required and if it was found that action was needed it should be taken by the European Union .\n",
            "8 He said a proposal last month by EU Farm Commissioner Franz Fischler to ban sheep brains , spleens and spinal cords from the human and animal food chains was a highly specific and precautionary move to protect human health .\n",
            "9 Fischler proposed EU-wide measures after reports from Britain and France that under laboratory conditions sheep could contract Bovine Spongiform Encephalopathy ( BSE ) -- mad cow disease .\n",
            "10 But Fischler agreed to review his proposal after the EU 's standing veterinary committee , mational animal health officials , questioned if such action was justified as there was only a slight risk to human health .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f1pzbaOJjB2"
      },
      "source": [
        "#### Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bujckdrmJl7d"
      },
      "source": [
        "Count occurence of each entity. Print number of occurences for each entity. Result must be a dictinary, where keys are entities from `dataset_base[\"train\"]['ner_tags']` and values are total number of occurencies for each key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmpBc17GTro1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2301b0f1-23d7-4d92-c5a8-1d880d03998e"
      },
      "source": [
        "%%time\n",
        "counter = {}\n",
        "\n",
        "for tags in dataset_base[\"train\"]['ner_tags']:\n",
        "  for tag in tags:\n",
        "    counter[mapping_[tag]] = counter.get(mapping_[tag], 0) + 1\n",
        "\n",
        "counter"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 181 ms, sys: 479 µs, total: 182 ms\n",
            "Wall time: 183 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSwUL6qYnIjU"
      },
      "source": [
        "assert len(counter) == 9\n",
        "assert counter['O'] > 169000"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGWxc-pCB8VK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7301792c-87bf-4489-ecb5-e61051351acc"
      },
      "source": [
        "counter"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'B-LOC': 7140,\n",
              " 'B-MISC': 3438,\n",
              " 'B-ORG': 6321,\n",
              " 'B-PER': 6600,\n",
              " 'I-LOC': 1157,\n",
              " 'I-MISC': 1155,\n",
              " 'I-ORG': 3704,\n",
              " 'I-PER': 4528,\n",
              " 'O': 169578}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOC2dHK0X9SL"
      },
      "source": [
        "As you see, we have dominating number of class `O`. Our main goal is to make such model, that will not overfit to predict always `O` token.<br>\n",
        "What metrics are more appropriate to measure quality of models for NER?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA5g7XGEJTya"
      },
      "source": [
        "### Sklearn-crf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pwR8trFYXmr"
      },
      "source": [
        "Now I'd like to introduce you great library, that can provide light and easy implementation for solving NER-task. It's name is `sklearn-crf`. It has familiar interface to basic sklearn, but is based on very powerful tool for NER-task - CRF(Conditional Random Field). <br>\n",
        "CRF is nowdays the de facto standard for solving the NLP problem. Even in the most modern SOTA neural networks approaches, a CRF layer can now often be seen as an output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Uco_i3lZgUd"
      },
      "source": [
        "!pip install sklearn_crfsuite > /dev/null"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcTxcqd6Zhb9"
      },
      "source": [
        "import sklearn\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1-YPeBkZqJa"
      },
      "source": [
        "As all sklearn-like libraries we need to get pandas.DataFrame as an input for this model. Let's create it.<br>\n",
        "In our DataFrame we will make each word, entity and sentence_id on each row."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htp-v1JRZmm-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "badaf3af-77f2-4690-a677-2fe27d0be123"
      },
      "source": [
        "df = pd.DataFrame({'sent_id': [i for j in [[i] * len(s['tokens']) for i, s in enumerate(dataset_base['train'])] for i in j],\n",
        "                   'data': [i for j in dataset_base['train'] for i in j['tokens']],\n",
        "                   'entities': [mapping_[i] for j in dataset_base['train'] for i in j['ner_tags']]})\n",
        "df.head(20)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent_id</th>\n",
              "      <th>data</th>\n",
              "      <th>entities</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>EU</td>\n",
              "      <td>B-ORG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>rejects</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>German</td>\n",
              "      <td>B-MISC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>call</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>to</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>boycott</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>British</td>\n",
              "      <td>B-MISC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>lamb</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>Peter</td>\n",
              "      <td>B-PER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>Blackburn</td>\n",
              "      <td>I-PER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2</td>\n",
              "      <td>BRUSSELS</td>\n",
              "      <td>B-LOC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2</td>\n",
              "      <td>1996-08-22</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>3</td>\n",
              "      <td>The</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>3</td>\n",
              "      <td>European</td>\n",
              "      <td>B-ORG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3</td>\n",
              "      <td>Commission</td>\n",
              "      <td>I-ORG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>3</td>\n",
              "      <td>said</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>3</td>\n",
              "      <td>on</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3</td>\n",
              "      <td>Thursday</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>3</td>\n",
              "      <td>it</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sent_id        data entities\n",
              "0         0          EU    B-ORG\n",
              "1         0     rejects        O\n",
              "2         0      German   B-MISC\n",
              "3         0        call        O\n",
              "4         0          to        O\n",
              "5         0     boycott        O\n",
              "6         0     British   B-MISC\n",
              "7         0        lamb        O\n",
              "8         0           .        O\n",
              "9         1       Peter    B-PER\n",
              "10        1   Blackburn    I-PER\n",
              "11        2    BRUSSELS    B-LOC\n",
              "12        2  1996-08-22        O\n",
              "13        3         The        O\n",
              "14        3    European    B-ORG\n",
              "15        3  Commission    I-ORG\n",
              "16        3        said        O\n",
              "17        3          on        O\n",
              "18        3    Thursday        O\n",
              "19        3          it        O"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeY96hlsZ-YT"
      },
      "source": [
        "Now we have dataframe, where only 3 columns exsists:\n",
        " - sentense_id - which mark each word belonging to each sentence\n",
        " - data contains words on each row\n",
        " - entities marks which entity does each word refer to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WImNIQKwbHYs"
      },
      "source": [
        "We also need a class, that will process each sentence and aggregate words and entities in it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAcrp66Qa7mY"
      },
      "source": [
        "class SentenceGetter(object):\n",
        "    \n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, t) for w, t in zip(s['data'].values.tolist(), \n",
        "                                                     s['entities'].values.tolist())]\n",
        "        self.grouped = self.data.groupby('sent_id').apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "        \n",
        "    def get_next(self):\n",
        "        try: \n",
        "            s = self.grouped['Sentence: {}'.format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s \n",
        "        except:\n",
        "            return None"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qa-NFSnKbDEd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5e47ce5-be4e-49e7-846c-c5dc1f392652"
      },
      "source": [
        "getter = SentenceGetter(df)\n",
        "sentences = getter.sentences\n",
        "sentences[0]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('EU', 'B-ORG'),\n",
              " ('rejects', 'O'),\n",
              " ('German', 'B-MISC'),\n",
              " ('call', 'O'),\n",
              " ('to', 'O'),\n",
              " ('boycott', 'O'),\n",
              " ('British', 'B-MISC'),\n",
              " ('lamb', 'O'),\n",
              " ('.', 'O')]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQsEoGBsbXwl"
      },
      "source": [
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "    \n",
        "    features = {\n",
        "        'bias': 1.0, \n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit()\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        features.update({\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper()\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        features.update({\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper()\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for token, label in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [token for token, label in sent]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txY55dbQb1DH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d2686c2-18f5-485e-bc1c-84659092086d"
      },
      "source": [
        "X = [sent2features(s) for s in sentences]\n",
        "y = [sent2labels(s) for s in sentences]\n",
        "len(X)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14041"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMlf5JJrb7WW"
      },
      "source": [
        "X_train = X[:10000]\n",
        "X_test = X[10000:]\n",
        "y_train = y[:10000]\n",
        "y_test = y[10000:]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOjIhfYcb-CK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d2870da-d4f7-4420-e75f-7e716c49b1d6"
      },
      "source": [
        "%%time\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True,\n",
        "    verbose=True\n",
        ")\n",
        "crf.fit(X_train, y_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading training data to CRFsuite: 100%|██████████| 10000/10000 [00:00<00:00, 14019.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature generation\n",
            "type: CRF1d\n",
            "feature.minfreq: 0.000000\n",
            "feature.possible_states: 0\n",
            "feature.possible_transitions: 1\n",
            "0....1....2....3....4....5....6....7....8....9....10\n",
            "Number of features: 2954\n",
            "Seconds required: 0.085\n",
            "\n",
            "L-BFGS optimization\n",
            "c1: 0.100000\n",
            "c2: 0.100000\n",
            "num_memories: 6\n",
            "max_iterations: 100\n",
            "epsilon: 0.000010\n",
            "stop: 10\n",
            "delta: 0.000010\n",
            "linesearch: MoreThuente\n",
            "linesearch.max_iterations: 20\n",
            "\n",
            "Iter 1   time=0.23  loss=173256.49 active=2930  feature_norm=1.00\n",
            "Iter 2   time=0.24  loss=132654.60 active=2796  feature_norm=3.04\n",
            "Iter 3   time=0.12  loss=110485.96 active=2699  feature_norm=2.59\n",
            "Iter 4   time=0.25  loss=97099.18 active=2747  feature_norm=2.22\n",
            "Iter 5   time=0.12  loss=88075.12 active=2874  feature_norm=2.58\n",
            "Iter 6   time=0.12  loss=80585.90 active=2849  feature_norm=3.05\n",
            "Iter 7   time=0.12  loss=62716.54 active=2801  feature_norm=5.39\n",
            "Iter 8   time=0.12  loss=56430.24 active=2862  feature_norm=6.07\n",
            "Iter 9   time=0.13  loss=50203.36 active=2877  feature_norm=7.93\n",
            "Iter 10  time=0.24  loss=47707.36 active=2871  feature_norm=9.20\n",
            "Iter 11  time=0.12  loss=44790.68 active=2898  feature_norm=10.32\n",
            "Iter 12  time=0.12  loss=43236.51 active=2908  feature_norm=11.29\n",
            "Iter 13  time=0.12  loss=41548.04 active=2900  feature_norm=12.31\n",
            "Iter 14  time=0.12  loss=39552.44 active=2872  feature_norm=14.01\n",
            "Iter 15  time=0.12  loss=38275.88 active=2885  feature_norm=15.29\n",
            "Iter 16  time=0.12  loss=37350.79 active=2881  feature_norm=16.65\n",
            "Iter 17  time=0.12  loss=36189.65 active=2888  feature_norm=18.17\n",
            "Iter 18  time=0.13  loss=34526.99 active=2874  feature_norm=22.34\n",
            "Iter 19  time=0.12  loss=33148.60 active=2874  feature_norm=25.61\n",
            "Iter 20  time=0.13  loss=32060.91 active=2875  feature_norm=27.95\n",
            "Iter 21  time=0.12  loss=30914.17 active=2886  feature_norm=30.44\n",
            "Iter 22  time=0.13  loss=30044.70 active=2893  feature_norm=33.78\n",
            "Iter 23  time=0.12  loss=29084.99 active=2890  feature_norm=36.49\n",
            "Iter 24  time=0.13  loss=28177.10 active=2882  feature_norm=39.37\n",
            "Iter 25  time=0.13  loss=27320.37 active=2875  feature_norm=43.24\n",
            "Iter 26  time=0.12  loss=26750.97 active=2878  feature_norm=46.54\n",
            "Iter 27  time=0.12  loss=26277.70 active=2878  feature_norm=49.20\n",
            "Iter 28  time=0.12  loss=25840.34 active=2871  feature_norm=51.39\n",
            "Iter 29  time=0.12  loss=25318.79 active=2864  feature_norm=55.25\n",
            "Iter 30  time=0.12  loss=24897.93 active=2866  feature_norm=58.08\n",
            "Iter 31  time=0.12  loss=24523.46 active=2869  feature_norm=60.95\n",
            "Iter 32  time=0.14  loss=24073.00 active=2854  feature_norm=65.95\n",
            "Iter 33  time=0.36  loss=24051.88 active=2871  feature_norm=68.17\n",
            "Iter 34  time=0.12  loss=23742.54 active=2866  feature_norm=70.89\n",
            "Iter 35  time=0.12  loss=23591.73 active=2861  feature_norm=73.44\n",
            "Iter 36  time=0.12  loss=23383.35 active=2864  feature_norm=76.72\n",
            "Iter 37  time=0.12  loss=23183.99 active=2851  feature_norm=80.50\n",
            "Iter 38  time=0.13  loss=22999.54 active=2869  feature_norm=83.97\n",
            "Iter 39  time=0.13  loss=22866.27 active=2862  feature_norm=86.59\n",
            "Iter 40  time=0.12  loss=22750.85 active=2857  feature_norm=89.80\n",
            "Iter 41  time=0.13  loss=22681.57 active=2856  feature_norm=91.62\n",
            "Iter 42  time=0.12  loss=22612.19 active=2862  feature_norm=93.04\n",
            "Iter 43  time=0.12  loss=22548.00 active=2853  feature_norm=94.80\n",
            "Iter 44  time=0.13  loss=22495.06 active=2852  feature_norm=96.22\n",
            "Iter 45  time=0.12  loss=22450.42 active=2862  feature_norm=97.21\n",
            "Iter 46  time=0.12  loss=22411.95 active=2855  feature_norm=98.91\n",
            "Iter 47  time=0.12  loss=22378.83 active=2840  feature_norm=100.16\n",
            "Iter 48  time=0.12  loss=22351.36 active=2849  feature_norm=100.53\n",
            "Iter 49  time=0.12  loss=22323.75 active=2832  feature_norm=101.11\n",
            "Iter 50  time=0.11  loss=22296.90 active=2829  feature_norm=101.52\n",
            "Iter 51  time=0.13  loss=22265.40 active=2832  feature_norm=101.69\n",
            "Iter 52  time=0.12  loss=22247.88 active=2827  feature_norm=101.78\n",
            "Iter 53  time=0.12  loss=22226.31 active=2813  feature_norm=101.91\n",
            "Iter 54  time=0.12  loss=22208.61 active=2815  feature_norm=102.13\n",
            "Iter 55  time=0.13  loss=22189.36 active=2825  feature_norm=102.34\n",
            "Iter 56  time=0.12  loss=22173.19 active=2827  feature_norm=102.50\n",
            "Iter 57  time=0.12  loss=22159.95 active=2827  feature_norm=102.78\n",
            "Iter 58  time=0.12  loss=22143.29 active=2824  feature_norm=103.00\n",
            "Iter 59  time=0.12  loss=22130.29 active=2826  feature_norm=103.27\n",
            "Iter 60  time=0.12  loss=22118.42 active=2827  feature_norm=103.57\n",
            "Iter 61  time=0.12  loss=22107.26 active=2820  feature_norm=103.81\n",
            "Iter 62  time=0.12  loss=22098.57 active=2823  feature_norm=104.03\n",
            "Iter 63  time=0.13  loss=22090.08 active=2816  feature_norm=104.28\n",
            "Iter 64  time=0.12  loss=22082.27 active=2813  feature_norm=104.53\n",
            "Iter 65  time=0.12  loss=22073.84 active=2808  feature_norm=104.77\n",
            "Iter 66  time=0.13  loss=22068.51 active=2808  feature_norm=104.99\n",
            "Iter 67  time=0.12  loss=22061.43 active=2806  feature_norm=105.17\n",
            "Iter 68  time=0.13  loss=22056.65 active=2806  feature_norm=105.31\n",
            "Iter 69  time=0.12  loss=22051.49 active=2805  feature_norm=105.40\n",
            "Iter 70  time=0.12  loss=22047.06 active=2803  feature_norm=105.50\n",
            "Iter 71  time=0.12  loss=22042.39 active=2804  feature_norm=105.63\n",
            "Iter 72  time=0.12  loss=22037.33 active=2809  feature_norm=105.76\n",
            "Iter 73  time=0.12  loss=22031.44 active=2808  feature_norm=105.88\n",
            "Iter 74  time=0.12  loss=22026.22 active=2803  feature_norm=105.97\n",
            "Iter 75  time=0.12  loss=22021.21 active=2799  feature_norm=106.08\n",
            "Iter 76  time=0.12  loss=22016.49 active=2797  feature_norm=106.17\n",
            "Iter 77  time=0.12  loss=22010.45 active=2797  feature_norm=106.27\n",
            "Iter 78  time=0.12  loss=22006.55 active=2796  feature_norm=106.36\n",
            "Iter 79  time=0.12  loss=22002.23 active=2795  feature_norm=106.45\n",
            "Iter 80  time=0.13  loss=21998.45 active=2795  feature_norm=106.54\n",
            "Iter 81  time=0.13  loss=21993.07 active=2797  feature_norm=106.64\n",
            "Iter 82  time=0.12  loss=21989.99 active=2800  feature_norm=106.72\n",
            "Iter 83  time=0.12  loss=21984.53 active=2801  feature_norm=106.80\n",
            "Iter 84  time=0.12  loss=21982.57 active=2801  feature_norm=106.88\n",
            "Iter 85  time=0.13  loss=21976.76 active=2801  feature_norm=106.96\n",
            "Iter 86  time=0.12  loss=21974.84 active=2803  feature_norm=107.03\n",
            "Iter 87  time=0.12  loss=21969.86 active=2804  feature_norm=107.10\n",
            "Iter 88  time=0.13  loss=21967.35 active=2803  feature_norm=107.17\n",
            "Iter 89  time=0.12  loss=21962.83 active=2801  feature_norm=107.24\n",
            "Iter 90  time=0.13  loss=21961.03 active=2803  feature_norm=107.32\n",
            "Iter 91  time=0.12  loss=21955.64 active=2802  feature_norm=107.38\n",
            "Iter 92  time=0.12  loss=21954.48 active=2798  feature_norm=107.46\n",
            "Iter 93  time=0.12  loss=21949.16 active=2799  feature_norm=107.52\n",
            "Iter 94  time=0.12  loss=21948.84 active=2799  feature_norm=107.60\n",
            "Iter 95  time=0.12  loss=21943.51 active=2800  feature_norm=107.65\n",
            "Iter 96  time=0.12  loss=21943.28 active=2799  feature_norm=107.72\n",
            "Iter 97  time=0.12  loss=21938.75 active=2799  feature_norm=107.76\n",
            "Iter 98  time=0.12  loss=21938.42 active=2799  feature_norm=107.82\n",
            "Iter 99  time=0.12  loss=21934.19 active=2797  feature_norm=107.87\n",
            "Iter 100 time=0.12  loss=21933.67 active=2800  feature_norm=107.92\n",
            "L-BFGS terminated with the maximum number of iterations\n",
            "Total seconds required for training: 12.916\n",
            "\n",
            "Storing the model\n",
            "Number of active features: 2800 (2954)\n",
            "Number of active attributes: 966 (1066)\n",
            "Number of active labels: 9 (9)\n",
            "Writing labels\n",
            "Writing attributes\n",
            "Writing feature references for transitions\n",
            "Writing feature references for attributes\n",
            "Seconds required: 0.001\n",
            "\n",
            "CPU times: user 13.6 s, sys: 94.1 ms, total: 13.7 s\n",
            "Wall time: 13.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h064girfcGdq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6633d3dd-5545-4e62-d63e-d2c8b520da80"
      },
      "source": [
        "all_entities = sorted(df.entities.unique().tolist())\n",
        "y_pred = crf.predict(X_test)\n",
        "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=[i for i in all_entities if i != 'O'])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5996010363706731"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOg9TIVOKOKD"
      },
      "source": [
        "#### Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF4g_dZmKP0p"
      },
      "source": [
        "Print classification report for all useful tokens (exluding token `O`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myDSv6zPcM4R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d9c4bab-bdf8-4098-9f04-a1ec6e798c47"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "print(metrics.flat_classification_report(y_test, y_pred, labels = [i for i in all_entities if i != 'O']))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.62      0.64      0.63      2205\n",
            "      B-MISC       0.64      0.61      0.63      1103\n",
            "       B-ORG       0.52      0.49      0.50      1739\n",
            "       B-PER       0.64      0.57      0.60      1976\n",
            "       I-LOC       0.53      0.41      0.46       422\n",
            "      I-MISC       0.55      0.37      0.45       370\n",
            "       I-ORG       0.60      0.62      0.61      1149\n",
            "       I-PER       0.68      0.80      0.73      1297\n",
            "\n",
            "   micro avg       0.61      0.60      0.60     10261\n",
            "   macro avg       0.60      0.56      0.58     10261\n",
            "weighted avg       0.61      0.60      0.60     10261\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFR46OtaJzsj"
      },
      "source": [
        "#### Task 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVnO-hahJ1di"
      },
      "source": [
        "Make some additional features to reach at least 0.82 weighted f1-score on detection all useful tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zLNARu9cYDv"
      },
      "source": [
        "##### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXaPZQVMcU7V"
      },
      "source": [
        "# 1. You can check for lower() each word\n",
        "# 2. You can add more words to features, for example last 3 words words[-3:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXHti7ticZYU"
      },
      "source": [
        "##### continue work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyPIr9VEdbfs"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "    \n",
        "    features = {\n",
        "        'bias': 1.0, \n",
        "        # add some here\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        'word.isalnum()': word.isalnum(),\n",
        "        'word.islower()': word.islower()\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        features.update({\n",
        "            # add something here\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "            '-1:word.isdigit()': word1.isdigit(),\n",
        "            '-1:word.isalnum()': word1.isalnum(),\n",
        "            '-1:word.islower()': word1.islower()\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        features.update({\n",
        "            # add something here\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "            '+1:word.isdigit()': word1.isdigit(),\n",
        "            '+1:word.isalnum()': word1.isalnum(),\n",
        "            '+1:word.islower()': word1.islower()\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    if i > 1:\n",
        "        word2 = sent[i-2][0]\n",
        "        features.update({\n",
        "            # add something here\n",
        "            '-2:word.istitle()': word2.istitle(),\n",
        "            '-2:word.isupper()': word2.isupper(),\n",
        "            '-2:word.isdigit()': word2.isdigit(),\n",
        "            '-2:word.isalnum()': word2.isalnum(),\n",
        "            '-2:word.islower()': word2.islower()\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent)-2:\n",
        "        word2 = sent[i+2][0]\n",
        "        features.update({\n",
        "            # add something here\n",
        "            '+2:word.istitle()': word2.istitle(),\n",
        "            '+2:word.isupper()': word2.isupper(),\n",
        "            '+2:word.isdigit()': word2.isdigit(),\n",
        "            '+2:word.isalnum()': word2.isalnum(),\n",
        "            '+2:word.islower()': word2.islower()\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    if i > 2:\n",
        "        word3 = sent[i-3][0]\n",
        "        features.update({\n",
        "            # add something here\n",
        "            '-3:word.istitle()': word3.istitle(),\n",
        "            '-3:word.isupper()': word3.isupper(),\n",
        "            '-3:word.isdigit()': word3.isdigit(),\n",
        "            '-3:word.isalnum()': word3.isalnum(),\n",
        "            '-3:word.islower()': word3.islower()\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent)-3:\n",
        "        word3 = sent[i+3][0]\n",
        "        features.update({\n",
        "            # add something here\n",
        "            '+3:word.istitle()': word3.istitle(),\n",
        "            '+3:word.isupper()': word3.isupper(),\n",
        "            '+3:word.isdigit()': word3.isdigit(),\n",
        "            '+3:word.isalnum()': word3.isalnum(),\n",
        "            '+3:word.islower()': word3.islower()\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    if i > 3:\n",
        "        word4 = sent[i-4][0]\n",
        "        features.update({\n",
        "            # add something here\n",
        "            '-4:word.istitle()': word4.istitle(),\n",
        "            '-4:word.isupper()': word4.isupper(),\n",
        "            '-4:word.isdigit()': word4.isdigit(),\n",
        "            '-4:word.isalnum()': word4.isalnum(),\n",
        "            '-4:word.islower()': word4.islower()\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent)-4:\n",
        "        word4 = sent[i+4][0]\n",
        "        features.update({\n",
        "            # add something here\n",
        "            '+4:word.istitle()': word4.istitle(),\n",
        "            '+4:word.isupper()': word4.isupper(),\n",
        "            '+4:word.isdigit()': word4.isdigit(),\n",
        "            '+4:word.isalnum()': word4.isalnum(),\n",
        "            '+4:word.islower()': word4.islower()\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for token, label in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [token for token, label in sent]"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSnRyXFrdfJM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30d6570d-2129-471c-d147-6d38d2996d59"
      },
      "source": [
        "# explore quality for your new features\n",
        "\n",
        "X = [sent2features(s) for s in sentences]\n",
        "y = [sent2labels(s) for s in sentences]\n",
        "len(X)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14041"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpAmkfSe7Pwe"
      },
      "source": [
        "X_train = X[:10000]\n",
        "X_test = X[10000:]\n",
        "y_train = y[:10000]\n",
        "y_test = y[10000:]"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zNwJEUs7Fwx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc085949-972c-4e63-bd19-ce6e33e561fe"
      },
      "source": [
        "%%time\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True,\n",
        "    verbose=False\n",
        ")\n",
        "crf.fit(X_train, y_train)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 31.1 s, sys: 110 ms, total: 31.2 s\n",
            "Wall time: 31.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZlm9I0n7coK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5135dfa3-ade5-4e94-8cea-4d9c73f3c46f"
      },
      "source": [
        "all_entities = sorted(df.entities.unique().tolist())\n",
        "y_pred = crf.predict(X_test)\n",
        "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=[i for i in all_entities if i != 'O'])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6613944287508109"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqbkWJ5p7TVm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f9535b3-237b-492c-a366-4d6b0a9bdc39"
      },
      "source": [
        "print(metrics.flat_classification_report(y_test, y_pred, labels=[i for i in all_entities if i != 'O']),)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.68      0.73      0.70      2205\n",
            "      B-MISC       0.71      0.65      0.68      1103\n",
            "       B-ORG       0.64      0.57      0.61      1739\n",
            "       B-PER       0.68      0.64      0.66      1976\n",
            "       I-LOC       0.55      0.50      0.53       422\n",
            "      I-MISC       0.55      0.42      0.48       370\n",
            "       I-ORG       0.62      0.64      0.63      1149\n",
            "       I-PER       0.74      0.83      0.79      1297\n",
            "\n",
            "   micro avg       0.67      0.66      0.66     10261\n",
            "   macro avg       0.65      0.62      0.63     10261\n",
            "weighted avg       0.67      0.66      0.66     10261\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqIEo9oBJXDz"
      },
      "source": [
        "### Converting markup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBLwmVHJKsYr"
      },
      "source": [
        "Now it's time to get acquainted to NER markup or NER data labeling.<br>\n",
        "When we work with almost every NLP task, we usually need our data to be labeled. For NER problem data labeling is often rather expensive. Often we ask to label just in text, and then simple label all tokens for `BIO`-markup.<br>\n",
        "But in some tasks in which we need to very accurately define separate entities, the `BILUO`-markup may come to the rescue.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe-STiqnkmv0"
      },
      "source": [
        "In our dataset we have `BIO-markup."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zmuw8OzKe8m"
      },
      "source": [
        "#### Task 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpmHaJKgKgvR"
      },
      "source": [
        "write function to convert `BIO`-markup into `BILUO`-markup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4Z_T1nDk9l5"
      },
      "source": [
        "entities_list = [[mapping_[token] for token in tokens] for tokens in dataset_base[\"train\"]['ner_tags']]\n",
        "# entities_list"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol31d3em9rHc"
      },
      "source": [
        "# B - 'beginning'\n",
        "# I - 'inside'\n",
        "# L - 'last'\n",
        "# O - 'outside'\n",
        "# U - 'unit'"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qql_NTIpktve"
      },
      "source": [
        "def bio_2_biluo(entities_list, missing: str = 'O'):\n",
        "  result = list()\n",
        "  for entities in entities_list:\n",
        "    current_new_markup = [entities[0]]\n",
        "# YOUR CODE HERE\n",
        "    entities_len = len(entities)\n",
        "    for id in range(1, entities_len - 1):\n",
        "      if entities[id][0] == 'I' and entities[id+1][0] == missing:\n",
        "        current_new_markup.append('L' + entities[id][1:])\n",
        "        continue\n",
        "      if entities[id][0] == 'B' and entities[id+1][0] == missing:\n",
        "        current_new_markup.append('U' + entities[id][1:])\n",
        "        continue\n",
        "      current_new_markup.append(entities[id])\n",
        "    if entities[-1][0] == 'I':\n",
        "        current_new_markup.append('L' + entities[-1][1:])\n",
        "    else:\n",
        "      current_new_markup.append(entities[-1])\n",
        "\n",
        "    # FILL code here\n",
        "    result.append(current_new_markup)\n",
        "  return result"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqSINs4MpNsk"
      },
      "source": [
        "assert len(bio_2_biluo(entities_list)) == len(entities_list)\n",
        "assert set(bio_2_biluo([entities_list[1]])[0]) == {'B-PER', 'L-PER'}\n",
        "assert len(set(bio_2_biluo([entities_list[7]])[0])) == 4"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZBBeLSdtlJZ"
      },
      "source": [
        "Sometimes after markup we have data labeled in offets: in plain text we get beginning and ending of each entity.<br>\n",
        "In this situations we can use function from spacy named `offsets_to_biluo_tags`. But you need to be careful, because sometimes it works incorrect. In this case you need to check translation of markup or write your own function to translate markups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnOZWlYhJcHF"
      },
      "source": [
        "Future readings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq7AOxcNVLDh"
      },
      "source": [
        "# you can also try to use spacy built-in ner model from spacy python library. Example of usage is here -> https://spacy.io/api/cli"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mqTd1lnVLvQ"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZVLX9gSuNCh"
      },
      "source": [
        "In this part we will try to use some basic approaches to solve NER-task. Dataset will be the same as above. In this part don't forget to change runtime of your notebook to `GPU`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5gnC8Fn6_D_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aa5cb91-b393-4a83-b0bd-6a4d3dfb3940"
      },
      "source": [
        "!pip install spacy==3.1"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy==3.1\n",
            "  Downloading spacy-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 10.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1) (3.7.4.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1) (0.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1) (21.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1) (4.62.3)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[K     |████████████████████████████████| 456 kB 33.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1) (2.23.0)\n",
            "Collecting thinc<8.1.0,>=8.0.7\n",
            "  Downloading thinc-8.0.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (623 kB)\n",
            "\u001b[K     |████████████████████████████████| 623 kB 42.4 MB/s \n",
            "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 41.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1) (1.19.5)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1) (3.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.1) (2.0.5)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.7\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.4\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.0-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==3.1) (57.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy==3.1) (3.6.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy==3.1) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy==3.1) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.1) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.1) (2.10)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy==3.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy==3.1) (2.0.1)\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-legacy, pathy, spacy\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.6 pathy-0.6.0 pydantic-1.8.2 spacy-3.1.0 spacy-legacy-3.0.8 srsly-2.4.1 thinc-8.0.10 typer-0.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLzibq6p8IMp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "de487045-a0f0-4e89-a591-4f2f9a6798f0"
      },
      "source": [
        "!pip install spacy-transformers"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy-transformers\n",
            "  Downloading spacy_transformers-1.0.6-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[?25l\r\u001b[K     |███████▊                        | 10 kB 22.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 20 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 30 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 40 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 42 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers) (1.9.0+cu111)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers) (3.1.0)\n",
            "Collecting spacy-alignments<1.0.0,>=0.7.2\n",
            "  Downloading spacy_alignments-0.8.3-cp37-cp37m-manylinux2014_x86_64.whl (998 kB)\n",
            "\u001b[K     |████████████████████████████████| 998 kB 26.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers) (2.4.1)\n",
            "Collecting transformers<4.10.0,>=3.4.0\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 43.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.1.0->spacy-transformers) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.1.0->spacy-transformers) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.1.0->spacy-transformers) (1.19.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.1.0->spacy-transformers) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.1.0->spacy-transformers) (8.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.1.0->spacy-transformers) (2.0.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.1.0->spacy-transformers) (0.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.1.0->spacy-transformers) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.1.0->spacy-transformers) (0.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.1.0->spacy-transformers) (21.0)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.1.0->spacy-transformers) (0.3.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.1.0->spacy-transformers) (4.62.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.1.0->spacy-transformers) (2.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.1.0->spacy-transformers) (0.4.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.1.0->spacy-transformers) (1.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.1.0->spacy-transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.1.0->spacy-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.1.0->spacy-transformers) (3.0.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy<4.0.0,>=3.1.0->spacy-transformers) (3.6.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<4.0.0,>=3.1.0->spacy-transformers) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<4.0.0,>=3.1.0->spacy-transformers) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.1.0->spacy-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.1.0->spacy-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.1.0->spacy-transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.1.0->spacy-transformers) (2.10)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers) (4.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers) (3.3.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 40.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 40.6 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 37.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<4.0.0,>=3.1.0->spacy-transformers) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4.0.0,>=3.1.0->spacy-transformers) (2.0.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.10.0,>=3.4.0->spacy-transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.10.0,>=3.4.0->spacy-transformers) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers, spacy-alignments, spacy-transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.0.19\n",
            "    Uninstalling huggingface-hub-0.0.19:\n",
            "      Successfully uninstalled huggingface-hub-0.0.19\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 1.12.1 requires huggingface-hub<0.1.0,>=0.0.14, but you have huggingface-hub 0.0.12 which is incompatible.\u001b[0m\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.46 spacy-alignments-0.8.3 spacy-transformers-1.0.6 tokenizers-0.10.3 transformers-4.9.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "huggingface_hub",
                  "yaml"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-aR7upn26qV"
      },
      "source": [
        "from IPython.display import Image\n",
        "from IPython.core.display import display, HTML\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import random\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "from collections import Counter\n",
        "from spacy.training import offsets_to_biluo_tags\n",
        "import spacy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from torch import LongTensor, FloatTensor\n",
        "from torch.nn import functional as F\n",
        "from typing import List, Dict, Tuple, Optional, Union\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMbmjRk1uuY7"
      },
      "source": [
        "Let's look at distribution of our data. Maybe we can deal with our problem by just using simple Neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmUSsRIfxY9B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7094ef95-37e1-4347-9cc0-5aeebc11b64e"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/snv-ds/NLP_course/master/week3/restauranttrain_updated.json"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-08 09:14:00--  https://raw.githubusercontent.com/snv-ds/NLP_course/master/week3/restauranttrain_updated.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15469921 (15M) [text/plain]\n",
            "Saving to: ‘restauranttrain_updated.json’\n",
            "\n",
            "restauranttrain_upd 100%[===================>]  14.75M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2021-10-08 09:14:01 (163 MB/s) - ‘restauranttrain_updated.json’ saved [15469921/15469921]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI5IdnZFcli6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "5240d4dd-ffd4-4242-aa6c-0526d01bdeb3"
      },
      "source": [
        "max_lens = list()\n",
        "for row in new_data:\n",
        "    max_lens.append(len(row))\n",
        "max_lens = pd.Series(max_lens)\n",
        "max_lens.plot();\n",
        "max_lens.describe()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-7b54d1b90383>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmax_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmax_lens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmax_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmax_lens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'new_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUXGJVCju8X_"
      },
      "source": [
        "As we can see, there are not so many long texts. And we can forecast all tokens at ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_GtmCDLXKPB"
      },
      "source": [
        "### FCNN for NER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jX73PIhur8i"
      },
      "source": [
        "For first approach we can just use basic FCNN. In production you will never see this, but for learning purpose it can be useful to explore."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1AzMdIwwK9K"
      },
      "source": [
        "As usual, we will write to fix words order in our vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgQnTm9oAaF5"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch import LongTensor, FloatTensor\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from typing import Any"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn1CcWyF4PkA"
      },
      "source": [
        "### to biluo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOqzHiTSbnot"
      },
      "source": [
        "This time we want to change our markup and train some models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09bbHjS_c9fZ"
      },
      "source": [
        "For whis purpose we will use spacy library. It contains built-in method that converts markup. But we need to correct it. That's why we wrote function that converts `BIO`-markup to `BILUO`-markup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fdftytA9tza"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URS5vbkg79mo"
      },
      "source": [
        "!python -m spacy init config base_config.cfg -p ner --force"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pOmC2XY9YU9"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")  # without vocabulary spacy can not work"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut4cXOBp2nom"
      },
      "source": [
        "with open('restauranttrain_updated.json', 'r') as f:\n",
        "    d = json.load(f)\n",
        "d[0]['paragraphs'][34]['sentences']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKP8sTSgdh8H"
      },
      "source": [
        "We will join our data, which is in list and then move it to spacy method "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu19Un5g9YKc"
      },
      "source": [
        "tokens_dict = d[0]['paragraphs'][34]['sentences'][0]['tokens']\n",
        "tokens = [i['orth'] for i in tokens_dict]\n",
        "\n",
        "text = ' '.join(tokens)\n",
        "doc = nlp(text)\n",
        "entities = d[0]['paragraphs'][34]['entities']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCZydwx3dyUt"
      },
      "source": [
        "For futher usage you can download and use this function in your work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9aV1nMi2ngA"
      },
      "source": [
        "from typing import List, Tuple, Union\n",
        "def convert_to_biluo(text: str = '',\n",
        "                     entities: List[Tuple] = None,\n",
        "                     tokens: list = None,\n",
        "                     missing: str = 'O') -> Tuple[Union[List[str], list, None], List[str]]:\n",
        "    \"\"\"\n",
        "    Tokenize text and return text tokens and ner labels.\n",
        "\n",
        "    Args:\n",
        "        text: text\n",
        "        entities: labels in spacy format\n",
        "        tokens: already tokenized text, if you want it\n",
        "        missing: lable for tokens without entities\n",
        "\n",
        "    Returns:\n",
        "        tokenized text and labels\n",
        "    \"\"\"\n",
        "\n",
        "    # create dicts with start/end position of token and its index\n",
        "    starts = []\n",
        "    ends = []\n",
        "    cur_index = 0\n",
        "    tokens = text.split() if tokens is None else tokens\n",
        "\n",
        "    for token in tokens:\n",
        "        starts.append(cur_index)\n",
        "        ends.append(cur_index + len(token))\n",
        "        cur_index += len(token) + 1\n",
        "\n",
        "    starts = {k: v for v, k in enumerate(starts)}\n",
        "    ends = {k: v for v, k in enumerate(ends)}\n",
        "\n",
        "    # this will be a list with token labels\n",
        "    biluo = [\"-\" for _ in text.split()]\n",
        "\n",
        "    # check that there are no overlapping entities\n",
        "    entities_indexes = [list(range(i[0], i[1])) for i in entities]\n",
        "    if max(Counter([i for j in entities_indexes for i in j]).values()) > 1:\n",
        "        raise ValueError('You have overlapping entities')\n",
        "\n",
        "    tokens_in_ents = {}\n",
        "\n",
        "    # Handle entity cases\n",
        "    for start_char, end_char, label in entities:\n",
        "        for token_index in range(start_char, end_char):\n",
        "            tokens_in_ents[token_index] = (start_char, end_char, label)\n",
        "        start_token = starts.get(start_char)\n",
        "        end_token = ends.get(end_char)\n",
        "        # Only interested if the tokenization is correct\n",
        "        if start_token is not None and end_token is not None:\n",
        "            if start_token == end_token:\n",
        "                biluo[start_token] = f\"U-{label}\"\n",
        "            else:\n",
        "                biluo[start_token] = f\"B-{label}\"\n",
        "                for i in range(start_token + 1, end_token):\n",
        "                    biluo[i] = f\"I-{label}\"\n",
        "                biluo[end_token] = f\"L-{label}\"\n",
        "\n",
        "    # put missing value for tokens without labels\n",
        "    entity_chars = set()\n",
        "    for start_char, end_char, label in entities:\n",
        "        for i in range(start_char, end_char):\n",
        "            entity_chars.add(i)\n",
        "\n",
        "    for ind, token in enumerate(tokens):\n",
        "        for i in range(list(starts.keys())[ind], list(ends.keys())[ind]):\n",
        "            if i in entity_chars:\n",
        "                break\n",
        "        else:\n",
        "            biluo[ind] = missing\n",
        "\n",
        "    return tokens, biluo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDTezl5X8zo7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "outputId": "e2afc73a-a356-476c-d6b2-d83114c4a53a"
      },
      "source": [
        "# convert the data\n",
        "%%time\n",
        "new_data = []\n",
        "biluo_labels = []\n",
        "for i in range(len(d[0]['paragraphs'])):\n",
        "    tokens_dict = d[0]['paragraphs'][i]['sentences'][0]['tokens']\n",
        "    tokens = [i['orth'] for i in tokens_dict]\n",
        "    if len([i['orth'] for i in tokens_dict]) > 1:\n",
        "        \n",
        "        text = ' '.join(tokens)\n",
        "        doc = nlp(text)\n",
        "        entities = d[0]['paragraphs'][i]['entities']\n",
        "\n",
        "        new_ents = offsets_to_biluo_tags(doc, entities)  # using spacy function\n",
        "        if entities == []:\n",
        "            new_ents = ['O'] * len(tokens)\n",
        "        new_data.append(tokens)\n",
        "        \n",
        "        biluo_labels.append(new_ents)\n",
        "        if len(tokens) != len(new_ents): # if lists from 2 methods don't match\n",
        "            \n",
        "            ents2 = convert_to_biluo(text, entities)[1]\n",
        "            biluo_labels[-1] = ents2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-80ff4b172868>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"new_data = []\\nbiluo_labels = []\\nfor i in range(len(d[0]['paragraphs'])):\\n    tokens_dict = d[0]['paragraphs'][i]['sentences'][0]['tokens']\\n    tokens = [i['orth'] for i in tokens_dict]\\n    if len([i['orth'] for i in tokens_dict]) > 1:\\n        \\n        text = ' '.join(tokens)\\n        doc = nlp(text)\\n        entities = d[0]['paragraphs'][i]['entities']\\n\\n        new_ents = offsets_to_biluo_tags(doc, entities)  # using spacy function\\n        if entities == []:\\n            new_ents = ['O'] * len(tokens)\\n        new_data.append(tokens)\\n        \\n        biluo_labels.append(new_ents)\\n        if len(tokens) != len(new_ents): # if lists from 2 methods don't match\\n            \\n            ents2 = convert_to_biluo(text, entities)[1]\\n            biluo_labels[-1] = ents2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ttzrWaGF_Qt"
      },
      "source": [
        "biluo_labels[0], new_data[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF_88li06w_A"
      },
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "from tqdm.notebook import tqdm\n",
        "import joblib\n",
        "from typing import List, Tuple, Union, Dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tRh7QQLHV_H"
      },
      "source": [
        "#### Task 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coZ8RhAfeXCp"
      },
      "source": [
        "create to variables, that will contains mappings between entities and indices. Each dictionary must include entities: `O` and `PAD`.\n",
        "Initialize variable `tag_to_idx`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EavWQqK-aVf"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "tags = sorted(list({i for j in biluo_labels for i in j}))\n",
        "\n",
        "tag_to_idx = {}\n",
        "\n",
        "with open('mapping.json', 'w') as f:\n",
        "  json.dump(tag_to_idx, f)\n",
        "\n",
        "idx_to_tag = {second: first for first, second in tag_to_idx.items()}\n",
        "\n",
        "tag_to_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ITsQT-SIWB5"
      },
      "source": [
        "def get_word_to_idx(count: List[Tuple[str, int]],\n",
        "                   min_words: Union[int, float] = 0.0,\n",
        "                   max_words: Union[int, float] = 1.0) -> Dict[str, int]:\n",
        "    max_count = count[0][1]\n",
        "    if isinstance(min_words, float):\n",
        "        min_words = max_count * min_words\n",
        "    if isinstance(max_words, float):\n",
        "        max_words = max_count * max_words\n",
        "    \n",
        "    all_words = [w[0] for w in count if max_words >= w[1] >= min_words]\n",
        "    \n",
        "    all_words = ['<pad>', '<unk>'] + all_words\n",
        "    \n",
        "    word_to_idx = {k: v for k, v in zip(all_words, range(0, len(all_words)))}\n",
        "    return word_to_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsMgDEWoeaEC"
      },
      "source": [
        "#### Task 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF2wHVhkeYtW"
      },
      "source": [
        "Count how many unique words are there in our train dataset. Parameters `min_words` and `max_words` should be initialized as default values. Initialize variable `word_to_idx` from method `get_word_to_idx`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKD4GgGpfybD"
      },
      "source": [
        "##### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J9Bf3xof0eB"
      },
      "source": [
        "# 1. first you can count occurences of each word\n",
        "# 2. second you can pass list of tuples for each pair (word, num_of_occurencies) to function get_word_to_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGSXKAasgIf6"
      },
      "source": [
        "##### Continue work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE62qt8k-ad-"
      },
      "source": [
        "count = Counter()\n",
        "word_to_idx = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-EIfsqqfNtZ"
      },
      "source": [
        "assert len(word_to_idx) == 3805"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7mJ29v2IV1-"
      },
      "source": [
        "def create_matrix_of_texts(dataset, max_sequence_length, \n",
        "                           pad_token, word2index):\n",
        "    texts = np.full((len(dataset), max_sequence_length),\n",
        "                    word2index[pad_token], dtype=np.int64)  # creating empty matrix\n",
        "\n",
        "    for ind, row in enumerate(dataset):\n",
        "          trim_length = min(max_sequence_length, len(row))\n",
        "          text = row[:trim_length]\n",
        "          texts[ind, :trim_length] = [word2index[item.lower()] for item in text]\n",
        "    return texts\n",
        "\n",
        "def create_matrix_of_tags(dataset, max_sequence_length, pad_index, tag2idx):\n",
        "    tags = np.full((len(dataset), max_sequence_length),\n",
        "                    pad_index, dtype=np.int64)  # creating empty matrix\n",
        "\n",
        "    for ind, row in enumerate(dataset):\n",
        "          trim_length = min(max_sequence_length, len(row))\n",
        "          labels = row[: trim_length]\n",
        "          tags[ind, : trim_length] = [tag2idx[item] for item in labels]\n",
        "    return tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3qdl6XJHq6z"
      },
      "source": [
        "texts = create_matrix_of_texts(new_data, \n",
        "                               int(max_lens.quantile(0.97)),\n",
        "                               '<pad>', word_to_idx)\n",
        "tags = create_matrix_of_tags(biluo_labels,\n",
        "                             int(max_lens.quantile(0.97)),\n",
        "                             tag_to_idx['PAD'],\n",
        "                             tag_to_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69okXnDhKpYW"
      },
      "source": [
        "class NerDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 texts: np.array,\n",
        "                 tags: np.array):\n",
        "        self.tags = tags\n",
        "        self.texts = texts\n",
        "        \n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
        "        tokens_tensor = torch.tensor(self.texts[idx], dtype=torch.int64)\n",
        "        return tokens_tensor, torch.tensor(self.tags[idx], dtype=torch.int64)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        dataset_len = self.texts.shape[0]\n",
        "        return dataset_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCe1keAYKpMb"
      },
      "source": [
        "ner_dataset = NerDataset(texts, tags)\n",
        "assert len(ner_dataset) == 7634"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG6oV-zTKo_e"
      },
      "source": [
        "from torch.utils.data.dataset import random_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXx7nlGeGo-R"
      },
      "source": [
        "BATCH_SIZE = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGrEjzG4gadC"
      },
      "source": [
        "#### Task 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8MniGMFgdlG"
      },
      "source": [
        "Initialize dataloaders for train and validation. There is no need to shuffle validation dataloader, but it is better to shuffle train and drop last batch from train dataloader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehjV96FzLItx"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "num_train = int(len(ner_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(ner_dataset, [num_train, len(ner_dataset) - num_train])\n",
        "\n",
        "train_dataloader = pass\n",
        "valid_dataloader = pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl8EJ-bzgz7t"
      },
      "source": [
        "In this toy example we will first try simple FCNN for your problem. Let's look how bad/good it fits our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWLYR4MFhWDB"
      },
      "source": [
        "#### Task 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10wgebnyhYiL"
      },
      "source": [
        "Initialize sequantial layers of our FCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ngc4mz_P7a5"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "class NerModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        word2idx: Dict,\n",
        "        embedding_dim: int = 100,\n",
        "        mapping: Dict[int, str] = None,\n",
        "        hidden_size: int = 256,\n",
        "    ):\n",
        "        super(NerModel, self).__init__()\n",
        "        if not mapping:\n",
        "            raise RuntimeError(f'Empty labels')\n",
        "        self.word2idx = word2idx\n",
        "        self.labels = mapping\n",
        "\n",
        "        self.linear_sigmoid_stack = nn.Sequential(\n",
        "            # FILL YOUR CODE HERE\n",
        "        )\n",
        "\n",
        "    def forward(self, tokens: LongTensor) -> FloatTensor:\n",
        "\n",
        "        return self.linear_sigmoid_stack(tokens).view(-1, len(self.labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il0TbOeVhkZC"
      },
      "source": [
        "Now we will create basic network and check how it calculate loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyw63e8YP_ko"
      },
      "source": [
        "model = NerModel(word_to_idx, 100, {idx: str(idx) for idx in range(10)})\n",
        "assert (\n",
        "        len(list(name for name, module in model.named_modules())) > 3\n",
        "    ), \"Not enough layers created\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QktELcGQEIw"
      },
      "source": [
        "num_classes = len(tag_to_idx)\n",
        "model = NerModel(word_to_idx, 30, {idx: str(idx) for idx in range(num_classes)})\n",
        "seq_len = 32\n",
        "example_input = torch.randint(0, 2, (BATCH_SIZE, seq_len), dtype=torch.int64)\n",
        "logits = model(example_input)\n",
        "assert isinstance(logits, torch.FloatTensor)\n",
        "assert logits.shape == (BATCH_SIZE * seq_len, num_classes), f\"current size of model output {logits.shape}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wsrqGA7hz1O"
      },
      "source": [
        "i = iter(train_dataloader)\n",
        "text, label = next(i)\n",
        "logits = model(text)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "loss_function(logits, label.view(-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaI3WuNFh3mK"
      },
      "source": [
        "Everything looks pretty well and seems correct. Lets now write evaluation function and begin our training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rENMRVxiJdR"
      },
      "source": [
        "#### Task 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6VN-xHQiNNY"
      },
      "source": [
        "Fill lines of code. First you need to initialize variable of `correct_labels` (labels, that are not special ones). Then you need to get `true_labels` and `predicted` variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rXHLysLiL3c"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "    correct_labels = None  # Fill your code here\n",
        "    predicted, true_labels = list(), list()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(dataloader):\n",
        "            tokens, label = batch\n",
        "            tokens = tokens.to(device)\n",
        "            \n",
        "            logits = model(tokens)\n",
        "            predictions = F.log_softmax(logits, dim=1).reshape(-1,\n",
        "                                                               int(max_lens.quantile(0.97)),\n",
        "                                                               len(tag_to_idx)).argmax(dim=2).flatten().detach().cpu().numpy()\n",
        "            predicted.extend(predictions)\n",
        "            true_labels.extend(label.flatten().detach().cpu().numpy())\n",
        "    \n",
        "    true_labels = None # Fill your code here\n",
        "    \n",
        "    predicted = None  # Fill your code here\n",
        "    print('\\n', classification_report(true_labels,\n",
        "                                      predicted,\n",
        "                                      labels=correct_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKo2r1WYizlt"
      },
      "source": [
        "Now we can create our model and start trainig"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ULTTPvri4YI"
      },
      "source": [
        "model = NerModel(word_to_idx, 300, tag_to_idx)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXa9nIHGi5uV"
      },
      "source": [
        "for e in range(6):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    for sent in tqdm(train_dataloader):\n",
        "\n",
        "            # (1) Set gradient to zero for new example: Set gradients to zero before pass\n",
        "            model.zero_grad()\n",
        "            \n",
        "            # (2) Encode sentence and tag sequence as sequences of indices\n",
        "            input_sent, gold_tags = sent\n",
        "\n",
        "            # (3) Predict tags (sentence by sentence)\n",
        "            if len(input_sent) > 0:\n",
        "                pred_scores = model(input_sent.to(device))\n",
        "                mask = gold_tags != 0\n",
        "                # (4) Compute loss and do backward step\n",
        "                loss = loss_function(pred_scores.to(device), gold_tags.view(-1).to(device))\n",
        "                loss.backward()\n",
        "              \n",
        "                # (5) Optimize parameter values\n",
        "                optimizer.step()\n",
        "          \n",
        "                # (6) Accumulate loss\n",
        "                total_loss += loss\n",
        "    print('\\nEpoch: %d, loss: %.4f' % (e, total_loss / len(train_dataloader)))\n",
        "    evaluate(valid_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sshz2tYrjBy9"
      },
      "source": [
        "We did it, but quality of model is rather bad. Now you can try RNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MxVkYxXjK8c"
      },
      "source": [
        "### RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-pn2aRRjS_D"
      },
      "source": [
        "All process from FCNN works fine, but we need to use new architecture. Let's write new model, that process data and uses some kind of Recurrent Neural Network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrEZNuAjjniz"
      },
      "source": [
        "#### Task 11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd6h4WsUjpzB"
      },
      "source": [
        "Fill missing layers of model. You can use any RNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhyuQhs1LIiI"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "class NerRNNModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        word2idx: Dict,\n",
        "        embedding_dim: int = 100,\n",
        "        mapping: Dict[int, str] = None,\n",
        "        hidden_size: int = 256\n",
        "    ):\n",
        "        super(NerRNNModel, self).__init__()\n",
        "        if not mapping:\n",
        "            raise RuntimeError(f'Empty labels')\n",
        "        self.word2idx = word2idx\n",
        "        self.labels = mapping\n",
        "        self.embedding = nn.Embedding(len(word_to_idx), embedding_dim)\n",
        "        self.encoder = nn.RNN(\n",
        "            # FILL YOUR CODE HERE\n",
        "        )\n",
        "        self.projection = nn.Linear(hidden_size, len(mapping))\n",
        "\n",
        "    def forward(self, tokens: LongTensor) -> FloatTensor:\n",
        "\n",
        "        emb = self.embedding(tokens)        \n",
        "        h, _ = self.encoder(emb)\n",
        "        pred = self.projection(h)\n",
        "        return pred.view(-1, len(self.labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHr5vn8Xj99l"
      },
      "source": [
        "Now we can duplicate all cells from above and simply just start new iteration of training new model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imiTzbYgLQEx"
      },
      "source": [
        "model = NerRNNModel(word_to_idx, 100, {idx: str(idx) for idx in range(10)})\n",
        "assert (\n",
        "        len(list(name for name, module in model.named_modules())) > 3\n",
        "    ), \"Not enough layers created\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM-Ti_5cLP3V"
      },
      "source": [
        "num_classes = len(tag_to_idx)\n",
        "model = NerRNNModel(word_to_idx, 30, {idx: str(idx) for idx in range(num_classes)})\n",
        "seq_len = 32\n",
        "example_input = torch.randint(0, 2, (BATCH_SIZE, seq_len), dtype=torch.int64)\n",
        "logits = model(example_input)\n",
        "assert isinstance(logits, torch.FloatTensor)\n",
        "assert logits.shape == (BATCH_SIZE * seq_len, num_classes), f\"current size of model output {logits.shape}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lDFmLHDLPox"
      },
      "source": [
        "i = iter(train_dataloader)\n",
        "text, label = next(i)\n",
        "logits = model(text)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "loss_function(logits, label.view(-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCaivj5uLiHA"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "    correct_labels = [value for value in idx_to_tag.values() if value != 'O' and value != 'PAD']\n",
        "    predicted, true_labels = list(), list()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(dataloader):\n",
        "            tokens, label = batch\n",
        "            tokens = tokens.to(device)\n",
        "            \n",
        "            logits = model(tokens)\n",
        "            predictions = F.log_softmax(logits, dim=1).reshape(-1,\n",
        "                                                               int(max_lens.quantile(0.97)),\n",
        "                                                               len(tag_to_idx)).argmax(dim=2).flatten().detach().cpu().numpy()\n",
        "            predicted.extend(predictions)\n",
        "            true_labels.extend(label.flatten().detach().cpu().numpy())\n",
        "    \n",
        "    true_labels = [idx_to_tag[val] for val in true_labels]\n",
        "    \n",
        "    predicted = [idx_to_tag[val] for val in predicted]\n",
        "    print('\\n', classification_report(true_labels,\n",
        "                                      predicted,\n",
        "                                      labels=correct_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LtiUtB5Lh5R"
      },
      "source": [
        "model = NerRNNModel(word_to_idx, 300, tag_to_idx)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spM7aX3ELhsC"
      },
      "source": [
        "sum([params.numel() for params in model.parameters() if params.requires_grad])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRf8qs1fLIVY"
      },
      "source": [
        "for e in range(6):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    for sent in tqdm(train_dataloader):\n",
        "\n",
        "            # (1) Set gradient to zero for new example: Set gradients to zero before pass\n",
        "            model.zero_grad()\n",
        "            \n",
        "            # (2) Encode sentence and tag sequence as sequences of indices\n",
        "            input_sent, gold_tags = sent\n",
        "\n",
        "            # (3) Predict tags (sentence by sentence)\n",
        "            if len(input_sent) > 0:\n",
        "                pred_scores = model(input_sent.to(device))\n",
        "                mask = gold_tags != 0\n",
        "                # (4) Compute loss and do backward step\n",
        "                loss = loss_function(pred_scores.to(device), gold_tags.view(-1).to(device))\n",
        "                loss.backward()\n",
        "              \n",
        "                # (5) Optimize parameter values\n",
        "                optimizer.step()\n",
        "          \n",
        "                # (6) Accumulate loss\n",
        "                total_loss += loss\n",
        "    print('\\nEpoch: %d, loss: %.4f' % (e, total_loss / len(train_dataloader)))\n",
        "    evaluate(valid_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkBKi5YtlpsT"
      },
      "source": [
        "Futher working:\n",
        "- Try more complex architecture\n",
        "- try bidirectional rnns\n",
        "- try other hyperparameters\n",
        "- try pretrained embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02pZb8CdmXNB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}