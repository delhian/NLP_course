{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seminar_3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_zLNARu9cYDv",
        "yKD4GgGpfybD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snv-ds/NLP_course/blob/master/week3/seminar_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFIUUYWtVMRB"
      },
      "source": [
        "# Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C6hPcSsPF42"
      },
      "source": [
        "In this lecture we will get insight into very popular NLP task - Named Entity Recognition.<br>Our goal is to:\n",
        "- build a good baseline solution\n",
        "- modify the data markup\n",
        "- learn how to solve this problem using neural network methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iilmDaCFVOXX"
      },
      "source": [
        "In first part we will explore how to get fast solution of this task, how to exlore metrics and how to convert labeling.<br>\n",
        "In the second part we will look how we can solve this task by using different architectures and measure them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsLFL7s0VRsK"
      },
      "source": [
        "What we will learn:\n",
        "- non neural approaches for NER-task;\n",
        "- measure quality of model for NER-task;\n",
        "- different markup for NER-task;\n",
        "- data preparation for neural network solution of NER;\n",
        "- using different neural approaches for NER;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIKjsBqIVU2x"
      },
      "source": [
        "# Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rrym_4yiVWok"
      },
      "source": [
        "## Solving NER task without Neural netowrks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdYixEjkRfrC"
      },
      "source": [
        "!pip install datasets > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI6yaeHZRjbc"
      },
      "source": [
        "import pytest\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import LongTensor, FloatTensor\n",
        "from torch.nn import functional as F\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from torch.utils.data import Dataset\n",
        "from torch.optim import Adam\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilQEoIuGW6YB"
      },
      "source": [
        "### look at the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoASj4VoRrqN"
      },
      "source": [
        "For this task we will use common NER-dataset which is always included in all benchmarks, when scientists measure quality of SOTA solutions for NER.<br>\n",
        "The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on four types of named entities: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzSRKAzLRpT_"
      },
      "source": [
        "dataset_base = load_dataset(\"conll2003\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju3RL22uR_Tl"
      },
      "source": [
        "dataset_base['train']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SejZA2vpSmUP"
      },
      "source": [
        "import json\n",
        "mapping_ = {v: k for k, v in dataset_base[\"train\"].features[\"ner_tags\"].feature._str2int.items()}\n",
        "\n",
        "with open('mapping.json', 'w') as f:\n",
        "  json.dump(mapping_, f)\n",
        "mapping_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3dkmzC6TGh9"
      },
      "source": [
        "for i in range(10):\n",
        "  print(i + 1, ' '.join(dataset_base[\"train\"]['tokens'][i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f1pzbaOJjB2"
      },
      "source": [
        "#### Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bujckdrmJl7d"
      },
      "source": [
        "Count occurence of each entity. Print number of occurences for each entity. Result must be a dictinary, where keys are entities from `dataset_base[\"train\"]['ner_tags']` and values are total number of occurencies for each key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmpBc17GTro1"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "counter = None\n",
        "counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSwUL6qYnIjU"
      },
      "source": [
        "assert len(counter) == 9\n",
        "assert counter['O'] > 169000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOC2dHK0X9SL"
      },
      "source": [
        "As you see, we have dominating number of class `O`. Our main goal is to make such model, that will not overfit to predict always `O` token.<br>\n",
        "What metrics are more appropriate to measure quality of models for NER?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA5g7XGEJTya"
      },
      "source": [
        "### Sklearn-crf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pwR8trFYXmr"
      },
      "source": [
        "Now I'd like to introduce you great library, that can provide light and easy implementation for solving NER-task. It's name is `sklearn-crf`. It has familiar interface to basic sklearn, but is based on very powerful tool for NER-task - CRF(Conditional Random Field). <br>\n",
        "CRF is nowdays the de facto standard for solving the NLP problem. Even in the most modern SOTA neural networks approaches, a CRF layer can now often be seen as an output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Uco_i3lZgUd"
      },
      "source": [
        "!pip install sklearn_crfsuite > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcTxcqd6Zhb9"
      },
      "source": [
        "import sklearn\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1-YPeBkZqJa"
      },
      "source": [
        "As all sklearn-like libraries we need to get pandas.DataFrame as an input for this model. Let's create it.<br>\n",
        "In our DataFrame we will make each word, entity and sentence_id on each row."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htp-v1JRZmm-"
      },
      "source": [
        "df = pd.DataFrame({'sent_id': [i for j in [[i] * len(s['tokens']) for i, s in enumerate(dataset_base['train'])] for i in j],\n",
        "                   'data': [i for j in dataset_base['train'] for i in j['tokens']],\n",
        "                   'entities': [mapping_[i] for j in dataset_base['train'] for i in j['ner_tags']]})\n",
        "df.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeY96hlsZ-YT"
      },
      "source": [
        "Now we have dataframe, where only 3 columns exsists:\n",
        " - sentense_id - which mark each word belonging to each sentence\n",
        " - data contains words on each row\n",
        " - entities marks which entity does each word refer to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WImNIQKwbHYs"
      },
      "source": [
        "We also need a class, that will process each sentence and aggregate words and entities in it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAcrp66Qa7mY"
      },
      "source": [
        "class SentenceGetter(object):\n",
        "    \n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, t) for w, t in zip(s['data'].values.tolist(), \n",
        "                                                     s['entities'].values.tolist())]\n",
        "        self.grouped = self.data.groupby('sent_id').apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "        \n",
        "    def get_next(self):\n",
        "        try: \n",
        "            s = self.grouped['Sentence: {}'.format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s \n",
        "        except:\n",
        "            return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qa-NFSnKbDEd"
      },
      "source": [
        "getter = SentenceGetter(df)\n",
        "sentences = getter.sentences\n",
        "sentences[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQsEoGBsbXwl"
      },
      "source": [
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "    \n",
        "    features = {\n",
        "        'bias': 1.0, \n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit()\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        features.update({\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper()\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        features.update({\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper()\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for token, label in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [token for token, label in sent]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txY55dbQb1DH"
      },
      "source": [
        "X = [sent2features(s) for s in sentences]\n",
        "y = [sent2labels(s) for s in sentences]\n",
        "len(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMlf5JJrb7WW"
      },
      "source": [
        "X_train = X[:10000]\n",
        "X_test = X[10000:]\n",
        "y_train = y[:10000]\n",
        "y_test = y[10000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOjIhfYcb-CK"
      },
      "source": [
        "%%time\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True,\n",
        "    verbose=True\n",
        ")\n",
        "crf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h064girfcGdq"
      },
      "source": [
        "all_entities = sorted(df.entities.unique().tolist())\n",
        "y_pred = crf.predict(X_test)\n",
        "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=all_entities)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOg9TIVOKOKD"
      },
      "source": [
        "#### Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF4g_dZmKP0p"
      },
      "source": [
        "Print classification report for all useful tokens (exluding token `O`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myDSv6zPcM4R"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "print(metrics.flat_classification_report()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFR46OtaJzsj"
      },
      "source": [
        "#### Task 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVnO-hahJ1di"
      },
      "source": [
        "Make some additional features to reach at least 0.82 weighted f1-score on detection all useful tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zLNARu9cYDv"
      },
      "source": [
        "##### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXaPZQVMcU7V"
      },
      "source": [
        "# 1. You can check for lower() each word\n",
        "# 2. You can add more words to features, for example last 3 words words[-3:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXHti7ticZYU"
      },
      "source": [
        "##### continue work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyPIr9VEdbfs"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "    \n",
        "    features = {\n",
        "        'bias': 1.0, \n",
        "        # add some here\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit()\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        features.update({\n",
        "            # add something here\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper()\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        features.update({\n",
        "            # add something here\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper()\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for token, label in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [token for token, label in sent]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSnRyXFrdfJM"
      },
      "source": [
        "# explore quality for your new features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqIEo9oBJXDz"
      },
      "source": [
        "### Converting markup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBLwmVHJKsYr"
      },
      "source": [
        "Now it's time to get acquainted to NER markup or NER data labeling.<br>\n",
        "When we work with almost every NLP task, we usually need our data to be labeled. For NER problem data labeling is often rather expensive. Often we ask to label just in text, and then simple label all tokens for `BIO`-markup.<br>\n",
        "But in some tasks in which we need to very accurately define separate entities, the `BILUO`-markup may come to the rescue.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe-STiqnkmv0"
      },
      "source": [
        "In our dataset we have `BIO-markup."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zmuw8OzKe8m"
      },
      "source": [
        "#### Task 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpmHaJKgKgvR"
      },
      "source": [
        "write function to convert `BIO`-markup into `BILUO`-markup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4Z_T1nDk9l5"
      },
      "source": [
        "entities_list = [[mapping_[token] for token in tokens] for tokens in dataset_base[\"train\"]['ner_tags']]\n",
        "# entities_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qql_NTIpktve"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "def bio_2_biluo(entities_list, missing: str = 'O'):\n",
        "  result = list()\n",
        "  for entities in entities_list:\n",
        "    current_new_markup = [entities[0]]\n",
        "    # FILL code here\n",
        "    result.append(current_new_markup)\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqSINs4MpNsk"
      },
      "source": [
        "assert len(bio_2_biluo(entities_list)) == len(entities_list)\n",
        "assert set(bio_2_biluo([entities_list[1]])[0]) == {'B-PER', 'L-PER'}\n",
        "assert len(set(bio_2_biluo([entities_list[7]])[0])) == 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZBBeLSdtlJZ"
      },
      "source": [
        "Sometimes after markup we have data labeled in offets: in plain text we get beginning and ending of each entity.<br>\n",
        "In this situations we can use function from spacy named `offsets_to_biluo_tags`. But you need to be careful, because sometimes it works incorrect. In this case you need to check translation of markup or write your own function to translate markups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnOZWlYhJcHF"
      },
      "source": [
        "Future readings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq7AOxcNVLDh"
      },
      "source": [
        "# you can also try to use spacy built-in ner model from spacy python library. Example of usage is here -> https://spacy.io/api/cli"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mqTd1lnVLvQ"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZVLX9gSuNCh"
      },
      "source": [
        "In this part we will try to use some basic approaches to solve NER-task. Dataset will be the same as above. In this part don't forget to change runtime of your notebook to `GPU`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5gnC8Fn6_D_"
      },
      "source": [
        "!pip install spacy==3.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLzibq6p8IMp"
      },
      "source": [
        "!pip install spacy-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-aR7upn26qV"
      },
      "source": [
        "from IPython.display import Image\n",
        "from IPython.core.display import display, HTML\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import random\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "from collections import Counter\n",
        "from spacy.training import offsets_to_biluo_tags\n",
        "import spacy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from torch import LongTensor, FloatTensor\n",
        "from torch.nn import functional as F\n",
        "from typing import List, Dict, Tuple, Optional, Union\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMbmjRk1uuY7"
      },
      "source": [
        "Let's look at distribution of our data. Maybe we can deal with our problem by just using simple Neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmUSsRIfxY9B"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/snv-ds/NLP_course/master/week3/restauranttrain_updated.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI5IdnZFcli6"
      },
      "source": [
        "max_lens = list()\n",
        "for row in new_data:\n",
        "    max_lens.append(len(row))\n",
        "max_lens = pd.Series(max_lens)\n",
        "max_lens.plot();\n",
        "max_lens.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUXGJVCju8X_"
      },
      "source": [
        "As we can see, there are not so many long texts. And we can forecast all tokens at ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_GtmCDLXKPB"
      },
      "source": [
        "### FCNN for NER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jX73PIhur8i"
      },
      "source": [
        "For first approach we can just use basic FCNN. In production you will never see this, but for learning purpose it can be useful to explore."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1AzMdIwwK9K"
      },
      "source": [
        "As usual, we will write to fix words order in our vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgQnTm9oAaF5"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch import LongTensor, FloatTensor\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from typing import Any"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn1CcWyF4PkA"
      },
      "source": [
        "### to biluo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOqzHiTSbnot"
      },
      "source": [
        "This time we want to change our markup and train some models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09bbHjS_c9fZ"
      },
      "source": [
        "For whis purpose we will use spacy library. It contains built-in method that converts markup. But we need to correct it. That's why we wrote function that converts `BIO`-markup to `BILUO`-markup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fdftytA9tza"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URS5vbkg79mo"
      },
      "source": [
        "!python -m spacy init config base_config.cfg -p ner --force"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pOmC2XY9YU9"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")  # without vocabulary spacy can not work"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut4cXOBp2nom"
      },
      "source": [
        "with open('restauranttrain_updated.json', 'r') as f:\n",
        "    d = json.load(f)\n",
        "d[0]['paragraphs'][34]['sentences']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKP8sTSgdh8H"
      },
      "source": [
        "We will join our data, which is in list and then move it to spacy method "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu19Un5g9YKc"
      },
      "source": [
        "tokens_dict = d[0]['paragraphs'][34]['sentences'][0]['tokens']\n",
        "tokens = [i['orth'] for i in tokens_dict]\n",
        "\n",
        "text = ' '.join(tokens)\n",
        "doc = nlp(text)\n",
        "entities = d[0]['paragraphs'][34]['entities']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCZydwx3dyUt"
      },
      "source": [
        "For futher usage you can download and use this function in your work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9aV1nMi2ngA"
      },
      "source": [
        "from typing import List, Tuple, Union\n",
        "def convert_to_biluo(text: str = '',\n",
        "                     entities: List[Tuple] = None,\n",
        "                     tokens: list = None,\n",
        "                     missing: str = 'O') -> Tuple[Union[List[str], list, None], List[str]]:\n",
        "    \"\"\"\n",
        "    Tokenize text and return text tokens and ner labels.\n",
        "\n",
        "    Args:\n",
        "        text: text\n",
        "        entities: labels in spacy format\n",
        "        tokens: already tokenized text, if you want it\n",
        "        missing: lable for tokens without entities\n",
        "\n",
        "    Returns:\n",
        "        tokenized text and labels\n",
        "    \"\"\"\n",
        "\n",
        "    # create dicts with start/end position of token and its index\n",
        "    starts = []\n",
        "    ends = []\n",
        "    cur_index = 0\n",
        "    tokens = text.split() if tokens is None else tokens\n",
        "\n",
        "    for token in tokens:\n",
        "        starts.append(cur_index)\n",
        "        ends.append(cur_index + len(token))\n",
        "        cur_index += len(token) + 1\n",
        "\n",
        "    starts = {k: v for v, k in enumerate(starts)}\n",
        "    ends = {k: v for v, k in enumerate(ends)}\n",
        "\n",
        "    # this will be a list with token labels\n",
        "    biluo = [\"-\" for _ in text.split()]\n",
        "\n",
        "    # check that there are no overlapping entities\n",
        "    entities_indexes = [list(range(i[0], i[1])) for i in entities]\n",
        "    if max(Counter([i for j in entities_indexes for i in j]).values()) > 1:\n",
        "        raise ValueError('You have overlapping entities')\n",
        "\n",
        "    tokens_in_ents = {}\n",
        "\n",
        "    # Handle entity cases\n",
        "    for start_char, end_char, label in entities:\n",
        "        for token_index in range(start_char, end_char):\n",
        "            tokens_in_ents[token_index] = (start_char, end_char, label)\n",
        "        start_token = starts.get(start_char)\n",
        "        end_token = ends.get(end_char)\n",
        "        # Only interested if the tokenization is correct\n",
        "        if start_token is not None and end_token is not None:\n",
        "            if start_token == end_token:\n",
        "                biluo[start_token] = f\"U-{label}\"\n",
        "            else:\n",
        "                biluo[start_token] = f\"B-{label}\"\n",
        "                for i in range(start_token + 1, end_token):\n",
        "                    biluo[i] = f\"I-{label}\"\n",
        "                biluo[end_token] = f\"L-{label}\"\n",
        "\n",
        "    # put missing value for tokens without labels\n",
        "    entity_chars = set()\n",
        "    for start_char, end_char, label in entities:\n",
        "        for i in range(start_char, end_char):\n",
        "            entity_chars.add(i)\n",
        "\n",
        "    for ind, token in enumerate(tokens):\n",
        "        for i in range(list(starts.keys())[ind], list(ends.keys())[ind]):\n",
        "            if i in entity_chars:\n",
        "                break\n",
        "        else:\n",
        "            biluo[ind] = missing\n",
        "\n",
        "    return tokens, biluo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDTezl5X8zo7"
      },
      "source": [
        "# convert the data\n",
        "%%time\n",
        "new_data = []\n",
        "biluo_labels = []\n",
        "for i in range(len(d[0]['paragraphs'])):\n",
        "    tokens_dict = d[0]['paragraphs'][i]['sentences'][0]['tokens']\n",
        "    tokens = [i['orth'] for i in tokens_dict]\n",
        "    if len([i['orth'] for i in tokens_dict]) > 1:\n",
        "        \n",
        "        text = ' '.join(tokens)\n",
        "        doc = nlp(text)\n",
        "        entities = d[0]['paragraphs'][i]['entities']\n",
        "\n",
        "        new_ents = offsets_to_biluo_tags(doc, entities)  # using spacy function\n",
        "        if entities == []:\n",
        "            new_ents = ['O'] * len(tokens)\n",
        "        new_data.append(tokens)\n",
        "        \n",
        "        biluo_labels.append(new_ents)\n",
        "        if len(tokens) != len(new_ents): # if lists from 2 methods don't match\n",
        "            \n",
        "            ents2 = convert_to_biluo(text, entities)[1]\n",
        "            biluo_labels[-1] = ents2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ttzrWaGF_Qt"
      },
      "source": [
        "biluo_labels[0], new_data[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF_88li06w_A"
      },
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "from tqdm.notebook import tqdm\n",
        "import joblib\n",
        "from typing import List, Tuple, Union, Dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tRh7QQLHV_H"
      },
      "source": [
        "#### Task 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coZ8RhAfeXCp"
      },
      "source": [
        "create to variables, that will contains mappings between entities and indices. Each dictionary must include entities: `O` and `PAD`.\n",
        "Initialize variable `tag_to_idx`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EavWQqK-aVf"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "tags = sorted(list({i for j in biluo_labels for i in j}))\n",
        "\n",
        "tag_to_idx = {}\n",
        "\n",
        "with open('mapping.json', 'w') as f:\n",
        "  json.dump(tag_to_idx, f)\n",
        "\n",
        "idx_to_tag = {second: first for first, second in tag_to_idx.items()}\n",
        "\n",
        "tag_to_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ITsQT-SIWB5"
      },
      "source": [
        "def get_word_to_idx(count: List[Tuple[str, int]],\n",
        "                   min_words: Union[int, float] = 0.0,\n",
        "                   max_words: Union[int, float] = 1.0) -> Dict[str, int]:\n",
        "    max_count = count[0][1]\n",
        "    if isinstance(min_words, float):\n",
        "        min_words = max_count * min_words\n",
        "    if isinstance(max_words, float):\n",
        "        max_words = max_count * max_words\n",
        "    \n",
        "    all_words = [w[0] for w in count if max_words >= w[1] >= min_words]\n",
        "    \n",
        "    all_words = ['<pad>', '<unk>'] + all_words\n",
        "    \n",
        "    word_to_idx = {k: v for k, v in zip(all_words, range(0, len(all_words)))}\n",
        "    return word_to_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsMgDEWoeaEC"
      },
      "source": [
        "#### Task 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF2wHVhkeYtW"
      },
      "source": [
        "Count how many unique words are there in our train dataset. Parameters `min_words` and `max_words` should be initialized as default values. Initialize variable `word_to_idx` from method `get_word_to_idx`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKD4GgGpfybD"
      },
      "source": [
        "##### help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J9Bf3xof0eB"
      },
      "source": [
        "# 1. first you can count occurences of each word\n",
        "# 2. second you can pass list of tuples for each pair (word, num_of_occurencies) to function get_word_to_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGSXKAasgIf6"
      },
      "source": [
        "##### Continue work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE62qt8k-ad-"
      },
      "source": [
        "count = Counter()\n",
        "word_to_idx = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-EIfsqqfNtZ"
      },
      "source": [
        "assert len(word_to_idx) == 3805"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7mJ29v2IV1-"
      },
      "source": [
        "def create_matrix_of_texts(dataset, max_sequence_length, \n",
        "                           pad_token, word2index):\n",
        "    texts = np.full((len(dataset), max_sequence_length),\n",
        "                    word2index[pad_token], dtype=np.int64)  # creating empty matrix\n",
        "\n",
        "    for ind, row in enumerate(dataset):\n",
        "          trim_length = min(max_sequence_length, len(row))\n",
        "          text = row[:trim_length]\n",
        "          texts[ind, :trim_length] = [word2index[item.lower()] for item in text]\n",
        "    return texts\n",
        "\n",
        "def create_matrix_of_tags(dataset, max_sequence_length, pad_index, tag2idx):\n",
        "    tags = np.full((len(dataset), max_sequence_length),\n",
        "                    pad_index, dtype=np.int64)  # creating empty matrix\n",
        "\n",
        "    for ind, row in enumerate(dataset):\n",
        "          trim_length = min(max_sequence_length, len(row))\n",
        "          labels = row[: trim_length]\n",
        "          tags[ind, : trim_length] = [tag2idx[item] for item in labels]\n",
        "    return tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3qdl6XJHq6z"
      },
      "source": [
        "texts = create_matrix_of_texts(new_data, \n",
        "                               int(max_lens.quantile(0.97)),\n",
        "                               '<pad>', word_to_idx)\n",
        "tags = create_matrix_of_tags(biluo_labels,\n",
        "                             int(max_lens.quantile(0.97)),\n",
        "                             tag_to_idx['PAD'],\n",
        "                             tag_to_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69okXnDhKpYW"
      },
      "source": [
        "class NerDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 texts: np.array,\n",
        "                 tags: np.array):\n",
        "        self.tags = tags\n",
        "        self.texts = texts\n",
        "        \n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
        "        tokens_tensor = torch.tensor(self.texts[idx], dtype=torch.int64)\n",
        "        return tokens_tensor, torch.tensor(self.tags[idx], dtype=torch.int64)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        dataset_len = self.texts.shape[0]\n",
        "        return dataset_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCe1keAYKpMb"
      },
      "source": [
        "ner_dataset = NerDataset(texts, tags)\n",
        "assert len(ner_dataset) == 7634"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG6oV-zTKo_e"
      },
      "source": [
        "from torch.utils.data.dataset import random_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXx7nlGeGo-R"
      },
      "source": [
        "BATCH_SIZE = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGrEjzG4gadC"
      },
      "source": [
        "#### Task 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8MniGMFgdlG"
      },
      "source": [
        "Initialize dataloaders for train and validation. There is no need to shuffle validation dataloader, but it is better to shuffle train and drop last batch from train dataloader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehjV96FzLItx"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "num_train = int(len(ner_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(ner_dataset, [num_train, len(ner_dataset) - num_train])\n",
        "\n",
        "train_dataloader = pass\n",
        "valid_dataloader = pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl8EJ-bzgz7t"
      },
      "source": [
        "In this toy example we will first try simple FCNN for your problem. Let's look how bad/good it fits our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWLYR4MFhWDB"
      },
      "source": [
        "#### Task 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10wgebnyhYiL"
      },
      "source": [
        "Initialize sequantial layers of our FCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ngc4mz_P7a5"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "class NerModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        word2idx: Dict,\n",
        "        embedding_dim: int = 100,\n",
        "        mapping: Dict[int, str] = None,\n",
        "        hidden_size: int = 256,\n",
        "    ):\n",
        "        super(NerModel, self).__init__()\n",
        "        if not mapping:\n",
        "            raise RuntimeError(f'Empty labels')\n",
        "        self.word2idx = word2idx\n",
        "        self.labels = mapping\n",
        "\n",
        "        self.linear_sigmoid_stack = nn.Sequential(\n",
        "            # FILL YOUR CODE HERE\n",
        "        )\n",
        "\n",
        "    def forward(self, tokens: LongTensor) -> FloatTensor:\n",
        "\n",
        "        return self.linear_sigmoid_stack(tokens).view(-1, len(self.labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il0TbOeVhkZC"
      },
      "source": [
        "Now we will create basic network and check how it calculate loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyw63e8YP_ko"
      },
      "source": [
        "model = NerModel(word_to_idx, 100, {idx: str(idx) for idx in range(10)})\n",
        "assert (\n",
        "        len(list(name for name, module in model.named_modules())) > 3\n",
        "    ), \"Not enough layers created\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QktELcGQEIw"
      },
      "source": [
        "num_classes = len(tag_to_idx)\n",
        "model = NerModel(word_to_idx, 30, {idx: str(idx) for idx in range(num_classes)})\n",
        "seq_len = 32\n",
        "example_input = torch.randint(0, 2, (BATCH_SIZE, seq_len), dtype=torch.int64)\n",
        "logits = model(example_input)\n",
        "assert isinstance(logits, torch.FloatTensor)\n",
        "assert logits.shape == (BATCH_SIZE * seq_len, num_classes), f\"current size of model output {logits.shape}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wsrqGA7hz1O"
      },
      "source": [
        "i = iter(train_dataloader)\n",
        "text, label = next(i)\n",
        "logits = model(text)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "loss_function(logits, label.view(-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaI3WuNFh3mK"
      },
      "source": [
        "Everything looks pretty well and seems correct. Lets now write evaluation function and begin our training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rENMRVxiJdR"
      },
      "source": [
        "#### Task 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6VN-xHQiNNY"
      },
      "source": [
        "Fill lines of code. First you need to initialize variable of `correct_labels` (labels, that are not special ones). Then you need to get `true_labels` and `predicted` variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rXHLysLiL3c"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "    correct_labels = None  # Fill your code here\n",
        "    predicted, true_labels = list(), list()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(dataloader):\n",
        "            tokens, label = batch\n",
        "            tokens = tokens.to(device)\n",
        "            \n",
        "            logits = model(tokens)\n",
        "            predictions = F.log_softmax(logits, dim=1).reshape(-1,\n",
        "                                                               int(max_lens.quantile(0.97)),\n",
        "                                                               len(tag_to_idx)).argmax(dim=2).flatten().detach().cpu().numpy()\n",
        "            predicted.extend(predictions)\n",
        "            true_labels.extend(label.flatten().detach().cpu().numpy())\n",
        "    \n",
        "    true_labels = None # Fill your code here\n",
        "    \n",
        "    predicted = None  # Fill your code here\n",
        "    print('\\n', classification_report(true_labels,\n",
        "                                      predicted,\n",
        "                                      labels=correct_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKo2r1WYizlt"
      },
      "source": [
        "Now we can create our model and start trainig"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ULTTPvri4YI"
      },
      "source": [
        "model = NerModel(word_to_idx, 300, tag_to_idx)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXa9nIHGi5uV"
      },
      "source": [
        "for e in range(6):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    for sent in tqdm(train_dataloader):\n",
        "\n",
        "            # (1) Set gradient to zero for new example: Set gradients to zero before pass\n",
        "            model.zero_grad()\n",
        "            \n",
        "            # (2) Encode sentence and tag sequence as sequences of indices\n",
        "            input_sent, gold_tags = sent\n",
        "\n",
        "            # (3) Predict tags (sentence by sentence)\n",
        "            if len(input_sent) > 0:\n",
        "                pred_scores = model(input_sent.to(device))\n",
        "                mask = gold_tags != 0\n",
        "                # (4) Compute loss and do backward step\n",
        "                loss = loss_function(pred_scores.to(device), gold_tags.view(-1).to(device))\n",
        "                loss.backward()\n",
        "              \n",
        "                # (5) Optimize parameter values\n",
        "                optimizer.step()\n",
        "          \n",
        "                # (6) Accumulate loss\n",
        "                total_loss += loss\n",
        "    print('\\nEpoch: %d, loss: %.4f' % (e, total_loss / len(train_dataloader)))\n",
        "    evaluate(valid_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sshz2tYrjBy9"
      },
      "source": [
        "We did it, but quality of model is rather bad. Now you can try RNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MxVkYxXjK8c"
      },
      "source": [
        "### RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-pn2aRRjS_D"
      },
      "source": [
        "All process from FCNN works fine, but we need to use new architecture. Let's write new model, that process data and uses some kind of Recurrent Neural Network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrEZNuAjjniz"
      },
      "source": [
        "#### Task 11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd6h4WsUjpzB"
      },
      "source": [
        "Fill missing layers of model. You can use any RNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhyuQhs1LIiI"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "class NerRNNModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        word2idx: Dict,\n",
        "        embedding_dim: int = 100,\n",
        "        mapping: Dict[int, str] = None,\n",
        "        hidden_size: int = 256\n",
        "    ):\n",
        "        super(NerRNNModel, self).__init__()\n",
        "        if not mapping:\n",
        "            raise RuntimeError(f'Empty labels')\n",
        "        self.word2idx = word2idx\n",
        "        self.labels = mapping\n",
        "        self.embedding = nn.Embedding(len(word_to_idx), embedding_dim)\n",
        "        self.encoder = nn.RNN(\n",
        "            # FILL YOUR CODE HERE\n",
        "        )\n",
        "        self.projection = nn.Linear(hidden_size, len(mapping))\n",
        "\n",
        "    def forward(self, tokens: LongTensor) -> FloatTensor:\n",
        "\n",
        "        emb = self.embedding(tokens)        \n",
        "        h, _ = self.encoder(emb)\n",
        "        pred = self.projection(h)\n",
        "        return pred.view(-1, len(self.labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHr5vn8Xj99l"
      },
      "source": [
        "Now we can duplicate all cells from above and simply just start new iteration of training new model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imiTzbYgLQEx"
      },
      "source": [
        "model = NerRNNModel(word_to_idx, 100, {idx: str(idx) for idx in range(10)})\n",
        "assert (\n",
        "        len(list(name for name, module in model.named_modules())) > 3\n",
        "    ), \"Not enough layers created\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM-Ti_5cLP3V"
      },
      "source": [
        "num_classes = len(tag_to_idx)\n",
        "model = NerRNNModel(word_to_idx, 30, {idx: str(idx) for idx in range(num_classes)})\n",
        "seq_len = 32\n",
        "example_input = torch.randint(0, 2, (BATCH_SIZE, seq_len), dtype=torch.int64)\n",
        "logits = model(example_input)\n",
        "assert isinstance(logits, torch.FloatTensor)\n",
        "assert logits.shape == (BATCH_SIZE * seq_len, num_classes), f\"current size of model output {logits.shape}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lDFmLHDLPox"
      },
      "source": [
        "i = iter(train_dataloader)\n",
        "text, label = next(i)\n",
        "logits = model(text)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "loss_function(logits, label.view(-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCaivj5uLiHA"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "    correct_labels = [value for value in idx_to_tag.values() if value != 'O' and value != 'PAD']\n",
        "    predicted, true_labels = list(), list()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(dataloader):\n",
        "            tokens, label = batch\n",
        "            tokens = tokens.to(device)\n",
        "            \n",
        "            logits = model(tokens)\n",
        "            predictions = F.log_softmax(logits, dim=1).reshape(-1,\n",
        "                                                               int(max_lens.quantile(0.97)),\n",
        "                                                               len(tag_to_idx)).argmax(dim=2).flatten().detach().cpu().numpy()\n",
        "            predicted.extend(predictions)\n",
        "            true_labels.extend(label.flatten().detach().cpu().numpy())\n",
        "    \n",
        "    true_labels = [idx_to_tag[val] for val in true_labels]\n",
        "    \n",
        "    predicted = [idx_to_tag[val] for val in predicted]\n",
        "    print('\\n', classification_report(true_labels,\n",
        "                                      predicted,\n",
        "                                      labels=correct_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LtiUtB5Lh5R"
      },
      "source": [
        "model = NerRNNModel(word_to_idx, 300, tag_to_idx)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spM7aX3ELhsC"
      },
      "source": [
        "sum([params.numel() for params in model.parameters() if params.requires_grad])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRf8qs1fLIVY"
      },
      "source": [
        "for e in range(6):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    for sent in tqdm(train_dataloader):\n",
        "\n",
        "            # (1) Set gradient to zero for new example: Set gradients to zero before pass\n",
        "            model.zero_grad()\n",
        "            \n",
        "            # (2) Encode sentence and tag sequence as sequences of indices\n",
        "            input_sent, gold_tags = sent\n",
        "\n",
        "            # (3) Predict tags (sentence by sentence)\n",
        "            if len(input_sent) > 0:\n",
        "                pred_scores = model(input_sent.to(device))\n",
        "                mask = gold_tags != 0\n",
        "                # (4) Compute loss and do backward step\n",
        "                loss = loss_function(pred_scores.to(device), gold_tags.view(-1).to(device))\n",
        "                loss.backward()\n",
        "              \n",
        "                # (5) Optimize parameter values\n",
        "                optimizer.step()\n",
        "          \n",
        "                # (6) Accumulate loss\n",
        "                total_loss += loss\n",
        "    print('\\nEpoch: %d, loss: %.4f' % (e, total_loss / len(train_dataloader)))\n",
        "    evaluate(valid_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkBKi5YtlpsT"
      },
      "source": [
        "Futher working:\n",
        "- Try more complex architecture\n",
        "- try bidirectional rnns\n",
        "- try other hyperparameters\n",
        "- try pretrained embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02pZb8CdmXNB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}